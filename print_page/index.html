
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://aygalic.github.io/handbook/print_page/">
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>Print Site - Interview Questions Handbook</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8608ea7d.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../css/print-site-enum-headings1.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings4.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings5.css">
    
      <link rel="stylesheet" href="../css/print-site-enum-headings6.css">
    
      <link rel="stylesheet" href="../css/print-site.css">
    
      <link rel="stylesheet" href="../css/print-site-material.css">
    
    <script>__md_scope=new URL("/handbook/",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Interview Questions Handbook" class="md-header__button md-logo" aria-label="Interview Questions Handbook" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Interview Questions Handbook
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print Site
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/aygalic/handbook" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    aygalic/handbook
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Thesis handbook

      </a>
    </li>
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../grassroots/coding_practices/" class="md-tabs__link">
          
  
    
  
  Grassroots

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../research/" class="md-tabs__link">
          
  
    
  
  Research

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Print Site

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Interview Questions Handbook" class="md-nav__button md-logo" aria-label="Interview Questions Handbook" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Interview Questions Handbook
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/aygalic/handbook" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    aygalic/handbook
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Thesis handbook
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Grassroots
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Grassroots
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/coding_practices/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Coding practices
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Coding practices
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/coding_practices/MLOps/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    MLOps
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_2">
            <span class="md-nav__icon md-icon"></span>
            MLOps
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/coding_practices/python/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Python
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_3">
            <span class="md-nav__icon md-icon"></span>
            Python
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/machine_learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Machine learning
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Machine learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/machine_learning/1_theoretical_fundamentals/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    1 theoretical fundamentals
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_2">
            <span class="md-nav__icon md-icon"></span>
            1 theoretical fundamentals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/machine_learning/2_python_fundamentals/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    2 python fundamentals
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_3">
            <span class="md-nav__icon md-icon"></span>
            2 python fundamentals
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/machine_learning/3_deep_learning/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    3 deep learning
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_4">
            <span class="md-nav__icon md-icon"></span>
            3 deep learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/machine_learning/4_transformers/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    4 transformers
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_5">
            <span class="md-nav__icon md-icon"></span>
            4 transformers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Statistics
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Statistics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/1_descriptive_statistics/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    1 descriptive statistics
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3_2" id="__nav_2_3_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_2">
            <span class="md-nav__icon md-icon"></span>
            1 descriptive statistics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/1_descriptive_statistics/central_tendency/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Central tendency
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/1_descriptive_statistics/dispersion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dispersion
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/1_descriptive_statistics/distribution_characteristics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distribution Characteristics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Expected Value - Variance Relationship
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/2_probability_distributions/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    2 probability distributions
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3_3" id="__nav_2_3_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_3">
            <span class="md-nav__icon md-icon"></span>
            2 probability distributions
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/2_probability_distributions/continuous_distributions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Continuous Probability Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/2_probability_distributions/discrete_distributions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Discrete Probability Distributions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/3_computational_rules/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    3 computational rules
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_4">
            <span class="md-nav__icon md-icon"></span>
            3 computational rules
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/4_statistical_inference/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    4 statistical inference
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3_5" id="__nav_2_3_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_5">
            <span class="md-nav__icon md-icon"></span>
            4 statistical inference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/4_statistical_inference/estimation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistical Estimation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/4_statistical_inference/hypothesis_testing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistical Hypothesis Testing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/4_statistical_inference/sampling_theory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sampling Theory
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/5_regression_analysis/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    5 regression analysis
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3_6" id="__nav_2_3_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_6">
            <span class="md-nav__icon md-icon"></span>
            5 regression analysis
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/5_regression_analysis/generalized_linear_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generalized Linear Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/5_regression_analysis/multiple_linear_regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simple Linear Regression: Mathematical Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/5_regression_analysis/simple_linear_regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simple Linear Regression
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/6_experimental_design/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    6 experimental design
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3_7" id="__nav_2_3_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_7">
            <span class="md-nav__icon md-icon"></span>
            6 experimental design
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/6_experimental_design/a_b_testing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A/B Testing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/6_experimental_design/basic_principles/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basic Principles of Experimental Design
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/7_bayesian_statistics/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    7 bayesian statistics
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3_8" id="__nav_2_3_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_8">
            <span class="md-nav__icon md-icon"></span>
            7 bayesian statistics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/7_bayesian_statistics/applications/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bayesian Statistical Applications
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/7_bayesian_statistics/fundamentals/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fundamentals of Bayesian Statistics
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../grassroots/statistics/8_advanced_topics/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    8 advanced topics
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2_3_9" id="__nav_2_3_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3_9">
            <span class="md-nav__icon md-icon"></span>
            8 advanced topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../grassroots/statistics/8_advanced_topics/dimensionality_reduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dimensionality Reduction
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../research/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Research
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Research
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../research/0_definition/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    0 definition
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            0 definition
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../research/1_classification/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    1 classification
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            1 classification
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../research/2_detection/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    2 detection
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            2 detection
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../research/3_benchmarks/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    3 benchmarks
    
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            3 benchmarks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Misc
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            Misc
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../research/misc/DeepSeek/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    What makes DeepSeek V3 / R1 so ✨ special ✨
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Print Site
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Print Site
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    <span class="md-ellipsis">
      1. Thesis handbook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-research" class="md-nav__link">
    <span class="md-ellipsis">
      II. Research
    </span>
  </a>
  
    <nav class="md-nav" aria-label="II. Research">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#research" class="md-nav__link">
    <span class="md-ellipsis">
      2. Research
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    <span class="md-ellipsis">
      1. Thesis handbook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-research" class="md-nav__link">
    <span class="md-ellipsis">
      II. Research
    </span>
  </a>
  
    <nav class="md-nav" aria-label="II. Research">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#research" class="md-nav__link">
    <span class="md-ellipsis">
      2. Research
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Table of Contents</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index"><h1 id="index-thesis-handbook">Thesis handbook<a class="headerlink" href="#index-thesis-handbook" title="Permanent link">&para;</a></h1>
<p>This handbook is design in two part that will be updated all along my thesis. </p>
<p>The first part will be dedicated to grass root knowledge, covering fundamental concepts for subsequent research.</p>
<p>The second part will cover everything related to my research.</p>
<p>Part 1:</p>
<ul>
<li>Linear Algebra (upcoming)</li>
<li><a href="#grassroots-machine_learning">Machine Learning</a></li>
<li><a href="#grassroots-statistics">Statistics</a></li>
<li><a href="#grassroots-coding_practices">Coding practices</a></li>
</ul>
<p>Part 2:
- <a href="#research">Research</a></p></section>
                        <h1 class='nav-section-title' id='section-grassroots'>
                            Grassroots <a class='headerlink' href='#section-grassroots' title='Permanent link'>↵</a>
                        </h1>
                        
                        <h2 class='nav-section-title' id='section-coding-practices'>
                            Coding practices <a class='headerlink' href='#section-coding-practices' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="grassroots-coding_practices"><h1 id="grassroots-coding_practices-coding-practices-basics">Coding practices basics<a class="headerlink" href="#grassroots-coding_practices-coding-practices-basics" title="Permanent link">&para;</a></h1>
<p>In this section, we cover coding practices and MLOps tool.</p>
<ul>
<li>1 - <a href="#grassroots-coding_practices-python">Python</a></li>
<li>2 - <a href="#grassroots-coding_practices-mlops">MLOps</a></li>
</ul></section>
                        <h3 class='nav-section-title' id='section-mlops'>
                            MLOps <a class='headerlink' href='#section-mlops' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-coding_practices-mlops"><h1 id="grassroots-coding_practices-mlops-mlops-and-machine-learning-engineering">MLOps and Machine Learning Engineering<a class="headerlink" href="#grassroots-coding_practices-mlops-mlops-and-machine-learning-engineering" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-coding_practices-mlops-machine-learning-pipeline-components">Machine Learning Pipeline Components<a class="headerlink" href="#grassroots-coding_practices-mlops-machine-learning-pipeline-components" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-mlops-version-control-for-ml">Version Control for ML<a class="headerlink" href="#grassroots-coding_practices-mlops-version-control-for-ml" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Code Version Control</strong></p>
<ul>
<li>Git best practices for ML projects</li>
<li>Branching strategies (feature branches, model versions)</li>
<li><code>.gitignore</code> for ML projects (data, models, logs)</li>
</ul>
</li>
<li>
<p><strong>Data Version Control</strong></p>
<ul>
<li>DVC (Data Version Control)
<div class="highlight"><pre><span></span><code><span class="c1"># DVC example</span>
<span class="n">dvc</span> <span class="n">init</span>
<span class="n">dvc</span> <span class="n">add</span> <span class="n">data</span><span class="o">/</span><span class="n">training_data</span><span class="o">.</span><span class="n">csv</span>
<span class="n">dvc</span> <span class="n">remote</span> <span class="n">add</span> <span class="o">-</span><span class="n">d</span> <span class="n">storage</span> <span class="n">s3</span><span class="p">:</span><span class="o">//</span><span class="n">mybucket</span><span class="o">/</span><span class="n">dvcstore</span>
<span class="n">dvc</span> <span class="n">push</span>
</code></pre></div></li>
<li>Benefits:<ul>
<li>Track large files</li>
<li>Reproduce experiments</li>
<li>Share data between team members</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Model Version Control</strong></p>
<ul>
<li>MLflow model registry</li>
<li>Model versioning strategies</li>
<li>Metadata tracking</li>
</ul>
</li>
</ul>
<h3 id="grassroots-coding_practices-mlops-experiment-tracking">Experiment Tracking<a class="headerlink" href="#grassroots-coding_practices-mlops-experiment-tracking" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Components to track:</p>
<ul>
<li>Model parameters</li>
<li>Training metrics</li>
<li>Dataset versions</li>
<li>Environment details</li>
</ul>
</li>
<li>
<p><strong>MLflow Example</strong>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">mlflow</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">():</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_epoch</span><span class="p">()</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">sklearn</span><span class="o">.</span><span class="n">log_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">)</span>
</code></pre></div></p>
</li>
</ul>
<h3 id="grassroots-coding_practices-mlops-model-serving">Model Serving<a class="headerlink" href="#grassroots-coding_practices-mlops-model-serving" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-coding_practices-mlops-rest-api-with-fastapi">REST API with FastAPI<a class="headerlink" href="#grassroots-coding_practices-mlops-rest-api-with-fastapi" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">fastapi</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastAPI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">joblib</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model.joblib&quot;</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PredictionInput</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">features</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>

<span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/predict&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">:</span> <span class="n">PredictionInput</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">input_data</span><span class="o">.</span><span class="n">features</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;prediction&quot;</span><span class="p">:</span> <span class="n">prediction</span><span class="o">.</span><span class="n">tolist</span><span class="p">()}</span>
</code></pre></div>
<h4 id="grassroots-coding_practices-mlops-model-serving-patterns">Model Serving Patterns<a class="headerlink" href="#grassroots-coding_practices-mlops-model-serving-patterns" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Online Serving</strong></p>
<ul>
<li>Real-time predictions</li>
<li>Low latency requirements</li>
<li>API endpoints</li>
</ul>
</li>
<li>
<p><strong>Batch Serving</strong></p>
<ul>
<li>Large-scale predictions</li>
<li>Scheduled jobs</li>
<li>Data pipeline integration</li>
</ul>
</li>
</ul>
<h2 id="grassroots-coding_practices-mlops-containerization-and-orchestration">Containerization and Orchestration<a class="headerlink" href="#grassroots-coding_practices-mlops-containerization-and-orchestration" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-mlops-docker-for-ml-applications">Docker for ML Applications<a class="headerlink" href="#grassroots-coding_practices-mlops-docker-for-ml-applications" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Dockerfile Example</strong>
<div class="highlight"><pre><span></span><code><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.9-slim</span>

<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>
<span class="k">COPY</span><span class="w"> </span>requirements.txt<span class="w"> </span>.
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt

<span class="k">COPY</span><span class="w"> </span>model/<span class="w"> </span>model/
<span class="k">COPY</span><span class="w"> </span>src/<span class="w"> </span>src/

<span class="k">EXPOSE</span><span class="w"> </span><span class="s">8000</span>
<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;uvicorn&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;src.main:app&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;--host&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;0.0.0.0&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;--port&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;8000&quot;</span><span class="p">]</span>
</code></pre></div></li>
</ul>
<h3 id="grassroots-coding_practices-mlops-kubernetes-for-ml-workloads">Kubernetes for ML Workloads<a class="headerlink" href="#grassroots-coding_practices-mlops-kubernetes-for-ml-workloads" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Key Concepts</strong></p>
<ul>
<li>Pods vs Deployments</li>
<li>Services</li>
<li>ConfigMaps and Secrets</li>
<li>Resource management</li>
</ul>
</li>
<li>
<p><strong>Example Deployment</strong>
<div class="highlight"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-service</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-service</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-service</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-service</span>
<span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-service:v1</span>
<span class="w">        </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">          </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1Gi&quot;</span>
<span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;500m&quot;</span>
<span class="w">        </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8000</span>
</code></pre></div></p>
</li>
</ul>
<h2 id="grassroots-coding_practices-mlops-monitoring-and-observability">Monitoring and Observability<a class="headerlink" href="#grassroots-coding_practices-mlops-monitoring-and-observability" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-mlops-model-monitoring">Model Monitoring<a class="headerlink" href="#grassroots-coding_practices-mlops-model-monitoring" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Metrics to Track</strong></p>
<ul>
<li>Model performance (accuracy, F1, etc.)</li>
<li>Prediction latency</li>
<li>Feature drift</li>
<li>Data quality</li>
</ul>
</li>
<li>
<p><strong>Monitoring Setup Example</strong>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">prometheus_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">Histogram</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">PREDICTION_LATENCY</span> <span class="o">=</span> <span class="n">Histogram</span><span class="p">(</span>
    <span class="s1">&#39;model_prediction_latency_seconds&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Time spent processing prediction&#39;</span>
<span class="p">)</span>
<span class="n">PREDICTION_COUNTER</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span>
    <span class="s1">&#39;model_predictions_total&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Total number of predictions&#39;</span>
<span class="p">)</span>

<span class="nd">@PREDICTION_LATENCY</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="n">PREDICTION_COUNTER</span><span class="o">.</span><span class="n">inc</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</code></pre></div></p>
</li>
</ul>
<h3 id="grassroots-coding_practices-mlops-data-quality-monitoring">Data Quality Monitoring<a class="headerlink" href="#grassroots-coding_practices-mlops-data-quality-monitoring" title="Permanent link">&para;</a></h3>
<ul>
<li>Schema validation</li>
<li>Feature statistics</li>
<li>Data drift detection</li>
<li>Example with Great Expectations:
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">great_expectations</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ge</span>

<span class="k">def</span><span class="w"> </span><span class="nf">validate_dataset</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">expectation_suite</span> <span class="o">=</span> <span class="n">ge</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">ExpectationSuite</span><span class="p">(</span>
        <span class="n">expectation_suite_name</span><span class="o">=</span><span class="s2">&quot;my_suite&quot;</span>
    <span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">expect_column_values_to_not_be_null</span><span class="p">(</span><span class="s2">&quot;important_feature&quot;</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">expect_column_values_to_be_between</span><span class="p">(</span>
        <span class="s2">&quot;numeric_feature&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">100</span>
    <span class="p">)</span>
</code></pre></div></li>
</ul>
<h2 id="grassroots-coding_practices-mlops-cicd-for-machine-learning">CI/CD for Machine Learning<a class="headerlink" href="#grassroots-coding_practices-mlops-cicd-for-machine-learning" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-mlops-continuous-integration">Continuous Integration<a class="headerlink" href="#grassroots-coding_practices-mlops-continuous-integration" title="Permanent link">&para;</a></h3>
<ul>
<li>Unit tests for ML code</li>
<li>Integration tests</li>
<li>Model validation tests</li>
<li>Example test:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">test_model_prediction_shape</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">load_test_data</span><span class="p">()</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">n_classes</span>
</code></pre></div></li>
</ul>
<h3 id="grassroots-coding_practices-mlops-continuous-deployment">Continuous Deployment<a class="headerlink" href="#grassroots-coding_practices-mlops-continuous-deployment" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Deployment Strategies</strong></p>
<ul>
<li>Blue-Green deployment</li>
<li>Canary releases</li>
<li>A/B testing</li>
</ul>
</li>
<li>
<p><strong>Example Feature Flag Configuration</strong>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_model_version</span><span class="p">(</span><span class="n">user_id</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">feature_flag</span><span class="o">.</span><span class="n">is_enabled</span><span class="p">(</span><span class="s1">&#39;new_model&#39;</span><span class="p">,</span> <span class="n">user_id</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;v2&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;v1&#39;</span><span class="p">)</span>
</code></pre></div></p>
</li>
</ul>
<h2 id="grassroots-coding_practices-mlops-infrastructure-as-code-iac">Infrastructure as Code (IaC)<a class="headerlink" href="#grassroots-coding_practices-mlops-infrastructure-as-code-iac" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-mlops-terraform-example">Terraform Example<a class="headerlink" href="#grassroots-coding_practices-mlops-terraform-example" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kr">resource</span><span class="w"> </span><span class="nc">&quot;aws_sagemaker_model&quot;</span><span class="w"> </span><span class="nv">&quot;example&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="na">name</span><span class="w">               </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;my-model&quot;</span>
<span class="w">  </span><span class="na">execution_role_arn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">aws_iam_role.example.arn</span>

<span class="w">  </span><span class="nb">primary_container</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="na">image</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;${aws_ecr_repository.example.repository_url}:latest&quot;</span>
<span class="w">    </span><span class="na">model_data_url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;s3://${aws_s3_bucket.example.bucket}/model.tar.gz&quot;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<h3 id="grassroots-coding_practices-mlops-cloud-specific-services">Cloud-Specific Services<a class="headerlink" href="#grassroots-coding_practices-mlops-cloud-specific-services" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-coding_practices-mlops-aws-ml-stack">AWS ML Stack<a class="headerlink" href="#grassroots-coding_practices-mlops-aws-ml-stack" title="Permanent link">&para;</a></h4>
<ul>
<li>SageMaker</li>
<li>Lambda</li>
<li>ECS/EKS</li>
<li>S3</li>
<li>CloudWatch</li>
</ul>
<h4 id="grassroots-coding_practices-mlops-gcp-ml-stack">GCP ML Stack<a class="headerlink" href="#grassroots-coding_practices-mlops-gcp-ml-stack" title="Permanent link">&para;</a></h4>
<ul>
<li>Vertex AI</li>
<li>Cloud Run</li>
<li>GKE</li>
<li>Cloud Storage</li>
<li>Cloud Monitoring</li>
</ul>
<h2 id="grassroots-coding_practices-mlops-best-practices">Best Practices<a class="headerlink" href="#grassroots-coding_practices-mlops-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-mlops-production-ml-checklist">Production ML Checklist<a class="headerlink" href="#grassroots-coding_practices-mlops-production-ml-checklist" title="Permanent link">&para;</a></h3>
<ol>
<li>Model versioning and reproducibility</li>
<li>Automated testing pipeline</li>
<li>Monitoring and alerting</li>
<li>Documentation</li>
<li>Resource optimization</li>
<li>Security considerations</li>
<li>Compliance requirements</li>
</ol>
<h3 id="grassroots-coding_practices-mlops-security-considerations">Security Considerations<a class="headerlink" href="#grassroots-coding_practices-mlops-security-considerations" title="Permanent link">&para;</a></h3>
<ul>
<li>Model access control</li>
<li>Data encryption</li>
<li>API authentication</li>
<li>Example secure endpoint:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">fastapi</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">Security</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fastapi.security</span><span class="w"> </span><span class="kn">import</span> <span class="n">APIKeyHeader</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">()</span>
<span class="n">api_key_header</span> <span class="o">=</span> <span class="n">APIKeyHeader</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;X-API-Key&quot;</span><span class="p">)</span>

<span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/predict&quot;</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>
    <span class="n">input_data</span><span class="p">:</span> <span class="n">PredictionInput</span><span class="p">,</span>
    <span class="n">api_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Security</span><span class="p">(</span><span class="n">api_key_header</span><span class="p">)</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_valid_api_key</span><span class="p">(</span><span class="n">api_key</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="n">status_code</span><span class="o">=</span><span class="mi">403</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;prediction&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">input_data</span><span class="o">.</span><span class="n">features</span><span class="p">])}</span>
</code></pre></div></li>
</ul>
<h3 id="grassroots-coding_practices-mlops-performance-optimization">Performance Optimization<a class="headerlink" href="#grassroots-coding_practices-mlops-performance-optimization" title="Permanent link">&para;</a></h3>
<ul>
<li>Model quantization</li>
<li>Batch prediction optimization</li>
<li>Caching strategies</li>
<li>Example caching:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">lru_cache</span>

<span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_prediction</span><span class="p">(</span><span class="n">feature_tuple</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">feature_tuple</span><span class="p">])</span>
</code></pre></div></li>
</ul>
<h2 id="grassroots-coding_practices-mlops-interview-tips-for-mlops">Interview Tips for MLOps<a class="headerlink" href="#grassroots-coding_practices-mlops-interview-tips-for-mlops" title="Permanent link">&para;</a></h2>
<ol>
<li>Be prepared to discuss end-to-end ML pipelines</li>
<li>Understand scalability challenges</li>
<li>Know common failure points and solutions</li>
<li>Be familiar with cloud services</li>
<li>Understanding of monitoring and observability</li>
<li>Knowledge of deployment strategies</li>
<li>Security best practices</li>
</ol>
<p>Remember to focus on:
- Real-world problem-solving
- Trade-offs between different approaches
- Scalability considerations
- Cost optimization
- Team collaboration aspects</p></section><h1 class='nav-section-title-end'>Ended: MLOps</h1>
                        <h3 class='nav-section-title' id='section-python'>
                            Python <a class='headerlink' href='#section-python' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-coding_practices-python"><h1 id="grassroots-coding_practices-python-python-and-computer-science-fundamentals">Python and Computer Science Fundamentals<a class="headerlink" href="#grassroots-coding_practices-python-python-and-computer-science-fundamentals" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-coding_practices-python-data-structures-in-python">Data Structures in Python<a class="headerlink" href="#grassroots-coding_practices-python-data-structures-in-python" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-python-lists-vs-tuples">Lists vs. Tuples<a class="headerlink" href="#grassroots-coding_practices-python-lists-vs-tuples" title="Permanent link">&para;</a></h3>
<p>Lists and tuples are both sequence types in Python, but they have crucial differences:</p>
<ul>
<li>
<p><strong>Mutability</strong></p>
<ul>
<li>Lists are mutable: Elements can be modified, added, or removed after creation</li>
<li>Tuples are immutable: Once created, elements cannot be modified</li>
</ul>
</li>
<li>
<p><strong>Syntax</strong></p>
<ul>
<li>Lists use square brackets: <code>my_list = [1, 2, 3]</code></li>
<li>Tuples use parentheses: <code>my_tuple = (1, 2, 3)</code></li>
</ul>
</li>
<li>
<p><strong>Memory and Performance</strong></p>
<ul>
<li>Tuples generally consume less memory than lists</li>
<li>Tuples are slightly faster to access and iterate over</li>
<li>Lists require extra memory for potential growth</li>
</ul>
</li>
<li>
<p><strong>Use Cases</strong></p>
<ul>
<li>Lists: When you need a collection that will change (e.g., growing dataset)</li>
<li>Tuples: When data shouldn't change (e.g., coordinates, database records)</li>
</ul>
</li>
</ul>
<h3 id="grassroots-coding_practices-python-sets-and-dictionaries">Sets and Dictionaries<a class="headerlink" href="#grassroots-coding_practices-python-sets-and-dictionaries" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-coding_practices-python-sets">Sets<a class="headerlink" href="#grassroots-coding_practices-python-sets" title="Permanent link">&para;</a></h4>
<ul>
<li>Unordered collections of unique elements</li>
<li>Useful for:<ul>
<li>Removing duplicates</li>
<li>Set operations (union, intersection, difference)</li>
<li>Membership testing (faster than lists)</li>
</ul>
</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="n">my_set</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">other_set</span> <span class="o">=</span> <span class="p">{</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">}</span>
<span class="n">union_set</span> <span class="o">=</span> <span class="n">my_set</span> <span class="o">|</span> <span class="n">other_set</span>  <span class="c1"># {1, 2, 3, 4, 5}</span>
<span class="n">intersection</span> <span class="o">=</span> <span class="n">my_set</span> <span class="o">&amp;</span> <span class="n">other_set</span>  <span class="c1"># {3}</span>
</code></pre></div></li>
</ul>
<h4 id="grassroots-coding_practices-python-dictionaries">Dictionaries<a class="headerlink" href="#grassroots-coding_practices-python-dictionaries" title="Permanent link">&para;</a></h4>
<ul>
<li>Key-value pairs with O(1) lookup time</li>
<li>Keys must be immutable (strings, numbers, tuples)</li>
<li>Common operations:
<div class="highlight"><pre><span></span><code><span class="c1"># Creation and access</span>
<span class="n">my_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">my_dict</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span>  <span class="c1"># Direct access</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">my_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Safe access with default</span>

<span class="c1"># Methods</span>
<span class="n">keys</span> <span class="o">=</span> <span class="n">my_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">my_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
<span class="n">items</span> <span class="o">=</span> <span class="n">my_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</code></pre></div></li>
</ul>
<h2 id="grassroots-coding_practices-python-iteration-and-generators">Iteration and Generators<a class="headerlink" href="#grassroots-coding_practices-python-iteration-and-generators" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-python-iterators">Iterators<a class="headerlink" href="#grassroots-coding_practices-python-iterators" title="Permanent link">&para;</a></h3>
<ul>
<li>An iterator is an object that implements:<ul>
<li><code>__iter__()</code>: Returns the iterator object itself</li>
<li><code>__next__()</code>: Returns the next value or raises StopIteration</li>
</ul>
</li>
<li>Used in for loops and comprehensions</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CountUpTo</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_value</span> <span class="o">=</span> <span class="n">max_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_value</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">current</span>

<span class="c1"># Usage</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">CountUpTo</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>  <span class="c1"># Prints 1, 2, 3</span>
</code></pre></div></li>
</ul>
<h3 id="grassroots-coding_practices-python-generators">Generators<a class="headerlink" href="#grassroots-coding_practices-python-generators" title="Permanent link">&para;</a></h3>
<ul>
<li>Special type of iterator created using functions with <code>yield</code></li>
<li>Memory efficient: Values generated on-the-fly</li>
<li>State is preserved between calls</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fibonacci</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">a</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Usage</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">fibonacci</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>  <span class="c1"># Prints 0, 1, 1, 2, 3</span>
</code></pre></div></li>
</ul>
<h3 id="grassroots-coding_practices-python-list-comprehensions-vs-generator-expressions">List Comprehensions vs. Generator Expressions<a class="headerlink" href="#grassroots-coding_practices-python-list-comprehensions-vs-generator-expressions" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="c1"># List comprehension (creates full list in memory)</span>
<span class="n">squares_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>

<span class="c1"># Generator expression (generates values on demand)</span>
<span class="n">squares_gen</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
</code></pre></div>
<h2 id="grassroots-coding_practices-python-memory-management">Memory Management<a class="headerlink" href="#grassroots-coding_practices-python-memory-management" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-python-reference-counting-and-garbage-collection">Reference Counting and Garbage Collection<a class="headerlink" href="#grassroots-coding_practices-python-reference-counting-and-garbage-collection" title="Permanent link">&para;</a></h3>
<ul>
<li>Python uses reference counting for memory management</li>
<li>Objects are deallocated when reference count reaches zero</li>
<li>Circular references handled by garbage collector</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="c1"># Reference counting</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># ref count: 1</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># ref count: 2</span>
<span class="k">del</span> <span class="n">x</span>  <span class="c1"># ref count: 1</span>
<span class="k">del</span> <span class="n">y</span>  <span class="c1"># ref count: 0, list is deallocated</span>
</code></pre></div></li>
</ul>
<h3 id="grassroots-coding_practices-python-memory-model">Memory Model<a class="headerlink" href="#grassroots-coding_practices-python-memory-model" title="Permanent link">&para;</a></h3>
<ul>
<li>Everything is an object</li>
<li>Variables are references to objects</li>
<li>Assignment creates new references</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span>  <span class="c1"># Both variables reference the same list</span>
<span class="n">b</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>  <span class="c1"># Modifies the list through either reference</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># [1, 2, 3, 4]</span>
</code></pre></div></li>
</ul>
<h2 id="grassroots-coding_practices-python-object-oriented-programming">Object-Oriented Programming<a class="headerlink" href="#grassroots-coding_practices-python-object-oriented-programming" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-python-classes-and-objects">Classes and Objects<a class="headerlink" href="#grassroots-coding_practices-python-classes-and-objects" title="Permanent link">&para;</a></h3>
<ul>
<li>Classes define blueprints for objects</li>
<li>Objects are instances of classes</li>
<li>Key concepts:<ul>
<li>Encapsulation</li>
<li>Inheritance</li>
<li>Polymorphism</li>
</ul>
</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DataProcessor</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Subclasses must implement process()&quot;</span><span class="p">)</span>
</code></pre></div></li>
</ul>
<h3 id="grassroots-coding_practices-python-special-methods-magic-methods">Special Methods (Magic Methods)<a class="headerlink" href="#grassroots-coding_practices-python-special-methods-magic-methods" title="Permanent link">&para;</a></h3>
<ul>
<li>Define behavior for built-in operations</li>
<li>Common methods:<ul>
<li><code>__init__</code>: Constructor</li>
<li><code>__str__</code>: String representation</li>
<li><code>__len__</code>: Length</li>
<li><code>__call__</code>: Make object callable</li>
</ul>
</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Dataset</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Dataset with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> records&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div></li>
</ul>
<h2 id="grassroots-coding_practices-python-common-interview-topics">Common Interview Topics<a class="headerlink" href="#grassroots-coding_practices-python-common-interview-topics" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-coding_practices-python-time-and-space-complexity">Time and Space Complexity<a class="headerlink" href="#grassroots-coding_practices-python-time-and-space-complexity" title="Permanent link">&para;</a></h3>
<ul>
<li>Big O notation basics</li>
<li>Common complexities:<ul>
<li>O(1): Constant time</li>
<li>O(log n): Logarithmic</li>
<li>O(n): Linear</li>
<li>O(n log n): Log-linear</li>
<li>O(n²): Quadratic</li>
</ul>
</li>
</ul>
<h3 id="grassroots-coding_practices-python-common-algorithms">Common Algorithms<a class="headerlink" href="#grassroots-coding_practices-python-common-algorithms" title="Permanent link">&para;</a></h3>
<ul>
<li>Sorting algorithms<ul>
<li>Quick sort: Average O(n log n)</li>
<li>Merge sort: Guaranteed O(n log n)</li>
<li>Bubble sort: O(n²)</li>
</ul>
</li>
<li>Search algorithms<ul>
<li>Binary search: O(log n)</li>
<li>Linear search: O(n)</li>
</ul>
</li>
</ul>
<h3 id="grassroots-coding_practices-python-threading-vs-multiprocessing">Threading vs. Multiprocessing<a class="headerlink" href="#grassroots-coding_practices-python-threading-vs-multiprocessing" title="Permanent link">&para;</a></h3>
<ul>
<li>Threading:<ul>
<li>Shares memory space</li>
<li>Good for I/O-bound tasks</li>
<li>Limited by GIL for CPU-bound tasks</li>
</ul>
</li>
<li>Multiprocessing:<ul>
<li>Separate memory spaces</li>
<li>Good for CPU-bound tasks</li>
<li>Higher memory overhead</li>
</ul>
</li>
</ul>
<h3 id="grassroots-coding_practices-python-context-managers">Context Managers<a class="headerlink" href="#grassroots-coding_practices-python-context-managers" title="Permanent link">&para;</a></h3>
<ul>
<li>Used with <code>with</code> statement</li>
<li>Ensures proper resource management</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DataConnection</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Opening connection&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Closing connection&quot;</span><span class="p">)</span>

<span class="c1"># Usage</span>
<span class="k">with</span> <span class="n">DataConnection</span><span class="p">()</span> <span class="k">as</span> <span class="n">conn</span><span class="p">:</span>
    <span class="c1"># Work with connection</span>
    <span class="k">pass</span>  <span class="c1"># Connection automatically closed</span>
</code></pre></div></li>
</ul>
<h3 id="grassroots-coding_practices-python-decorators">Decorators<a class="headerlink" href="#grassroots-coding_practices-python-decorators" title="Permanent link">&para;</a></h3>
<ul>
<li>Modify or enhance functions</li>
<li>Common uses:<ul>
<li>Logging</li>
<li>Timing</li>
<li>Access control</li>
</ul>
</li>
<li>Example:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">timer</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Function took </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>
    <span class="k">return</span> <span class="n">wrapper</span>

<span class="nd">@timer</span>
<span class="k">def</span><span class="w"> </span><span class="nf">slow_function</span><span class="p">():</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></li>
</ul>
<h2 id="grassroots-coding_practices-python-best-practices-for-interviews">Best Practices for Interviews<a class="headerlink" href="#grassroots-coding_practices-python-best-practices-for-interviews" title="Permanent link">&para;</a></h2>
<ol>
<li>Always discuss trade-offs between different approaches</li>
<li>Explain your thought process clearly</li>
<li>Consider edge cases</li>
<li>Be prepared to explain time and space complexity</li>
<li>Have examples ready for each concept</li>
<li>Be familiar with Python-specific implementations</li>
<li>Understand how concepts relate to data science tasks</li>
</ol>
<p>Remember to practice implementing these concepts and be ready to explain them in the context of real-world data science problems.</p></section><h1 class='nav-section-title-end'>Ended: Python</h1><h1 class='nav-section-title-end'>Ended: Coding practices</h1>
                        <h2 class='nav-section-title' id='section-machine-learning'>
                            Machine learning <a class='headerlink' href='#section-machine-learning' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="grassroots-machine_learning"><p>The following sections cover the basic ideas and tools for machine learning.</p>
<ul>
<li>1 - <a href="#grassroots-machine_learning-1_theoretical_fundamentals">Theoretical Fundamentals</a></li>
<li>2 - <a href="#grassroots-machine_learning-2_python_fundamentals">Python Fundamentals</a></li>
<li>3 - <a href="#grassroots-machine_learning-3_deep_learning">Deep Learning</a></li>
<li>4 - <a href="#grassroots-machine_learning-4_transformers">Transformers</a></li>
</ul></section>
                        <h3 class='nav-section-title' id='section-1-theoretical-fundamentals'>
                            1 theoretical fundamentals <a class='headerlink' href='#section-1-theoretical-fundamentals' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-machine_learning-1_theoretical_fundamentals"><h1 id="grassroots-machine_learning-1_theoretical_fundamentals-machine-learning-theory-classical-models-and-methods">Machine Learning Theory: Classical Models and Methods<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-machine-learning-theory-classical-models-and-methods" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-machine_learning-1_theoretical_fundamentals-linear-models">Linear Models<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-linear-models" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-linear-regression">Linear Regression<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-linear-regression" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Concept</strong>: Models relationship between dependent variable y and independent variables X using a linear equation</li>
<li><strong>Mathematical Form</strong>: y = Xβ + ε</li>
<li>
<p><strong>Loss Function</strong>: Mean Squared Error (MSE)</p>
<ul>
<li>L(β) = 1/n Σ(yi - xiᵀβ)²</li>
</ul>
</li>
<li>
<p><strong>Key Properties</strong>:</p>
<ul>
<li>Easy to interpret</li>
<li>Fast to train</li>
<li>Assumes linear relationship</li>
<li>Sensitive to outliers</li>
<li>Assumes independence of features</li>
</ul>
</li>
<li>
<p><strong>Regularization Variants</strong>:</p>
<ol>
<li>
<p><strong>Ridge Regression (L2)</strong></p>
<ul>
<li>Adds penalty term: λ||β||₂²</li>
<li>Shrinks coefficients toward zero</li>
<li>Good for handling multicollinearity</li>
</ul>
</li>
<li>
<p><strong>Lasso Regression (L1)</strong></p>
<ul>
<li>Adds penalty term: λ||β||₁</li>
<li>Can zero out coefficients</li>
<li>Performs feature selection</li>
</ul>
</li>
<li>
<p><strong>Elastic Net</strong></p>
<ul>
<li>Combines L1 and L2 penalties</li>
<li>Penalty term: λ₁||β||₁ + λ₂||β||₂²</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-logistic-regression">Logistic Regression<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-logistic-regression" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Concept</strong>: Models probability of binary outcome using logistic function</li>
<li><strong>Mathematical Form</strong>: P(y=1|X) = 1/(1 + e^(-Xβ))</li>
<li>
<p><strong>Loss Function</strong>: Binary Cross-Entropy</p>
<ul>
<li>L(β) = -1/n Σ(yi log(pi) + (1-yi)log(1-pi))</li>
</ul>
</li>
<li>
<p><strong>Key Properties</strong>:</p>
<ul>
<li>Outputs probabilities between 0 and 1</li>
<li>Can be extended to multiclass (one-vs-rest or multinomial)</li>
<li>Assumes linear decision boundary</li>
<li>Less sensitive to outliers than linear regression</li>
</ul>
</li>
</ul>
<h2 id="grassroots-machine_learning-1_theoretical_fundamentals-support-vector-machines-svm">Support Vector Machines (SVM)<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-support-vector-machines-svm" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-linear-svm">Linear SVM<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-linear-svm" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Concept</strong>: Finds optimal hyperplane maximizing margin between classes</li>
<li>
<p><strong>Mathematical Form</strong>: </p>
<ul>
<li>Primal: min 1/2||w||² subject to yi(wᵀxi + b) ≥ 1</li>
<li>Dual: max Σαi - 1/2Σ(αiαjyiyj<xi,xj>)</li>
</ul>
</li>
<li>
<p><strong>Key Properties</strong>:</p>
<ul>
<li>Maximizes margin between classes</li>
<li>Less prone to overfitting</li>
<li>Sensitive to feature scaling</li>
<li>Memory efficient (only stores support vectors)</li>
</ul>
</li>
</ul>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-kernel-svm">Kernel SVM<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-kernel-svm" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Concept</strong>: Projects data to higher dimensional space using kernel trick</li>
<li>
<p><strong>Common Kernels</strong>:</p>
<ol>
<li>
<p><strong>RBF (Gaussian)</strong>:</p>
<ul>
<li>K(x,y) = exp(-γ||x-y||²)</li>
<li>γ controls flexibility</li>
</ul>
</li>
<li>
<p><strong>Polynomial</strong>:</p>
<ul>
<li>K(x,y) = (γ<x,y> + r)^d</li>
<li>d is polynomial degree</li>
</ul>
</li>
<li>
<p><strong>Sigmoid</strong>:</p>
<ul>
<li>K(x,y) = tanh(γ<x,y> + r)</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>Key Properties</strong>:</p>
<ul>
<li>Can learn non-linear decision boundaries</li>
<li>Computationally intensive for large datasets</li>
<li>Requires careful kernel selection</li>
</ul>
</li>
</ul>
<h2 id="grassroots-machine_learning-1_theoretical_fundamentals-decision-trees">Decision Trees<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-decision-trees" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-basic-decision-tree">Basic Decision Tree<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-basic-decision-tree" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Concept</strong>: Recursively splits data based on feature values</li>
<li>
<p><strong>Splitting Criteria</strong>:</p>
<ol>
<li>
<p><strong>Classification</strong>:</p>
<ul>
<li>Gini Impurity: 1 - Σpi²</li>
<li>Entropy: -Σpi log(pi)</li>
</ul>
</li>
<li>
<p><strong>Regression</strong>:</p>
<ul>
<li>MSE: 1/n Σ(yi - ȳ)²</li>
<li>MAE: 1/n Σ|yi - ȳ|</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>Key Properties</strong>:</p>
<ul>
<li>Easy to interpret</li>
<li>Can handle non-linear relationships</li>
<li>No feature scaling needed</li>
<li>Prone to overfitting</li>
<li>Can capture feature interactions</li>
</ul>
</li>
<li>
<p><strong>Hyperparameters</strong>:</p>
<ul>
<li>Maximum depth</li>
<li>Minimum samples per leaf</li>
<li>Maximum features</li>
<li>Minimum impurity decrease</li>
</ul>
</li>
</ul>
<h2 id="grassroots-machine_learning-1_theoretical_fundamentals-ensemble-methods">Ensemble Methods<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-ensemble-methods" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-bagging-bootstrap-aggregating">Bagging (Bootstrap Aggregating)<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-bagging-bootstrap-aggregating" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-1_theoretical_fundamentals-random-forest">Random Forest<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-random-forest" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Concept</strong>: Averages predictions from multiple decorrelated trees</li>
<li>
<p><strong>Key Components</strong>:</p>
<ol>
<li>Bootstrap sampling of data</li>
<li>Random feature subset at each split</li>
<li>Averaging (regression) or voting (classification)</li>
</ol>
</li>
<li>
<p><strong>Key Properties</strong>:</p>
<ul>
<li>Reduces overfitting</li>
<li>Lower variance than single trees</li>
<li>Can estimate feature importance</li>
<li>Parallelizable</li>
</ul>
</li>
<li>
<p><strong>Mathematical Basis</strong>:</p>
<ul>
<li>Variance reduction through averaging</li>
<li>Error rate bound related to tree correlation</li>
</ul>
</li>
</ul>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-boosting">Boosting<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-boosting" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-1_theoretical_fundamentals-adaboost-adaptive-boosting">AdaBoost (Adaptive Boosting)<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-adaboost-adaptive-boosting" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Concept</strong>: Sequentially builds weak learners focusing on misclassified samples</li>
<li><strong>Algorithm</strong>:<ol>
<li>Initialize sample weights: wi = 1/n</li>
<li>For each iteration t:<ul>
<li>Train weak learner ht</li>
<li>Calculate error: εt = Σwi[yi ≠ ht(xi)]</li>
<li>Calculate weight: αt = 1/2 ln((1-εt)/εt)</li>
<li>Update sample weights</li>
</ul>
</li>
<li>Final prediction: H(x) = sign(Σαtht(x))</li>
</ol>
</li>
</ul>
<h4 id="grassroots-machine_learning-1_theoretical_fundamentals-gradient-boosting">Gradient Boosting<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-gradient-boosting" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Concept</strong>: Sequentially builds models to minimize loss function gradient</li>
<li>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Initialize prediction: F₀(x) = argmin(γ) Σ L(yi,γ)</li>
<li>For each iteration m:<ul>
<li>Calculate negative gradients: rim</li>
<li>Fit base learner hm to rim</li>
<li>Find optimal step size: γm</li>
<li>Update model: Fm(x) = Fm-₁(x) + γm hm(x)</li>
</ul>
</li>
</ol>
</li>
<li>
<p><strong>Variants</strong>:</p>
<ol>
<li>
<p><strong>XGBoost</strong>:</p>
<ul>
<li>Adds regularization term</li>
<li>Second-order approximation</li>
<li>Efficient implementation</li>
</ul>
</li>
<li>
<p><strong>LightGBM</strong>:</p>
<ul>
<li>Gradient-based one-side sampling</li>
<li>Exclusive feature bundling</li>
<li>Leaf-wise tree growth</li>
</ul>
</li>
<li>
<p><strong>CatBoost</strong>:</p>
<ul>
<li>Ordered boosting</li>
<li>Efficient categorical feature handling</li>
<li>Less prone to overfitting</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="grassroots-machine_learning-1_theoretical_fundamentals-model-selection-and-complexity">Model Selection and Complexity<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-model-selection-and-complexity" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-bias-variance-tradeoff">Bias-Variance Tradeoff<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-bias-variance-tradeoff" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Bias</strong>: Model's ability to capture underlying patterns</li>
<li><strong>Variance</strong>: Model's sensitivity to training data variation</li>
<li><strong>Total Error</strong>: Error = Bias² + Variance + Irreducible Error</li>
</ul>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-model-complexity-control">Model Complexity Control<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-model-complexity-control" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Regularization Methods</strong>:</p>
<ol>
<li>Parameter penalties (L1, L2)</li>
<li>Early stopping</li>
<li>Pruning (for trees)</li>
<li>Dropout (for neural networks)</li>
</ol>
</li>
<li>
<p><strong>Validation Strategies</strong>:</p>
<ol>
<li>Hold-out validation</li>
<li>K-fold cross-validation</li>
<li>Leave-one-out cross-validation</li>
<li>Stratified cross-validation</li>
</ol>
</li>
</ul>
<h2 id="grassroots-machine_learning-1_theoretical_fundamentals-feature-importance-and-model-interpretation">Feature Importance and Model Interpretation<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-feature-importance-and-model-interpretation" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-global-interpretation">Global Interpretation<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-global-interpretation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Feature Importance Methods</strong>:<ol>
<li>Coefficient magnitude (linear models)</li>
<li>Gini importance (trees)</li>
<li>Permutation importance</li>
<li>SHAP values</li>
</ol>
</li>
</ul>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-local-interpretation">Local Interpretation<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-local-interpretation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Methods</strong>:<ol>
<li>LIME (Local Interpretable Model-agnostic Explanations)</li>
<li>Individual SHAP values</li>
<li>Local feature importance</li>
</ol>
</li>
</ul>
<h2 id="grassroots-machine_learning-1_theoretical_fundamentals-assumptions-and-limitations">Assumptions and Limitations<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-assumptions-and-limitations" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-linear-models_1">Linear Models<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-linear-models_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Linearity</li>
<li>Independence</li>
<li>Homoscedasticity</li>
<li>Normal distribution (for inference)</li>
</ul>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-svm">SVM<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-svm" title="Permanent link">&para;</a></h3>
<ul>
<li>Feature scaling importance</li>
<li>Kernel selection</li>
<li>Computational complexity</li>
</ul>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-trees-and-ensembles">Trees and Ensembles<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-trees-and-ensembles" title="Permanent link">&para;</a></h3>
<ul>
<li>Prone to overfitting (single trees)</li>
<li>Memory requirements</li>
<li>Training time (especially boosting)</li>
<li>Black box nature (ensembles)</li>
</ul>
<h2 id="grassroots-machine_learning-1_theoretical_fundamentals-practical-considerations">Practical Considerations<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-practical-considerations" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-1_theoretical_fundamentals-model-selection-guidelines">Model Selection Guidelines<a class="headerlink" href="#grassroots-machine_learning-1_theoretical_fundamentals-model-selection-guidelines" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Dataset Size</strong>:</p>
<ul>
<li>Small: Linear models, SVM</li>
<li>Medium: Random Forest, Boosting</li>
<li>Large: Gradient Boosting, Deep Learning</li>
</ul>
</li>
<li>
<p><strong>Feature Types</strong>:</p>
<ul>
<li>Numerical: Any model</li>
<li>Categorical: Trees, CatBoost</li>
<li>Mixed: Trees, Ensemble methods</li>
</ul>
</li>
<li>
<p><strong>Interpretability Requirements</strong>:</p>
<ul>
<li>High: Linear models, Single trees</li>
<li>Medium: Random Forest</li>
<li>Low: Complex ensembles</li>
</ul>
</li>
<li>
<p><strong>Training Time Constraints</strong>:</p>
<ul>
<li>Fast: Linear models, Single trees</li>
<li>Medium: Random Forest</li>
<li>Slow: Boosting, Kernel SVM</li>
</ul>
</li>
</ol>
<p>Remember:
1. No free lunch theorem - no universally best model
2. Start simple, increase complexity as needed
3. Domain knowledge is crucial
4. Consider computational resources
5. Balance accuracy vs. interpretability</p></section><h1 class='nav-section-title-end'>Ended: 1 theoretical fundamentals</h1>
                        <h3 class='nav-section-title' id='section-2-python-fundamentals'>
                            2 python fundamentals <a class='headerlink' href='#section-2-python-fundamentals' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-machine_learning-2_python_fundamentals"><h1 id="grassroots-machine_learning-2_python_fundamentals-machine-learning-with-python">Machine Learning with Python<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-machine-learning-with-python" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-machine_learning-2_python_fundamentals-data-processing-and-analysis">Data Processing and Analysis<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-data-processing-and-analysis" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-2_python_fundamentals-loading-and-initial-data-exploration">Loading and Initial Data Exploration<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-loading-and-initial-data-exploration" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Load data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data.csv&#39;</span><span class="p">)</span>

<span class="c1"># Basic exploration</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>  <span class="c1"># Data types and missing values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>  <span class="c1"># Statistical summary</span>

<span class="c1"># Missing values analysis</span>
<span class="n">missing_values</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="n">missing_values</span><span class="p">[</span><span class="n">missing_values</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Correlation analysis</span>
<span class="n">correlation_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-2_python_fundamentals-data-cleaning-and-preprocessing">Data Cleaning and Preprocessing<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-data-cleaning-and-preprocessing" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-2_python_fundamentals-handling-missing-values">Handling Missing Values<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-handling-missing-values" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Strategy 1: Remove rows with missing values</span>
<span class="n">df_cleaned</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Strategy 2: Imputation</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span><span class="p">,</span> <span class="n">KNNImputer</span>

<span class="c1"># Mean imputation</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;column&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;column&#39;</span><span class="p">]])</span>

<span class="c1"># KNN imputation for more complex relationships</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">df_imputed</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span>
<span class="p">)</span>

<span class="c1"># Strategy 3: Advanced imputation</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.experimental</span><span class="w"> </span><span class="kn">import</span> <span class="n">enable_iterative_imputer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">IterativeImputer</span>
<span class="c1"># Uses the relationships between features to impute values</span>
<span class="n">imputer</span> <span class="o">=</span> <span class="n">IterativeImputer</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-2_python_fundamentals-feature-scaling">Feature Scaling<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-feature-scaling" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">,</span> <span class="n">RobustScaler</span>

<span class="c1"># Standardization (z-score)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Min-Max scaling</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Robust scaling (handles outliers better)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">RobustScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-2_python_fundamentals-handling-categorical-variables">Handling Categorical Variables<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-handling-categorical-variables" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># One-hot encoding</span>
<span class="n">X_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;categorical_column&#39;</span><span class="p">])</span>

<span class="c1"># Label encoding</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;encoded_column&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;categorical_column&#39;</span><span class="p">])</span>

<span class="c1"># Ordinal encoding</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">()</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-2_python_fundamentals-feature-engineering">Feature Engineering<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-feature-engineering" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-2_python_fundamentals-creating-new-features">Creating New Features<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-creating-new-features" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="c1"># Polynomial features</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Interaction terms</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;interaction&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;feature1&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;feature2&#39;</span><span class="p">]</span>

<span class="c1"># Date features</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;month&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">month</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;day_of_week&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">dayofweek</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-2_python_fundamentals-feature-selection">Feature Selection<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-feature-selection" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">f_classif</span><span class="p">,</span> <span class="n">RFE</span><span class="p">,</span> <span class="n">SelectFromModel</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Statistical selection</span>
<span class="n">selector</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">score_func</span><span class="o">=</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Recursive feature elimination</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">rfe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># L1-based selection</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lasso</span>
<span class="n">selector</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">Lasso</span><span class="p">())</span>
<span class="n">X_selected</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>
<h2 id="grassroots-machine_learning-2_python_fundamentals-model-training-and-evaluation">Model Training and Evaluation<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-model-training-and-evaluation" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-2_python_fundamentals-train-test-split">Train-Test Split<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-train-test-split" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Simple split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Stratified split for classification</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-2_python_fundamentals-cross-validation">Cross-Validation<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-cross-validation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">StratifiedKFold</span><span class="p">,</span>
    <span class="n">TimeSeriesSplit</span>
<span class="p">)</span>

<span class="c1"># K-Fold CV</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>

<span class="c1"># Stratified K-Fold for classification</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>

<span class="c1"># Time series CV</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">TimeSeriesSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-2_python_fundamentals-model-training-and-hyperparameter-tuning">Model Training and Hyperparameter Tuning<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-model-training-and-hyperparameter-tuning" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">RandomizedSearchCV</span>
<span class="p">)</span>

<span class="c1"># Grid search</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Random search</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">randint</span>
<span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_dist</span><span class="p">,</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-2_python_fundamentals-model-evaluation">Model Evaluation<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-model-evaluation" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-2_python_fundamentals-classification-metrics">Classification Metrics<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-classification-metrics" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span>
    <span class="n">f1_score</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span>
    <span class="n">classification_report</span>
<span class="p">)</span>

<span class="c1"># Basic metrics</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision: </span><span class="si">{</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">,</span><span class="w"> </span><span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recall: </span><span class="si">{</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">,</span><span class="w"> </span><span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F1 Score: </span><span class="si">{</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">,</span><span class="w"> </span><span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ROC-AUC for binary classification</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ROC-AUC: </span><span class="si">{</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_prob</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Detailed classification report</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-2_python_fundamentals-regression-metrics">Regression Metrics<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-regression-metrics" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span><span class="p">,</span>
    <span class="n">r2_score</span><span class="p">,</span> <span class="n">mean_absolute_percentage_error</span>
<span class="p">)</span>

<span class="c1"># Calculate metrics</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RMSE: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">))</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MAE: </span><span class="si">{</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R2 Score: </span><span class="si">{</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MAPE: </span><span class="si">{</span><span class="n">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-2_python_fundamentals-model-interpretability">Model Interpretability<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-model-interpretability" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.inspection</span><span class="w"> </span><span class="kn">import</span> <span class="n">permutation_importance</span>

<span class="c1"># SHAP values</span>
<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Feature importance</span>
<span class="n">feature_imp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;feature&#39;</span><span class="p">:</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="s1">&#39;importance&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">}</span>
<span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;importance&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Permutation importance</span>
<span class="n">perm_importance</span> <span class="o">=</span> <span class="n">permutation_importance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div>
<h2 id="grassroots-machine_learning-2_python_fundamentals-best-practices">Best Practices<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-2_python_fundamentals-pipeline-creation">Pipeline Creation<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-pipeline-creation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.compose</span><span class="w"> </span><span class="kn">import</span> <span class="n">ColumnTransformer</span>

<span class="c1"># Define preprocessing for numerical and categorical columns</span>
<span class="n">numeric_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;feature1&#39;</span><span class="p">,</span> <span class="s1">&#39;feature2&#39;</span><span class="p">]</span>
<span class="n">categorical_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;feature3&#39;</span><span class="p">,</span> <span class="s1">&#39;feature4&#39;</span><span class="p">]</span>

<span class="n">numeric_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())</span>
<span class="p">])</span>

<span class="n">categorical_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="s1">&#39;missing&#39;</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;onehot&#39;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># Combine preprocessing steps</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">numeric_transformer</span><span class="p">,</span> <span class="n">numeric_features</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">categorical_transformer</span><span class="p">,</span> <span class="n">categorical_features</span><span class="p">)</span>
    <span class="p">])</span>

<span class="c1"># Create full pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;preprocessor&#39;</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())</span>
<span class="p">])</span>

<span class="c1"># Use pipeline</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-2_python_fundamentals-model-persistence">Model Persistence<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-model-persistence" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">joblib</span>

<span class="c1"># Save model</span>
<span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;model.joblib&#39;</span><span class="p">)</span>

<span class="c1"># Load model</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;model.joblib&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-2_python_fundamentals-cross-validation-best-practices">Cross-Validation Best Practices<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-cross-validation-best-practices" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Data Leakage Prevention</strong>
<div class="highlight"><pre><span></span><code><span class="c1"># Wrong: Scaling before splitting</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Correct: Scaling after splitting</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Nested Cross-Validation</strong>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">KFold</span>

<span class="k">def</span><span class="w"> </span><span class="nf">nested_cv</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">inner_cv</span><span class="p">,</span> <span class="n">outer_cv</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">):</span>
    <span class="n">outer_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">outer_cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

        <span class="c1"># Inner CV for parameter tuning</span>
        <span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">inner_cv</span>
        <span class="p">)</span>
        <span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Evaluate on test set</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        <span class="n">outer_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outer_scores</span>
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-machine_learning-2_python_fundamentals-common-pitfalls-and-solutions">Common Pitfalls and Solutions<a class="headerlink" href="#grassroots-machine_learning-2_python_fundamentals-common-pitfalls-and-solutions" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Class Imbalance</strong>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">imblearn.over_sampling</span><span class="w"> </span><span class="kn">import</span> <span class="n">SMOTE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">imblearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span> <span class="k">as</span> <span class="n">ImbPipeline</span>

<span class="c1"># Handling imbalanced data</span>
<span class="n">smote</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_resampled</span><span class="p">,</span> <span class="n">y_resampled</span> <span class="o">=</span> <span class="n">smote</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Or use pipeline with SMOTE</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">ImbPipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;preprocessor&#39;</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;smote&#39;</span><span class="p">,</span> <span class="n">SMOTE</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())</span>
<span class="p">])</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Feature Scaling for Tree-Based Models</strong>
<div class="highlight"><pre><span></span><code><span class="c1"># Tree-based models don&#39;t require scaling</span>
<span class="n">tree_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;preprocessor&#39;</span><span class="p">,</span> <span class="n">categorical_transformer</span><span class="p">),</span>  <span class="c1"># Only handle categorical</span>
    <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())</span>
<span class="p">])</span>

<span class="c1"># Non-tree models need scaling</span>
<span class="n">linear_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;preprocessor&#39;</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">),</span>  <span class="c1"># Handle both numeric and categorical</span>
    <span class="p">(</span><span class="s1">&#39;classifier&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())</span>
<span class="p">])</span>
</code></pre></div></p>
</li>
</ol>
<p>Remember:
1. Always split data before preprocessing
2. Use pipelines to prevent data leakage
3. Choose appropriate cross-validation strategy
4. Consider domain-specific requirements
5. Monitor for overfitting/underfitting
6. Document preprocessing steps and parameters</p></section><h1 class='nav-section-title-end'>Ended: 2 python fundamentals</h1>
                        <h3 class='nav-section-title' id='section-3-deep-learning'>
                            3 deep learning <a class='headerlink' href='#section-3-deep-learning' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-machine_learning-3_deep_learning"><h1 id="grassroots-machine_learning-3_deep_learning-deep-learning-from-theory-to-practice">Deep Learning: From Theory to Practice<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-deep-learning-from-theory-to-practice" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-machine_learning-3_deep_learning-neural-network-foundations">Neural Network Foundations<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-neural-network-foundations" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-3_deep_learning-basic-architecture">Basic Architecture<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-basic-architecture" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Neurons (Units)</strong></p>
<ul>
<li>Sum of weighted inputs: z = Σ(wixi) + b</li>
<li>Activation function: a = f(z)</li>
</ul>
</li>
<li>
<p><strong>Common Activation Functions</strong></p>
<ol>
<li>
<p><strong>ReLU</strong>: f(x) = max(0, x)</p>
<ul>
<li>Fast to compute</li>
<li>Helps with vanishing gradient</li>
<li>Can cause "dying ReLU" problem</li>
</ul>
</li>
<li>
<p><strong>Sigmoid</strong>: f(x) = 1/(1 + e^(-x))</p>
<ul>
<li>Outputs between [0,1]</li>
<li>Can cause vanishing gradient</li>
<li>Used in binary classification output</li>
</ul>
</li>
<li>
<p><strong>Tanh</strong>: f(x) = (e^x - e^(-x))/(e^x + e^(-x))</p>
<ul>
<li>Outputs between [-1,1]</li>
<li>Zero-centered</li>
<li>Still has vanishing gradient issues</li>
</ul>
</li>
<li>
<p><strong>LeakyReLU</strong>: f(x) = max(αx, x)</p>
<ul>
<li>Prevents dying ReLU</li>
<li>α typically small (e.g., 0.01)</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="grassroots-machine_learning-3_deep_learning-forward-propagation">Forward Propagation<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-forward-propagation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Layer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_neurons</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-3_deep_learning-backpropagation">Backpropagation<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-backpropagation" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Chain Rule Application</strong></p>
<ul>
<li>∂L/∂w = ∂L/∂a * ∂a/∂z * ∂z/∂w</li>
<li>∂L/∂b = ∂L/∂a * ∂a/∂z * ∂z/∂b</li>
</ul>
</li>
<li>
<p><strong>Gradient Descent</strong></p>
<ul>
<li>w = w - α * ∂L/∂w</li>
<li>b = b - α * ∂L/∂b</li>
</ul>
</li>
</ul>
<h2 id="grassroots-machine_learning-3_deep_learning-loss-functions-and-optimization">Loss Functions and Optimization<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-loss-functions-and-optimization" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-3_deep_learning-common-loss-functions">Common Loss Functions<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-common-loss-functions" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-3_deep_learning-classification">Classification<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-classification" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> <span class="o">+</span> 
                   <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">categorical_cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-3_deep_learning-regression">Regression<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-regression" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">huber_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span>
    <span class="n">is_small_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">delta</span>
    <span class="n">squared_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">error</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">linear_loss</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">delta</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_small_error</span><span class="p">,</span> <span class="n">squared_loss</span><span class="p">,</span> <span class="n">linear_loss</span><span class="p">))</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-3_deep_learning-optimizers">Optimizers<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-optimizers" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-3_deep_learning-sgd-with-momentum">SGD with Momentum<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-sgd-with-momentum" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SGDMomentum</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">velocity</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">velocity</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">velocity</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> 
                           <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">velocity</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">velocity</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-</span> 
                                <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
            <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">velocity</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-3_deep_learning-adam">Adam<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-adam" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Adam</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">beta2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="c1"># Momentum</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> 
                          <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
            <span class="c1"># RMSprop</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+</span> 
                          <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Bias correction</span>
            <span class="n">m_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>
            <span class="n">v_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>

            <span class="c1"># Update parameters</span>
            <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m_hat</span> <span class="o">/</span> 
                          <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v_hat</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">))</span>
</code></pre></div>
<h2 id="grassroots-machine_learning-3_deep_learning-deep-learning-architectures">Deep Learning Architectures<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-deep-learning-architectures" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-3_deep_learning-convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-convolutional-neural-networks-cnn" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-3_deep_learning-basic-components">Basic Components<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-basic-components" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Convolutional Layer</strong>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Conv2D</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filters</span> <span class="o">=</span> <span class="n">filters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Implement convolution operation</span>
        <span class="k">pass</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Pooling Layer</strong>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MaxPool2D</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool_size</span> <span class="o">=</span> <span class="n">pool_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span> <span class="ow">or</span> <span class="n">pool_size</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Implement max pooling</span>
        <span class="k">pass</span>
</code></pre></div></p>
</li>
</ol>
<h4 id="grassroots-machine_learning-3_deep_learning-common-architectures">Common Architectures<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-common-architectures" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>VGG</strong></p>
<ul>
<li>Stack of 3x3 convolutions</li>
<li>Max pooling reduces spatial dimensions</li>
<li>Dense layers for classification</li>
</ul>
</li>
<li>
<p><strong>ResNet</strong></p>
<ul>
<li>Skip connections</li>
<li>Batch normalization</li>
<li>Deep architecture (50+ layers)</li>
</ul>
</li>
</ol>
<h3 id="grassroots-machine_learning-3_deep_learning-recurrent-neural-networks-rnn">Recurrent Neural Networks (RNN)<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-recurrent-neural-networks-rnn" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-3_deep_learning-basic-rnn-cell">Basic RNN Cell<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-basic-rnn-cell" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">RNNCell</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wxh</span><span class="p">)</span> <span class="o">+</span> 
                      <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h_prev</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Whh</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bh</span><span class="p">)</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-3_deep_learning-lstm-cell">LSTM Cell<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-lstm-cell" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LSTMCell</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="c1"># Initialize gates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_gate</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_gate</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_gate</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_gate</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">,</span> <span class="n">c_prev</span><span class="p">):</span>
        <span class="c1"># Concatenate input and previous hidden state</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Gate computations</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forget_gate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_gate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_gate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>
        <span class="n">c_tilde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cell_gate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">combined</span><span class="p">))</span>

        <span class="c1"># Update cell and hidden state</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">f</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">c_tilde</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">o</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-3_deep_learning-transformers">Transformers<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-transformers" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-3_deep_learning-self-attention-mechanism">Self-Attention Mechanism<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-self-attention-mechanism" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_scores</span> <span class="o">+=</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-3_deep_learning-multi-head-attention">Multi-Head Attention<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-multi-head-attention" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wk</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">wv</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</code></pre></div>
<h2 id="grassroots-machine_learning-3_deep_learning-advanced-topics">Advanced Topics<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-advanced-topics" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-3_deep_learning-regularization-techniques">Regularization Techniques<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-regularization-techniques" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-3_deep_learning-dropout">Dropout<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-dropout" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span> <span class="o">/</span> <span class="n">keep_prob</span>
</code></pre></div>
<h4 id="grassroots-machine_learning-3_deep_learning-batch-normalization">Batch Normalization<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-batch-normalization" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">BatchNorm</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="c1"># Update running statistics</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> 
                               <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">mean</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> 
                              <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">var</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span>
            <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span>

        <span class="c1"># Normalize</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_norm</span>
</code></pre></div>
<h3 id="grassroots-machine_learning-3_deep_learning-transfer-learning">Transfer Learning<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-transfer-learning" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Feature Extraction</strong></p>
<ul>
<li>Freeze pre-trained layers</li>
<li>Add new classification head</li>
<li>Train only new layers</li>
</ul>
</li>
<li>
<p><strong>Fine-Tuning</strong></p>
<ul>
<li>Start with pre-trained model</li>
<li>Unfreeze some/all layers</li>
<li>Train with small learning rate</li>
</ul>
</li>
</ol>
<h3 id="grassroots-machine_learning-3_deep_learning-model-deployment">Model Deployment<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-model-deployment" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-machine_learning-3_deep_learning-model-optimization">Model Optimization<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-model-optimization" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Quantization</strong></p>
<ul>
<li>Reduce precision of weights</li>
<li>Integer-only inference</li>
<li>Reduced memory footprint</li>
</ul>
</li>
<li>
<p><strong>Pruning</strong></p>
<ul>
<li>Remove unnecessary connections</li>
<li>Maintain accuracy</li>
<li>Reduce model size</li>
</ul>
</li>
<li>
<p><strong>Knowledge Distillation</strong></p>
<ul>
<li>Teacher-student architecture</li>
<li>Transfer knowledge to smaller model</li>
<li>Maintain performance</li>
</ul>
</li>
</ol>
<h2 id="grassroots-machine_learning-3_deep_learning-best-practices">Best Practices<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-3_deep_learning-training-tips">Training Tips<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-training-tips" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Learning Rate Selection</strong></p>
<ul>
<li>Use learning rate finder</li>
<li>Implement learning rate scheduling</li>
<li>Monitor training dynamics</li>
</ul>
</li>
<li>
<p><strong>Batch Size Selection</strong></p>
<ul>
<li>Memory constraints</li>
<li>Training stability</li>
<li>Generalization impact</li>
</ul>
</li>
<li>
<p><strong>Initialization</strong></p>
<ul>
<li>Xavier/Glorot initialization</li>
<li>He initialization for ReLU</li>
<li>Careful with deep networks</li>
</ul>
</li>
</ol>
<h3 id="grassroots-machine_learning-3_deep_learning-debugging-strategies">Debugging Strategies<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-debugging-strategies" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Loss Not Decreasing</strong></p>
<ul>
<li>Check gradients</li>
<li>Verify data preprocessing</li>
<li>Inspect learning rate</li>
</ul>
</li>
<li>
<p><strong>Overfitting</strong></p>
<ul>
<li>Add regularization</li>
<li>Increase training data</li>
<li>Reduce model capacity</li>
</ul>
</li>
<li>
<p><strong>Underfitting</strong></p>
<ul>
<li>Increase model capacity</li>
<li>Reduce regularization</li>
<li>Check for data issues</li>
</ul>
</li>
</ol>
<h3 id="grassroots-machine_learning-3_deep_learning-performance-optimization">Performance Optimization<a class="headerlink" href="#grassroots-machine_learning-3_deep_learning-performance-optimization" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Memory Management</strong></p>
<ul>
<li>Gradient accumulation</li>
<li>Mixed precision training</li>
<li>Efficient data loading</li>
</ul>
</li>
<li>
<p><strong>Training Speed</strong></p>
<ul>
<li>Parallel processing</li>
<li>GPU utilization</li>
<li>Efficient data pipeline</li>
</ul>
</li>
</ol>
<p>Remember:
1. Start simple and gradually increase complexity
2. Monitor training metrics carefully
3. Use appropriate regularization techniques
4. Consider computational resources
5. Test thoroughly before deployment</p></section><h1 class='nav-section-title-end'>Ended: 3 deep learning</h1>
                        <h3 class='nav-section-title' id='section-4-transformers'>
                            4 transformers <a class='headerlink' href='#section-4-transformers' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-machine_learning-4_transformers"><h1 id="grassroots-machine_learning-4_transformers-understanding-transformers-and-the-attention-mechanism">Understanding Transformers and the Attention Mechanism<a class="headerlink" href="#grassroots-machine_learning-4_transformers-understanding-transformers-and-the-attention-mechanism" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-machine_learning-4_transformers-introduction">Introduction<a class="headerlink" href="#grassroots-machine_learning-4_transformers-introduction" title="Permanent link">&para;</a></h2>
<p>Transformers have revolutionized natural language processing and many other domains since their introduction in the "Attention Is All You Need" paper (Vaswhatever et al., 2017). This chapter breaks down their core mechanisms and explains how they process information.</p>
<h2 id="grassroots-machine_learning-4_transformers-core-architecture">Core Architecture<a class="headerlink" href="#grassroots-machine_learning-4_transformers-core-architecture" title="Permanent link">&para;</a></h2>
<p>The Transformer architecture consists of two main components:
1. An encoder that processes the input sequence
2. A decoder that generates the output sequence</p>
<p>Both components are built from stacks of identical layers, each containing:
- Multi-head self-attention mechanisms
- Feed-forward neural networks
- Layer normalization and residual connections</p>
<h2 id="grassroots-machine_learning-4_transformers-the-attention-mechanism-step-by-step">The Attention Mechanism: Step by Step<a class="headerlink" href="#grassroots-machine_learning-4_transformers-the-attention-mechanism-step-by-step" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-machine_learning-4_transformers-1-input-embedding-and-positional-encoding">1. Input Embedding and Positional Encoding<a class="headerlink" href="#grassroots-machine_learning-4_transformers-1-input-embedding-and-positional-encoding" title="Permanent link">&para;</a></h3>
<ul>
<li>First, input tokens are converted to embeddings (dense vectors)</li>
<li>Positional encodings are added to maintain sequence order information</li>
<li>These use sine and cosine functions of different frequencies:
  pos_encoding(pos, 2i) = sin(pos/10000^(2i/d_model))
  pos_encoding(pos, 2i+1) = cos(pos/10000^(2i/d_model))</li>
</ul>
<h3 id="grassroots-machine_learning-4_transformers-2-query-key-and-value-computation">2. Query, Key, and Value Computation<a class="headerlink" href="#grassroots-machine_learning-4_transformers-2-query-key-and-value-computation" title="Permanent link">&para;</a></h3>
<p>The attention mechanism uses three main components:
- Query (Q): What we're looking for
- Key (K): What we match against
- Value (V): What we want to retrieve</p>
<p>These are computed by multiplying the input embeddings by learned weight matrices:
<div class="highlight"><pre><span></span><code>Q = input × W_Q
K = input × W_K
V = input × W_V
</code></pre></div></p>
<h3 id="grassroots-machine_learning-4_transformers-3-attention-score-calculation">3. Attention Score Calculation<a class="headerlink" href="#grassroots-machine_learning-4_transformers-3-attention-score-calculation" title="Permanent link">&para;</a></h3>
<p>The attention scores are computed in several steps:</p>
<p>a. <strong>Compatibility Function</strong>
<div class="highlight"><pre><span></span><code>scores = (Q × K^T) / √d_k
</code></pre></div>
where d_k is the dimension of the key vectors, and the division by √d_k prevents values from becoming too large for the softmax.</p>
<p>b. <strong>Masking (if necessary)</strong>
In decoder self-attention or for padding tokens:
- Set unwanted attention scores to -infinity
- This ensures these positions have zero weight after softmax</p>
<p>c. <strong>Softmax Application</strong>
<div class="highlight"><pre><span></span><code>attention_weights = softmax(scores)
</code></pre></div>
This normalizes the scores into probabilities that sum to 1.</p>
<h3 id="grassroots-machine_learning-4_transformers-4-computing-the-output">4. Computing the Output<a class="headerlink" href="#grassroots-machine_learning-4_transformers-4-computing-the-output" title="Permanent link">&para;</a></h3>
<p>The final attention output is computed by:
<div class="highlight"><pre><span></span><code>attention_output = attention_weights × V
</code></pre></div></p>
<h3 id="grassroots-machine_learning-4_transformers-5-multi-head-attention">5. Multi-Head Attention<a class="headerlink" href="#grassroots-machine_learning-4_transformers-5-multi-head-attention" title="Permanent link">&para;</a></h3>
<p>Instead of performing attention once, Transformers use multiple attention heads:</p>
<ol>
<li>Each head has its own Q, K, V projections</li>
<li>Compute attention independently for each head</li>
<li>Concatenate the results</li>
<li>Project back to the model dimension:
<div class="highlight"><pre><span></span><code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h) × W_O
where head_i = Attention(Q × W_Q_i, K × W_K_i, V × W_V_i)
</code></pre></div></li>
</ol>
<h2 id="grassroots-machine_learning-4_transformers-feed-forward-networks-and-residual-connections">Feed-Forward Networks and Residual Connections<a class="headerlink" href="#grassroots-machine_learning-4_transformers-feed-forward-networks-and-residual-connections" title="Permanent link">&para;</a></h2>
<p>After attention, each sub-layer contains:</p>
<ol>
<li>
<p><strong>Layer Normalization</strong>
<div class="highlight"><pre><span></span><code>norm(x) = γ × (x - μ)/σ + β
</code></pre></div>
where μ and σ are mean and standard deviation, γ and β are learned parameters</p>
</li>
<li>
<p><strong>Feed-Forward Network</strong>
<div class="highlight"><pre><span></span><code>FFN(x) = GELU(xW_1 + b_1)W_2 + b_2
</code></pre></div></p>
</li>
</ol>
<p>where:</p>
<p>GELU is the Gaussian Error Linear Unit activation function
W_1 transforms from model dimension to intermediate dimension (typically 4x larger)
W_2 transforms back to model dimension
b_1 and b_2 are bias terms</p>
<p>Note: While earlier transformer implementations used ReLU (max(0, x)), modern transformers typically use GELU activation functions which provide smoother gradients.</p>
<ol>
<li><strong>Residual Connection</strong>
<div class="highlight"><pre><span></span><code>output = LayerNorm(x + Sublayer(x))
</code></pre></div></li>
</ol>
<h2 id="grassroots-machine_learning-4_transformers-practical-example-computing-self-attention">Practical Example: Computing Self-Attention<a class="headerlink" href="#grassroots-machine_learning-4_transformers-practical-example-computing-self-attention" title="Permanent link">&para;</a></h2>
<p>Let's walk through a simplified example with 4-dimensional vectors:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Input embeddings (batch_size=1, seq_len=3, d_model=4)</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># First token</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Second token</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>   <span class="c1"># Third token</span>
<span class="p">]</span>

<span class="c1"># Project to Q, K, V (simplified weights)</span>
<span class="n">W_Q</span> <span class="o">=</span> <span class="n">W_K</span> <span class="o">=</span> <span class="n">W_V</span> <span class="o">=</span> <span class="n">I</span>  <span class="c1"># Identity matrix for simplicity</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">=</span> <span class="n">V</span> <span class="o">=</span> <span class="n">X</span>        <span class="c1"># In practice, these would be different</span>

<span class="c1"># Compute attention scores</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">Q</span> <span class="err">×</span> <span class="n">K</span><span class="o">^</span><span class="n">T</span> <span class="o">/</span> <span class="err">√</span><span class="mi">4</span>
<span class="c1"># Results in:</span>
<span class="c1"># [[1.0, 0.5, 0.5],</span>
<span class="c1">#  [0.5, 1.0, 0.5],</span>
<span class="c1">#  [0.5, 0.5, 1.0]]</span>

<span class="c1"># Apply softmax</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="c1"># Results in attention weights focusing most on matching positions</span>

<span class="c1"># Final output</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">weights</span> <span class="err">×</span> <span class="n">V</span>
<span class="c1"># Each position now contains a weighted mix of values</span>
</code></pre></div>
<h2 id="grassroots-machine_learning-4_transformers-training-and-optimization">Training and Optimization<a class="headerlink" href="#grassroots-machine_learning-4_transformers-training-and-optimization" title="Permanent link">&para;</a></h2>
<p>Transformers are typically trained using:
- Adam optimizer with β1 = 0.9, β2 = 0.98, ε = 10^-9
- Learning rate scheduling with warmup
- Dropout for regularization
- Label smoothing</p>
<p>The loss function varies by task:
- Cross-entropy for classification/language modeling
- Custom losses for specific applications</p>
<h2 id="grassroots-machine_learning-4_transformers-common-variations-and-improvements">Common Variations and Improvements<a class="headerlink" href="#grassroots-machine_learning-4_transformers-common-variations-and-improvements" title="Permanent link">&para;</a></h2>
<p>Modern transformers often include:
- Relative positional encoding
- Sparse attention patterns
- Parameter sharing across layers
- Efficient attention approximations</p>
<p>These modifications help with:
- Longer sequence handling
- Computational efficiency
- Task-specific requirements</p>
<h2 id="grassroots-machine_learning-4_transformers-references">References<a class="headerlink" href="#grassroots-machine_learning-4_transformers-references" title="Permanent link">&para;</a></h2>
<p>Note: While this technical explanation is accurate, you should verify specific implementation details against primary sources and documentation for your particular use case.</p></section><h1 class='nav-section-title-end'>Ended: 4 transformers</h1><h1 class='nav-section-title-end'>Ended: Machine learning</h1>
                        <h2 class='nav-section-title' id='section-statistics'>
                            Statistics <a class='headerlink' href='#section-statistics' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="grassroots-statistics"><h1 id="grassroots-statistics-core-statistical-concepts-for-data-science-interviews">Core Statistical Concepts for Data Science Interviews<a class="headerlink" href="#grassroots-statistics-core-statistical-concepts-for-data-science-interviews" title="Permanent link">&para;</a></h1>
<p>In this section, we introduce the main tool for statistical analysis.</p>
<ul>
<li>1 - <a href="#grassroots-statistics-1_descriptive_statistics">Descriptive statistiques</a></li>
<li>2 - <a href="#grassroots-statistics-2_probability_distributions">Probability distributions</a></li>
<li>3 - <a href="#grassroots-statistics-3_computational_rules">Computational rules</a></li>
<li>4 - <a href="#grassroots-statistics-4_statistical_inference">Statistical inference</a></li>
<li>5 - <a href="#grassroots-statistics-5_regression_analysis">Regression analysis</a></li>
<li>6 - <a href="#grassroots-statistics-6_experimental_design">Experimental design</a></li>
<li>7 - <a href="#grassroots-statistics-7_bayesian_statistics">Bayesian statistics</a></li>
<li>8 - <a href="#grassroots-statistics-8_advanced_topics">Advanced topics</a></li>
</ul>
<h2 id="grassroots-statistics-common-interview-questions">Common Interview Questions<a class="headerlink" href="#grassroots-statistics-common-interview-questions" title="Permanent link">&para;</a></h2>
<details>
<summary>When would you use a t-test vs z-test?</summary>
<p>Let me break down the key differences between t-tests and z-tests and explain when to use each one:</p>
<h3 id="grassroots-statistics-key-distinctions">Key Distinctions<a class="headerlink" href="#grassroots-statistics-key-distinctions" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>Population Standard Deviation</p>
<ul>
<li>Z-test: Used when we KNOW the population standard deviation (σ)</li>
<li>T-test: Used when we DON'T know the population standard deviation and must estimate it using sample standard deviation (s)</li>
</ul>
</li>
<li>
<p>Sample Size</p>
<ul>
<li>Z-test: Generally used for large samples (n &gt; 30)</li>
<li>T-test: Better for small samples (n &lt; 30) because it accounts for extra uncertainty in estimating the standard deviation</li>
</ul>
</li>
<li>
<p>Distribution</p>
<ul>
<li>Z-test: Assumes data follows a normal distribution</li>
<li>T-test: Uses Student's t-distribution, which has heavier tails than normal distribution to account for additional uncertainty</li>
</ul>
</li>
</ol>
<h3 id="grassroots-statistics-practical-examples">Practical Examples<a class="headerlink" href="#grassroots-statistics-practical-examples" title="Permanent link">&para;</a></h3>
<p><strong>Scenario 1: Quality Control in Large Manufacturing Plant</strong>
- Testing widget weights
- Years of historical data available
- Known population standard deviation
- Large daily samples
→ Use Z-test because you know σ and have large samples</p>
<p><strong>Scenario 2: Medical Research Study</strong>
- Testing new drug effectiveness
- Small patient group (n=20)
- No known population standard deviation
- Need to estimate variance from sample
→ Use T-test because of small sample size and unknown σ</p>
<h3 id="grassroots-statistics-mathematical-details">Mathematical Details<a class="headerlink" href="#grassroots-statistics-mathematical-details" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-heavier-tails-of-t-distribution">Heavier Tails of T-Distribution<a class="headerlink" href="#grassroots-statistics-heavier-tails-of-t-distribution" title="Permanent link">&para;</a></h4>
<p>The t-distribution is defined as:</p>
<div class="arithmatex">\[
t = \frac{Z}{\sqrt{V/n}}
\]</div>
<p>where:
- Z follows N(0,1)
- V follows χ²(n) (chi-square with n degrees of freedom)
- Z and V are independent</p>
<p>The probability density function (PDF) of t-distribution with ν degrees of freedom is:</p>
<div class="arithmatex">\[
f(t) = \frac{\Gamma(\frac{\nu + 1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1 + \frac{t^2}{\nu})^{-\frac{\nu + 1}{2}}
\]</div>
<p>Compare this to the normal distribution PDF:</p>
<div class="arithmatex">\[
f(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
\]</div>
<h4 id="grassroots-statistics-convergence-to-normal-distribution">Convergence to Normal Distribution<a class="headerlink" href="#grassroots-statistics-convergence-to-normal-distribution" title="Permanent link">&para;</a></h4>
<p>As n → ∞, we can prove convergence using:</p>
<ol>
<li>
<p>The Central Limit Theorem for V/n:
    $$
    \frac{V/n - 1}{\sqrt{2/n}} \xrightarrow{d} N(0,1)
    $$</p>
</li>
<li>
<p>Therefore, as n → ∞:
    $$
    \sqrt{V/n} \xrightarrow{p} 1
    $$</p>
</li>
<li>
<p>Thus:
    $$
    t = \frac{Z}{\sqrt{V/n}} \xrightarrow{d} Z \sim N(0,1)
    $$</p>
</li>
</ol>
<h4 id="grassroots-statistics-power-analysis">Power Analysis<a class="headerlink" href="#grassroots-statistics-power-analysis" title="Permanent link">&para;</a></h4>
<p>The power function for a z-test:
$$
\pi_Z(\mu) = 1 - \Phi(z_{α/2} - \frac{\mu - \mu_0}{\sigma/\sqrt{n}}) + \Phi(-z_{α/2} - \frac{\mu - \mu_0}{\sigma/\sqrt{n}})
$$</p>
<p>The power function for a t-test:
$$
\pi_T(\mu) = 1 - F_t(t_{α/2,n-1} - \frac{\mu - \mu_0}{s/\sqrt{n}}) + F_t(-t_{α/2,n-1} - \frac{\mu - \mu_0}{s/\sqrt{n}})
$$</p>
<p>where:
- Φ is the standard normal CDF
- F_t is the t-distribution CDF
- z_{α/2} is the normal critical value
- t_{α/2,n-1} is the t critical value</p>
</details>
<details>
<summary>How do you handle missing data?</summary>
<p>Content for handling missing data...</p>
</details>
<details>
<summary>What's the difference between correlation and causation?</summary>
<p>Content for correlation vs causation...</p>
</details>
<details>
<summary>Explain the central limit theorem</summary>
<p>Content for central limit theorem...</p>
</details>
<details>
<summary>How do you detect and handle outliers?</summary>
<p>Content for detecting and handling outliers...</p>
</details>
<details>
<summary>What's the difference between parametric and non-parametric tests?</summary>
<p>Content for parametric vs non-parametric tests...</p>
</details>
<details>
<summary>How do you choose between different types of regression?</summary>
<p>Content for choosing between different types of regression...</p>
</details>
<details>
<summary>Explain cross-validation and its importance</summary>
<p>Content for cross-validation and its importance...</p>
</details></section>
                        <h3 class='nav-section-title' id='section-1-descriptive-statistics'>
                            1 descriptive statistics <a class='headerlink' href='#section-1-descriptive-statistics' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-statistics-1_descriptive_statistics"><h1 id="grassroots-statistics-1_descriptive_statistics-descriptive-statistics">Descriptive Statistics<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-descriptive-statistics" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-1_descriptive_statistics-measures-of-central-tendency">Measures of Central Tendency<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-measures-of-central-tendency" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-1-arithmetic-mean-am">1. Arithmetic Mean (AM)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-arithmetic-mean-am" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-definition">Definition<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-definition" title="Permanent link">&para;</a></h4>
<p>For a set of numbers {x₁, x₂, ..., xₙ}:
<span class="arithmatex">\(AM = \bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-properties">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-properties" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Minimizes squared deviations</strong>:
   AM minimizes <span class="arithmatex">\(\sum_{i=1}^{n} (x_i - \mu)^2\)</span></p>
</li>
<li>
<p><strong>Linear property</strong>:
   <span class="arithmatex">\(\bar{ax + b} = a\bar{x} + b\)</span></p>
</li>
<li>
<p><strong>Effect of outliers</strong>:
   Highly sensitive to extreme values</p>
</li>
</ol>
<h4 id="grassroots-statistics-1_descriptive_statistics-use-cases">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-use-cases" title="Permanent link">&para;</a></h4>
<ul>
<li>Default measure of central tendency</li>
<li>When data points contribute equally</li>
<li>Financial calculations (e.g., average daily returns)</li>
<li>Physical measurements with random errors</li>
</ul>
<h4 id="grassroots-statistics-1_descriptive_statistics-relationship-with-standard-deviation">Relationship with Standard Deviation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-relationship-with-standard-deviation" title="Permanent link">&para;</a></h4>
<p>For a dataset {x₁, ..., xₙ}:
<span class="arithmatex">\(\sigma^2 = \frac{1}{n}\sum_{i=1}^{n} (x_i - \bar{x})^2 = \frac{1}{n}\sum_{i=1}^{n} x_i^2 - (\bar{x})^2\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-other-properties">Other properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-other-properties" title="Permanent link">&para;</a></h3>
<ul>
<li>Median: Robustness to outliers</li>
<li>Mode: Use in categorical data</li>
<li>Relationship between mean, median, mode in skewed distributions</li>
</ul>
<h2 id="grassroots-statistics-1_descriptive_statistics-measures-of-dispersion">Measures of Dispersion<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-measures-of-dispersion" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-1-variance-and-standard-deviation">1. Variance and Standard Deviation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-variance-and-standard-deviation" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-population-variance-2">Population Variance (σ²)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-population-variance-2" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(\sigma^2 = \frac{1}{N}\sum_{i=1}^{N} (x_i - \mu)^2\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-sample-variance-s2">Sample Variance (s²)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-sample-variance-s2" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(s^2 = \frac{1}{n-1}\sum_{i=1}^{n} (x_i - \bar{x})^2\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-properties_1">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-properties_1" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-computational-formula">Computational Formula:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-computational-formula" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(s^2 = \frac{1}{n-1}(\sum_{i=1}^{n} x_i^2 - \frac{1}{n}(\sum_{i=1}^{n} x_i)^2)\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-algebraic-properties">Algebraic Properties:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-algebraic-properties" title="Permanent link">&para;</a></h5>
<p>$$ Var(aX + b) = a^2Var(X) $$
   $$ Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y) $$
   For independent X,Y: $$ Var(X + Y) = Var(X) + Var(Y) $$</p>
<h5 id="grassroots-statistics-1_descriptive_statistics-standard-deviation">Standard Deviation:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-standard-deviation" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(\sigma = \sqrt{\sigma^2}\)</span> or <span class="arithmatex">\(s = \sqrt{s^2}\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-use-cases_1">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-use-cases_1" title="Permanent link">&para;</a></h4>
<ul>
<li>Most common measure of variability</li>
<li>Optimal for normal distributions</li>
<li>Input for many statistical procedures</li>
<li>Basis for least squares estimation</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-2-range-and-interquartile-range-iqr">2. Range and Interquartile Range (IQR)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-2-range-and-interquartile-range-iqr" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-range">Range<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-range" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(R = x_{max} - x_{min}\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-interquartile-range">Interquartile Range<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-interquartile-range" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(IQR = Q_3 - Q_1\)</span></p>
<p>where:
- Q₁ is the 25th percentile
- Q₃ is the 75th percentile</p>
<h4 id="grassroots-statistics-1_descriptive_statistics-properties_2">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-properties_2" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-1-outlier-detection">1. Outlier Detection:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-outlier-detection" title="Permanent link">&para;</a></h5>
<ul>
<li>Lower fence: <span class="arithmatex">\(Q_1 - 1.5 × IQR\)</span></li>
<li>Upper fence: <span class="arithmatex">\(Q_3 + 1.5 × IQR\)</span></li>
<li>Points beyond fences are potential outliers</li>
</ul>
<h5 id="grassroots-statistics-1_descriptive_statistics-2-relationship-to-standard-deviation">2. Relationship to Standard Deviation:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-2-relationship-to-standard-deviation" title="Permanent link">&para;</a></h5>
<p>For normal distribution:
   - <span class="arithmatex">\(IQR ≈ 1.349\sigma\)</span>
   - Range ≈ 4σ (for moderate sample sizes)</p>
<h4 id="grassroots-statistics-1_descriptive_statistics-use-cases_2">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-use-cases_2" title="Permanent link">&para;</a></h4>
<ul>
<li>Non-parametric analyses</li>
<li>Robust statistics</li>
<li>Box plots</li>
<li>Quick dispersion estimates</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-3-coefficient-of-variation-cv">3. Coefficient of Variation (CV)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-3-coefficient-of-variation-cv" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-definition_1">Definition<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-definition_1" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(CV = \frac{s}{\bar{x}} \times 100\%\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-properties_3">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-properties_3" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-1-scale-independence">1. Scale Independence:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-scale-independence" title="Permanent link">&para;</a></h5>
<ul>
<li>Unitless measure</li>
<li>Allows comparison across different scales</li>
</ul>
<h5 id="grassroots-statistics-1_descriptive_statistics-2-limitations">2. Limitations:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-2-limitations" title="Permanent link">&para;</a></h5>
<ul>
<li>Only meaningful for ratio scales</li>
<li>Not suitable when mean ≈ 0</li>
<li>Not defined for negative values</li>
</ul>
<h4 id="grassroots-statistics-1_descriptive_statistics-use-cases_3">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-use-cases_3" title="Permanent link">&para;</a></h4>
<ul>
<li>Comparing variability across different units</li>
<li>Quality control</li>
<li>Investment risk assessment</li>
<li>Biological variation studies</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-4-mean-absolute-deviation-mad">4. Mean Absolute Deviation (MAD)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-4-mean-absolute-deviation-mad" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-population-mad">Population MAD<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-population-mad" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(MAD = \frac{1}{N}\sum_{i=1}^{N} |x_i - \mu|\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-sample-mad">Sample MAD<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-sample-mad" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(MAD = \frac{1}{n}\sum_{i=1}^{n} |x_i - \bar{x}|\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-properties_4">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-properties_4" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-1-relationship-to-standard-deviation">1. Relationship to Standard Deviation:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-relationship-to-standard-deviation" title="Permanent link">&para;</a></h5>
<p>For normal distribution:
   <span class="arithmatex">\(MAD ≈ 0.8\sigma\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-2-robustness">2. Robustness:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-2-robustness" title="Permanent link">&para;</a></h5>
<ul>
<li>Less sensitive to outliers than variance</li>
<li>L1 norm vs L2 norm for variance</li>
</ul>
<h4 id="grassroots-statistics-1_descriptive_statistics-use-cases_4">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-use-cases_4" title="Permanent link">&para;</a></h4>
<ul>
<li>Robust statistics</li>
<li>Financial risk measures</li>
<li>Time series analysis</li>
<li>Bio statistics</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-5-covariance-matrix">5. Covariance Matrix (Σ)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-5-covariance-matrix" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-definition_2">Definition<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-definition_2" title="Permanent link">&para;</a></h4>
<p>For random vectors X = [X₁, ..., Xₚ]:
<span class="arithmatex">\(\Sigma_{ij} = Cov(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)]\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-sample-covariance-matrix-s">Sample Covariance Matrix (S)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-sample-covariance-matrix-s" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(S_{ij} = \frac{1}{n-1}\sum_{k=1}^{n} (x_{ki} - \bar{x}_i)(x_{kj} - \bar{x}_j)\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-properties_5">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-properties_5" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-1-matrix-properties">1. Matrix Properties:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-matrix-properties" title="Permanent link">&para;</a></h5>
<ul>
<li>Symmetric: <span class="arithmatex">\(\Sigma_{ij} = \Sigma_{ji}\)</span></li>
<li>Positive semi-definite</li>
<li>Diagonal elements are variances</li>
</ul>
<h5 id="grassroots-statistics-1_descriptive_statistics-2-eigendecomposition">2. Eigendecomposition:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-2-eigendecomposition" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(\Sigma = PDP^T\)</span>
   where:
   - D: diagonal matrix of eigenvalues
   - P: matrix of eigenvectors</p>
<h4 id="grassroots-statistics-1_descriptive_statistics-use-cases_5">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-use-cases_5" title="Permanent link">&para;</a></h4>
<ul>
<li>Principal Component Analysis</li>
<li>Multivariate analysis</li>
<li>Portfolio optimization</li>
<li>Machine learning algorithms</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-6-relationships-and-comparisons">6. Relationships and Comparisons<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-6-relationships-and-comparisons" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-1-robustness-hierarchy-most-to-least-robust">1. Robustness Hierarchy (most to least robust):<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-robustness-hierarchy-most-to-least-robust" title="Permanent link">&para;</a></h4>
<ol>
<li>IQR</li>
<li>MAD</li>
<li>Standard Deviation</li>
<li>Range</li>
</ol>
<h4 id="grassroots-statistics-1_descriptive_statistics-2-efficiency-under-normality-most-to-least-efficient">2. Efficiency under Normality (most to least efficient):<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-2-efficiency-under-normality-most-to-least-efficient" title="Permanent link">&para;</a></h4>
<ol>
<li>Standard Deviation</li>
<li>MAD</li>
<li>IQR</li>
<li>Range</li>
</ol>
<h4 id="grassroots-statistics-1_descriptive_statistics-3-computational-complexity">3. Computational Complexity:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-3-computational-complexity" title="Permanent link">&para;</a></h4>
<ul>
<li>Range: O(n)</li>
<li>IQR: O(n log n)</li>
<li>MAD: O(n)</li>
<li>Variance: O(n)</li>
<li>Covariance Matrix: O(n²)</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-7-selection-guidelines">7. Selection Guidelines<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-7-selection-guidelines" title="Permanent link">&para;</a></h3>
<p><strong>Use Standard Deviation when</strong>:
   - Data is approximately normal
   - Need mathematical tractability
   - Working with inferential statistics</p>
<p><strong>Use IQR when</strong>:
   - Data has outliers
   - Distribution is skewed
   - Need robust measure</p>
<p><strong>Use CV when</strong>:
   - Comparing different scales
   - Working with strictly positive data
   - Need relative variation</p>
<p><strong>Use MAD when</strong>:
   - Need robust measure
   - Working with time series
   - Dealing with non-normal data</p>
<h1 id="grassroots-statistics-1_descriptive_statistics-expected-value-variance-relationship">Expected Value - Variance Relationship<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected-value-variance-relationship" title="Permanent link">&para;</a></h1>
<h3 id="grassroots-statistics-1_descriptive_statistics-1-basic-definitions">1. Basic Definitions<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-basic-definitions" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-discrete-random-variables">Discrete Random Variables<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-discrete-random-variables" title="Permanent link">&para;</a></h5>
<p>For a discrete random variable X:
<span class="arithmatex">\(E[X] = \sum_{x} x \cdot P(X = x)\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-continuous-random-variables">Continuous Random Variables<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-continuous-random-variables" title="Permanent link">&para;</a></h5>
<p>For a continuous random variable X:
<span class="arithmatex">\(E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx\)</span></p>
<p>where f(x) is the probability density function.</p>
<h5 id="grassroots-statistics-1_descriptive_statistics-alternative-notation">Alternative Notation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-alternative-notation" title="Permanent link">&para;</a></h5>
<ul>
<li><span class="arithmatex">\(E[X]\)</span></li>
<li><span class="arithmatex">\(\mu\)</span></li>
<li><span class="arithmatex">\(\mu_X\)</span></li>
<li><span class="arithmatex">\(\langle X \rangle\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-2-fundamental-properties">2. Fundamental Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-2-fundamental-properties" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-linearity-of-expectation">Linearity of Expectation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-linearity-of-expectation" title="Permanent link">&para;</a></h5>
<ol>
<li><span class="arithmatex">\(E[aX + b] = aE[X] + b\)</span></li>
<li><span class="arithmatex">\(E[X + Y] = E[X] + E[Y]\)</span></li>
</ol>
<h6 id="grassroots-statistics-1_descriptive_statistics-proof-for-discrete-case">Proof for Discrete Case:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-proof-for-discrete-case" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code>E[aX + b] = ∑(ax + b)P(X = x)
          = a∑xP(X = x) + b∑P(X = x)
          = aE[X] + b    (since ∑P(X = x) = 1)
</code></pre></div>
<h5 id="grassroots-statistics-1_descriptive_statistics-law-of-total-expectation">Law of Total Expectation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-law-of-total-expectation" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(E[X] = E[E[X|Y]]\)</span></p>
<h6 id="grassroots-statistics-1_descriptive_statistics-proof">Proof:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-proof" title="Permanent link">&para;</a></h6>
<p>For discrete case:
<div class="highlight"><pre><span></span><code>E[E[X|Y]] = ∑_y E[X|Y=y]P(Y=y)
          = ∑_y ∑_x xP(X=x|Y=y)P(Y=y)
          = ∑_x x∑_y P(X=x|Y=y)P(Y=y)
          = ∑_x xP(X=x)
          = E[X]
</code></pre></div></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-3-special-cases-and-important-results">3. Special Cases and Important Results<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-3-special-cases-and-important-results" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-functions-of-random-variables">Functions of Random Variables<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-functions-of-random-variables" title="Permanent link">&para;</a></h5>
<p>For any function g(X):
<span class="arithmatex">\(E[g(X)] = \begin{cases} 
\sum_x g(x)P(X=x) &amp; \text{discrete case} \\
\int_{-\infty}^{\infty} g(x)f(x)dx &amp; \text{continuous case}
\end{cases}\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected-value-of-product">Expected Value of Product<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected-value-of-product" title="Permanent link">&para;</a></h5>
<p>For any two random variables X and Y:
<span class="arithmatex">\(E[XY] = E[X]E[Y] + Cov(X,Y)\)</span></p>
<p>For independent variables:
<span class="arithmatex">\(E[XY] = E[X]E[Y]\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-4-sample-mean-properties">4. Sample Mean Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-4-sample-mean-properties" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-sample-mean">Sample Mean<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-sample-mean" title="Permanent link">&para;</a></h5>
<p>For observations {X₁, ..., Xₙ}:
<span class="arithmatex">\(\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-properties-of-sample-mean">Properties of Sample Mean<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-properties-of-sample-mean" title="Permanent link">&para;</a></h5>
<ol>
<li><span class="arithmatex">\(E[\bar{X}] = \mu\)</span> (unbiased)</li>
<li><span class="arithmatex">\(Var(\bar{X}) = \frac{\sigma^2}{n}\)</span></li>
<li><span class="arithmatex">\(SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}\)</span></li>
</ol>
<h3 id="grassroots-statistics-1_descriptive_statistics-5-important-inequalities">5. Important Inequalities<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-5-important-inequalities" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-jensens-inequality">Jensen's Inequality<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-jensens-inequality" title="Permanent link">&para;</a></h5>
<p>For convex function g:
<span class="arithmatex">\(g(E[X]) \leq E[g(X)]\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-markovs-inequality">Markov's Inequality<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-markovs-inequality" title="Permanent link">&para;</a></h5>
<p>For non-negative X and a &gt; 0:
<span class="arithmatex">\(P(X \geq a) \leq \frac{E[X]}{a}\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-chebyshevs-inequality">Chebyshev's Inequality<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-chebyshevs-inequality" title="Permanent link">&para;</a></h5>
<p>For any k &gt; 0:
<span class="arithmatex">\(P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2}\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-6-relationship-with-characteristic-function">6. Relationship with Characteristic Function<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-6-relationship-with-characteristic-function" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-definition_3">Definition<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-definition_3" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(\phi_X(t) = E[e^{itX}]\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-moments-from-characteristic-function">Moments from Characteristic Function<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-moments-from-characteristic-function" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(E[X^n] = i^{-n}\frac{d^n}{dt^n}\phi_X(t)|_{t=0}\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-7-computational-considerations">7. Computational Considerations<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-7-computational-considerations" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-monte-carlo-estimation">Monte Carlo Estimation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-monte-carlo-estimation" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(E[X] \approx \frac{1}{n}\sum_{i=1}^{n} X_i\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-error-in-estimation">Error in Estimation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-error-in-estimation" title="Permanent link">&para;</a></h5>
<p>Standard Error = <span class="arithmatex">\(\frac{\sigma}{\sqrt{n}}\)</span></p>
<p>95% Confidence Interval ≈ <span class="arithmatex">\(\bar{X} \pm 1.96\frac{\sigma}{\sqrt{n}}\)</span></p>
<h1 id="grassroots-statistics-1_descriptive_statistics-distribution-characteristics">Distribution Characteristics<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution-characteristics" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-1_descriptive_statistics-1-descriptive-measures-of-distributions">1. Descriptive Measures of Distributions<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-1-descriptive-measures-of-distributions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-skewness">Skewness<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-skewness" title="Permanent link">&para;</a></h3>
<p>Skewness measures the asymmetry of a probability distribution:
- <strong>Left-skewed (Negative)</strong>: Tail extends to the left, mean &lt; median
- <strong>Right-skewed (Positive)</strong>: Tail extends to the right, mean &gt; median
- <strong>Symmetric</strong>: Mean = median, skewness = 0</p>
<h3 id="grassroots-statistics-1_descriptive_statistics-kurtosis">Kurtosis<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-kurtosis" title="Permanent link">&para;</a></h3>
<p>Kurtosis measures the "tailedness" of a probability distribution:
- <strong>Heavy-tailed (Leptokurtic)</strong>: Higher kurtosis, more extreme values
- <strong>Light-tailed (Platykurtic)</strong>: Lower kurtosis, fewer extreme values
- <strong>Normal (Mesokurtic)</strong>: Reference kurtosis = 3 for normal distribution</p>
<h3 id="grassroots-statistics-1_descriptive_statistics-moments-of-a-distribution">Moments of a Distribution<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-moments-of-a-distribution" title="Permanent link">&para;</a></h3>
<p>Moments are quantitative measures that describe the shape of a distribution:
- <strong>First Moment</strong>: Mean (Expected Value)
- <strong>Second Moment</strong>: Variance and Standard Deviation
- <strong>Third Moment</strong>: Related to Skewness
- <strong>Fourth Moment</strong>: Related to Kurtosis</p>
<h2 id="grassroots-statistics-1_descriptive_statistics-2-common-distribution-expected-values-and-variances">2. Common Distribution Expected Values and Variances<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-2-common-distribution-expected-values-and-variances" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-normal-distribution-n-2">Normal Distribution N(μ, σ²)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-normal-distribution-n-2" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = \mu\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = \sigma^2\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-exponential-distribution-rate">Exponential Distribution (rate λ)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-exponential-distribution-rate" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = \frac{1}{\lambda}\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = \frac{1}{\lambda^2}\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-poisson-distribution-rate">Poisson Distribution (rate λ)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-poisson-distribution-rate" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = \lambda\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = \lambda\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-binomial-distribution-n-trials-probability-p">Binomial Distribution (n trials, probability p)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-binomial-distribution-n-trials-probability-p" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = np\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = np(1-p)\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-geometric-distribution-probability-p">Geometric Distribution (probability p)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-geometric-distribution-probability-p" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = \frac{1}{p}\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = \frac{1-p}{p^2}\)</span></li>
</ul>
<h2 id="grassroots-statistics-1_descriptive_statistics-3-higher-moments">3. Higher Moments<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-3-higher-moments" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-kth-moment">kth Moment<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-kth-moment" title="Permanent link">&para;</a></h3>
<p>The kth moment about zero (raw moment) is defined as:</p>
<p><span class="arithmatex">\(E[X^k] = \begin{cases}
\sum_x x^k P(X=x) &amp; \text{discrete case} \\
\int_{-\infty}^{\infty} x^k f(x)dx &amp; \text{continuous case}
\end{cases}\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-central-moments">Central Moments<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-central-moments" title="Permanent link">&para;</a></h3>
<p>The kth central moment (moment about the mean) is defined as:</p>
<p><span class="arithmatex">\(E[(X-\mu)^k] = \begin{cases}
\sum_x (x-\mu)^k P(X=x) &amp; \text{discrete case} \\
\int_{-\infty}^{\infty} (x-\mu)^k f(x)dx &amp; \text{continuous case}
\end{cases}\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-important-relations">Important Relations<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-important-relations" title="Permanent link">&para;</a></h3>
<ul>
<li>First Central Moment: <span class="arithmatex">\(E[X-\mu] = 0\)</span></li>
<li>Second Central Moment: <span class="arithmatex">\(E[(X-\mu)^2] = Var(X)\)</span></li>
<li>Third Standardized Moment: <span class="arithmatex">\(E[(X-\mu)^3]/\sigma^3\)</span> (Skewness)</li>
<li>Fourth Standardized Moment: <span class="arithmatex">\(E[(X-\mu)^4]/\sigma^4\)</span> (Kurtosis)</li>
</ul></section><section class="print-page" id="grassroots-statistics-1_descriptive_statistics-central_tendency"><h1 id="grassroots-statistics-1_descriptive_statistics-central_tendency-central-tendency">Central tendency</h1><h2 id="measures-of-central-tendency">Measures of Central Tendency<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-central_tendency-measures-of-central-tendency" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-central_tendency-1-arithmetic-mean-am">1. Arithmetic Mean (AM)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-central_tendency-1-arithmetic-mean-am" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-central_tendency-definition">Definition<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-central_tendency-definition" title="Permanent link">&para;</a></h4>
<p>For a set of numbers {x₁, x₂, ..., xₙ}:
<span class="arithmatex">\(AM = \bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-central_tendency-properties">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-central_tendency-properties" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Minimizes squared deviations</strong>:
   AM minimizes <span class="arithmatex">\(\sum_{i=1}^{n} (x_i - \mu)^2\)</span></p>
</li>
<li>
<p><strong>Linear property</strong>:
   <span class="arithmatex">\(\bar{ax + b} = a\bar{x} + b\)</span></p>
</li>
<li>
<p><strong>Effect of outliers</strong>:
   Highly sensitive to extreme values</p>
</li>
</ol>
<h4 id="grassroots-statistics-1_descriptive_statistics-central_tendency-use-cases">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-central_tendency-use-cases" title="Permanent link">&para;</a></h4>
<ul>
<li>Default measure of central tendency</li>
<li>When data points contribute equally</li>
<li>Financial calculations (e.g., average daily returns)</li>
<li>Physical measurements with random errors</li>
</ul>
<h4 id="grassroots-statistics-1_descriptive_statistics-central_tendency-relationship-with-standard-deviation">Relationship with Standard Deviation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-central_tendency-relationship-with-standard-deviation" title="Permanent link">&para;</a></h4>
<p>For a dataset {x₁, ..., xₙ}:
<span class="arithmatex">\(\sigma^2 = \frac{1}{n}\sum_{i=1}^{n} (x_i - \bar{x})^2 = \frac{1}{n}\sum_{i=1}^{n} x_i^2 - (\bar{x})^2\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-central_tendency-other-properties">Other properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-central_tendency-other-properties" title="Permanent link">&para;</a></h3>
<ul>
<li>Median: Robustness to outliers</li>
<li>Mode: Use in categorical data</li>
<li>Relationship between mean, median, mode in skewed distributions</li>
</ul></section><section class="print-page" id="grassroots-statistics-1_descriptive_statistics-dispersion"><h1 id="grassroots-statistics-1_descriptive_statistics-dispersion-dispersion">Dispersion</h1><h2 id="measures-of-dispersion">Measures of Dispersion<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-measures-of-dispersion" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-dispersion-1-variance-and-standard-deviation">1. Variance and Standard Deviation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-1-variance-and-standard-deviation" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-population-variance-2">Population Variance (σ²)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-population-variance-2" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(\sigma^2 = \frac{1}{N}\sum_{i=1}^{N} (x_i - \mu)^2\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-sample-variance-s2">Sample Variance (s²)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-sample-variance-s2" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(s^2 = \frac{1}{n-1}\sum_{i=1}^{n} (x_i - \bar{x})^2\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-properties">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-properties" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-computational-formula">Computational Formula:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-computational-formula" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(s^2 = \frac{1}{n-1}(\sum_{i=1}^{n} x_i^2 - \frac{1}{n}(\sum_{i=1}^{n} x_i)^2)\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-algebraic-properties">Algebraic Properties:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-algebraic-properties" title="Permanent link">&para;</a></h5>
<p>$$ Var(aX + b) = a^2Var(X) $$
   $$ Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y) $$
   For independent X,Y: $$ Var(X + Y) = Var(X) + Var(Y) $$</p>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-standard-deviation">Standard Deviation:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-standard-deviation" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(\sigma = \sqrt{\sigma^2}\)</span> or <span class="arithmatex">\(s = \sqrt{s^2}\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-use-cases">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-use-cases" title="Permanent link">&para;</a></h4>
<ul>
<li>Most common measure of variability</li>
<li>Optimal for normal distributions</li>
<li>Input for many statistical procedures</li>
<li>Basis for least squares estimation</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-dispersion-2-range-and-interquartile-range-iqr">2. Range and Interquartile Range (IQR)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-2-range-and-interquartile-range-iqr" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-range">Range<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-range" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(R = x_{max} - x_{min}\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-interquartile-range">Interquartile Range<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-interquartile-range" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(IQR = Q_3 - Q_1\)</span></p>
<p>where:
- Q₁ is the 25th percentile
- Q₃ is the 75th percentile</p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-properties_1">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-properties_1" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-1-outlier-detection">1. Outlier Detection:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-1-outlier-detection" title="Permanent link">&para;</a></h5>
<ul>
<li>Lower fence: <span class="arithmatex">\(Q_1 - 1.5 × IQR\)</span></li>
<li>Upper fence: <span class="arithmatex">\(Q_3 + 1.5 × IQR\)</span></li>
<li>Points beyond fences are potential outliers</li>
</ul>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-2-relationship-to-standard-deviation">2. Relationship to Standard Deviation:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-2-relationship-to-standard-deviation" title="Permanent link">&para;</a></h5>
<p>For normal distribution:
   - <span class="arithmatex">\(IQR ≈ 1.349\sigma\)</span>
   - Range ≈ 4σ (for moderate sample sizes)</p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-use-cases_1">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-use-cases_1" title="Permanent link">&para;</a></h4>
<ul>
<li>Non-parametric analyses</li>
<li>Robust statistics</li>
<li>Box plots</li>
<li>Quick dispersion estimates</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-dispersion-3-coefficient-of-variation-cv">3. Coefficient of Variation (CV)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-3-coefficient-of-variation-cv" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-definition">Definition<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-definition" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(CV = \frac{s}{\bar{x}} \times 100\%\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-properties_2">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-properties_2" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-1-scale-independence">1. Scale Independence:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-1-scale-independence" title="Permanent link">&para;</a></h5>
<ul>
<li>Unitless measure</li>
<li>Allows comparison across different scales</li>
</ul>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-2-limitations">2. Limitations:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-2-limitations" title="Permanent link">&para;</a></h5>
<ul>
<li>Only meaningful for ratio scales</li>
<li>Not suitable when mean ≈ 0</li>
<li>Not defined for negative values</li>
</ul>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-use-cases_2">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-use-cases_2" title="Permanent link">&para;</a></h4>
<ul>
<li>Comparing variability across different units</li>
<li>Quality control</li>
<li>Investment risk assessment</li>
<li>Biological variation studies</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-dispersion-4-mean-absolute-deviation-mad">4. Mean Absolute Deviation (MAD)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-4-mean-absolute-deviation-mad" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-population-mad">Population MAD<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-population-mad" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(MAD = \frac{1}{N}\sum_{i=1}^{N} |x_i - \mu|\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-sample-mad">Sample MAD<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-sample-mad" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(MAD = \frac{1}{n}\sum_{i=1}^{n} |x_i - \bar{x}|\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-properties_3">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-properties_3" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-1-relationship-to-standard-deviation">1. Relationship to Standard Deviation:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-1-relationship-to-standard-deviation" title="Permanent link">&para;</a></h5>
<p>For normal distribution:
   <span class="arithmatex">\(MAD ≈ 0.8\sigma\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-2-robustness">2. Robustness:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-2-robustness" title="Permanent link">&para;</a></h5>
<ul>
<li>Less sensitive to outliers than variance</li>
<li>L1 norm vs L2 norm for variance</li>
</ul>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-use-cases_3">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-use-cases_3" title="Permanent link">&para;</a></h4>
<ul>
<li>Robust statistics</li>
<li>Financial risk measures</li>
<li>Time series analysis</li>
<li>Bio statistics</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-dispersion-5-covariance-matrix">5. Covariance Matrix (Σ)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-5-covariance-matrix" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-definition_1">Definition<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-definition_1" title="Permanent link">&para;</a></h4>
<p>For random vectors X = [X₁, ..., Xₚ]:
<span class="arithmatex">\(\Sigma_{ij} = Cov(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)]\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-sample-covariance-matrix-s">Sample Covariance Matrix (S)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-sample-covariance-matrix-s" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(S_{ij} = \frac{1}{n-1}\sum_{k=1}^{n} (x_{ki} - \bar{x}_i)(x_{kj} - \bar{x}_j)\)</span></p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-properties_4">Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-properties_4" title="Permanent link">&para;</a></h4>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-1-matrix-properties">1. Matrix Properties:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-1-matrix-properties" title="Permanent link">&para;</a></h5>
<ul>
<li>Symmetric: <span class="arithmatex">\(\Sigma_{ij} = \Sigma_{ji}\)</span></li>
<li>Positive semi-definite</li>
<li>Diagonal elements are variances</li>
</ul>
<h5 id="grassroots-statistics-1_descriptive_statistics-dispersion-2-eigendecomposition">2. Eigendecomposition:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-2-eigendecomposition" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(\Sigma = PDP^T\)</span>
   where:
   - D: diagonal matrix of eigenvalues
   - P: matrix of eigenvectors</p>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-use-cases_4">Use Cases<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-use-cases_4" title="Permanent link">&para;</a></h4>
<ul>
<li>Principal Component Analysis</li>
<li>Multivariate analysis</li>
<li>Portfolio optimization</li>
<li>Machine learning algorithms</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-dispersion-6-relationships-and-comparisons">6. Relationships and Comparisons<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-6-relationships-and-comparisons" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-1-robustness-hierarchy-most-to-least-robust">1. Robustness Hierarchy (most to least robust):<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-1-robustness-hierarchy-most-to-least-robust" title="Permanent link">&para;</a></h4>
<ol>
<li>IQR</li>
<li>MAD</li>
<li>Standard Deviation</li>
<li>Range</li>
</ol>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-2-efficiency-under-normality-most-to-least-efficient">2. Efficiency under Normality (most to least efficient):<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-2-efficiency-under-normality-most-to-least-efficient" title="Permanent link">&para;</a></h4>
<ol>
<li>Standard Deviation</li>
<li>MAD</li>
<li>IQR</li>
<li>Range</li>
</ol>
<h4 id="grassroots-statistics-1_descriptive_statistics-dispersion-3-computational-complexity">3. Computational Complexity:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-3-computational-complexity" title="Permanent link">&para;</a></h4>
<ul>
<li>Range: O(n)</li>
<li>IQR: O(n log n)</li>
<li>MAD: O(n)</li>
<li>Variance: O(n)</li>
<li>Covariance Matrix: O(n²)</li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-dispersion-7-selection-guidelines">7. Selection Guidelines<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-dispersion-7-selection-guidelines" title="Permanent link">&para;</a></h3>
<p><strong>Use Standard Deviation when</strong>:
   - Data is approximately normal
   - Need mathematical tractability
   - Working with inferential statistics</p>
<p><strong>Use IQR when</strong>:
   - Data has outliers
   - Distribution is skewed
   - Need robust measure</p>
<p><strong>Use CV when</strong>:
   - Comparing different scales
   - Working with strictly positive data
   - Need relative variation</p>
<p><strong>Use MAD when</strong>:
   - Need robust measure
   - Working with time series
   - Dealing with non-normal data</p></section><section class="print-page" id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics"><h1 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-distribution-characteristics">Distribution Characteristics<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-distribution-characteristics" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-1-descriptive-measures-of-distributions">1. Descriptive Measures of Distributions<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-1-descriptive-measures-of-distributions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-skewness">Skewness<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-skewness" title="Permanent link">&para;</a></h3>
<p>Skewness measures the asymmetry of a probability distribution:
- <strong>Left-skewed (Negative)</strong>: Tail extends to the left, mean &lt; median
- <strong>Right-skewed (Positive)</strong>: Tail extends to the right, mean &gt; median
- <strong>Symmetric</strong>: Mean = median, skewness = 0</p>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-kurtosis">Kurtosis<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-kurtosis" title="Permanent link">&para;</a></h3>
<p>Kurtosis measures the "tailedness" of a probability distribution:
- <strong>Heavy-tailed (Leptokurtic)</strong>: Higher kurtosis, more extreme values
- <strong>Light-tailed (Platykurtic)</strong>: Lower kurtosis, fewer extreme values
- <strong>Normal (Mesokurtic)</strong>: Reference kurtosis = 3 for normal distribution</p>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-moments-of-a-distribution">Moments of a Distribution<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-moments-of-a-distribution" title="Permanent link">&para;</a></h3>
<p>Moments are quantitative measures that describe the shape of a distribution:
- <strong>First Moment</strong>: Mean (Expected Value)
- <strong>Second Moment</strong>: Variance and Standard Deviation
- <strong>Third Moment</strong>: Related to Skewness
- <strong>Fourth Moment</strong>: Related to Kurtosis</p>
<h2 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-2-common-distribution-expected-values-and-variances">2. Common Distribution Expected Values and Variances<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-2-common-distribution-expected-values-and-variances" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-normal-distribution-n-2">Normal Distribution N(μ, σ²)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-normal-distribution-n-2" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = \mu\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = \sigma^2\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-exponential-distribution-rate">Exponential Distribution (rate λ)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-exponential-distribution-rate" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = \frac{1}{\lambda}\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = \frac{1}{\lambda^2}\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-poisson-distribution-rate">Poisson Distribution (rate λ)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-poisson-distribution-rate" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = \lambda\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = \lambda\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-binomial-distribution-n-trials-probability-p">Binomial Distribution (n trials, probability p)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-binomial-distribution-n-trials-probability-p" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = np\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = np(1-p)\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-geometric-distribution-probability-p">Geometric Distribution (probability p)<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-geometric-distribution-probability-p" title="Permanent link">&para;</a></h3>
<ul>
<li>Expected Value: <span class="arithmatex">\(E[X] = \frac{1}{p}\)</span></li>
<li>Variance: <span class="arithmatex">\(Var(X) = \frac{1-p}{p^2}\)</span></li>
</ul>
<h2 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-3-higher-moments">3. Higher Moments<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-3-higher-moments" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-kth-moment">kth Moment<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-kth-moment" title="Permanent link">&para;</a></h3>
<p>The kth moment about zero (raw moment) is defined as:</p>
<p><span class="arithmatex">\(E[X^k] = \begin{cases}
\sum_x x^k P(X=x) &amp; \text{discrete case} \\
\int_{-\infty}^{\infty} x^k f(x)dx &amp; \text{continuous case}
\end{cases}\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-central-moments">Central Moments<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-central-moments" title="Permanent link">&para;</a></h3>
<p>The kth central moment (moment about the mean) is defined as:</p>
<p><span class="arithmatex">\(E[(X-\mu)^k] = \begin{cases}
\sum_x (x-\mu)^k P(X=x) &amp; \text{discrete case} \\
\int_{-\infty}^{\infty} (x-\mu)^k f(x)dx &amp; \text{continuous case}
\end{cases}\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-distribution_characteristics-important-relations">Important Relations<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-distribution_characteristics-important-relations" title="Permanent link">&para;</a></h3>
<ul>
<li>First Central Moment: <span class="arithmatex">\(E[X-\mu] = 0\)</span></li>
<li>Second Central Moment: <span class="arithmatex">\(E[(X-\mu)^2] = Var(X)\)</span></li>
<li>Third Standardized Moment: <span class="arithmatex">\(E[(X-\mu)^3]/\sigma^3\)</span> (Skewness)</li>
<li>Fourth Standardized Moment: <span class="arithmatex">\(E[(X-\mu)^4]/\sigma^4\)</span> (Kurtosis)</li>
</ul></section><section class="print-page" id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship"><h1 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-expected-value-variance-relationship">Expected Value - Variance Relationship<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-expected-value-variance-relationship" title="Permanent link">&para;</a></h1>
<h3 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-1-basic-definitions">1. Basic Definitions<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-1-basic-definitions" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-discrete-random-variables">Discrete Random Variables<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-discrete-random-variables" title="Permanent link">&para;</a></h5>
<p>For a discrete random variable X:
<span class="arithmatex">\(E[X] = \sum_{x} x \cdot P(X = x)\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-continuous-random-variables">Continuous Random Variables<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-continuous-random-variables" title="Permanent link">&para;</a></h5>
<p>For a continuous random variable X:
<span class="arithmatex">\(E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx\)</span></p>
<p>where f(x) is the probability density function.</p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-alternative-notation">Alternative Notation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-alternative-notation" title="Permanent link">&para;</a></h5>
<ul>
<li><span class="arithmatex">\(E[X]\)</span></li>
<li><span class="arithmatex">\(\mu\)</span></li>
<li><span class="arithmatex">\(\mu_X\)</span></li>
<li><span class="arithmatex">\(\langle X \rangle\)</span></li>
</ul>
<h3 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-2-fundamental-properties">2. Fundamental Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-2-fundamental-properties" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-linearity-of-expectation">Linearity of Expectation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-linearity-of-expectation" title="Permanent link">&para;</a></h5>
<ol>
<li><span class="arithmatex">\(E[aX + b] = aE[X] + b\)</span></li>
<li><span class="arithmatex">\(E[X + Y] = E[X] + E[Y]\)</span></li>
</ol>
<h6 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-proof-for-discrete-case">Proof for Discrete Case:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-proof-for-discrete-case" title="Permanent link">&para;</a></h6>
<div class="highlight"><pre><span></span><code>E[aX + b] = ∑(ax + b)P(X = x)
          = a∑xP(X = x) + b∑P(X = x)
          = aE[X] + b    (since ∑P(X = x) = 1)
</code></pre></div>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-law-of-total-expectation">Law of Total Expectation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-law-of-total-expectation" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(E[X] = E[E[X|Y]]\)</span></p>
<h6 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-proof">Proof:<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-proof" title="Permanent link">&para;</a></h6>
<p>For discrete case:
<div class="highlight"><pre><span></span><code>E[E[X|Y]] = ∑_y E[X|Y=y]P(Y=y)
          = ∑_y ∑_x xP(X=x|Y=y)P(Y=y)
          = ∑_x x∑_y P(X=x|Y=y)P(Y=y)
          = ∑_x xP(X=x)
          = E[X]
</code></pre></div></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-3-special-cases-and-important-results">3. Special Cases and Important Results<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-3-special-cases-and-important-results" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-functions-of-random-variables">Functions of Random Variables<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-functions-of-random-variables" title="Permanent link">&para;</a></h5>
<p>For any function g(X):
<span class="arithmatex">\(E[g(X)] = \begin{cases} 
\sum_x g(x)P(X=x) &amp; \text{discrete case} \\
\int_{-\infty}^{\infty} g(x)f(x)dx &amp; \text{continuous case}
\end{cases}\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-expected-value-of-product">Expected Value of Product<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-expected-value-of-product" title="Permanent link">&para;</a></h5>
<p>For any two random variables X and Y:
<span class="arithmatex">\(E[XY] = E[X]E[Y] + Cov(X,Y)\)</span></p>
<p>For independent variables:
<span class="arithmatex">\(E[XY] = E[X]E[Y]\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-4-sample-mean-properties">4. Sample Mean Properties<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-4-sample-mean-properties" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-sample-mean">Sample Mean<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-sample-mean" title="Permanent link">&para;</a></h5>
<p>For observations {X₁, ..., Xₙ}:
<span class="arithmatex">\(\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-properties-of-sample-mean">Properties of Sample Mean<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-properties-of-sample-mean" title="Permanent link">&para;</a></h5>
<ol>
<li><span class="arithmatex">\(E[\bar{X}] = \mu\)</span> (unbiased)</li>
<li><span class="arithmatex">\(Var(\bar{X}) = \frac{\sigma^2}{n}\)</span></li>
<li><span class="arithmatex">\(SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}\)</span></li>
</ol>
<h3 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-5-important-inequalities">5. Important Inequalities<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-5-important-inequalities" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-jensens-inequality">Jensen's Inequality<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-jensens-inequality" title="Permanent link">&para;</a></h5>
<p>For convex function g:
<span class="arithmatex">\(g(E[X]) \leq E[g(X)]\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-markovs-inequality">Markov's Inequality<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-markovs-inequality" title="Permanent link">&para;</a></h5>
<p>For non-negative X and a &gt; 0:
<span class="arithmatex">\(P(X \geq a) \leq \frac{E[X]}{a}\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-chebyshevs-inequality">Chebyshev's Inequality<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-chebyshevs-inequality" title="Permanent link">&para;</a></h5>
<p>For any k &gt; 0:
<span class="arithmatex">\(P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2}\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-6-relationship-with-characteristic-function">6. Relationship with Characteristic Function<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-6-relationship-with-characteristic-function" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-definition">Definition<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-definition" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(\phi_X(t) = E[e^{itX}]\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-moments-from-characteristic-function">Moments from Characteristic Function<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-moments-from-characteristic-function" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(E[X^n] = i^{-n}\frac{d^n}{dt^n}\phi_X(t)|_{t=0}\)</span></p>
<h3 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-7-computational-considerations">7. Computational Considerations<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-7-computational-considerations" title="Permanent link">&para;</a></h3>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-monte-carlo-estimation">Monte Carlo Estimation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-monte-carlo-estimation" title="Permanent link">&para;</a></h5>
<p><span class="arithmatex">\(E[X] \approx \frac{1}{n}\sum_{i=1}^{n} X_i\)</span></p>
<h5 id="grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-error-in-estimation">Error in Estimation<a class="headerlink" href="#grassroots-statistics-1_descriptive_statistics-expected_value_variance_relationship-error-in-estimation" title="Permanent link">&para;</a></h5>
<p>Standard Error = <span class="arithmatex">\(\frac{\sigma}{\sqrt{n}}\)</span></p>
<p>95% Confidence Interval ≈ <span class="arithmatex">\(\bar{X} \pm 1.96\frac{\sigma}{\sqrt{n}}\)</span></p></section><h1 class='nav-section-title-end'>Ended: 1 descriptive statistics</h1>
                        <h3 class='nav-section-title' id='section-2-probability-distributions'>
                            2 probability distributions <a class='headerlink' href='#section-2-probability-distributions' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-statistics-2_probability_distributions"><h1 id="grassroots-statistics-2_probability_distributions-probability-distributions">Probability Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-probability-distributions" title="Permanent link">&para;</a></h1>
<h1 id="grassroots-statistics-2_probability_distributions-continuous-probability-distributions">Continuous Probability Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous-probability-distributions" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-2_probability_distributions-normalgaussian-distribution">Normal/Gaussian Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-normalgaussian-distribution" title="Permanent link">&para;</a></h2>
<p>The normal distribution is fundamental to statistics, arising naturally in many phenomena due to the Central Limit Theorem. Its mathematical elegance and theoretical properties make it the cornerstone of statistical inference.</p>
<h3 id="grassroots-statistics-2_probability_distributions-mathematical-foundation">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-mathematical-foundation" title="Permanent link">&para;</a></h3>
<p>The probability density function is given by:</p>
<p><span class="arithmatex">\(f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span></p>
<p>where:
- μ determines the center (location)
- σ controls the spread (scale)</p>
<p>The standard normal distribution (μ = 0, σ = 1) simplifies this to:</p>
<p><span class="arithmatex">\(f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}\)</span></p>
<h3 id="grassroots-statistics-2_probability_distributions-key-properties">Key Properties<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-key-properties" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Symmetry</strong>: The distribution is perfectly symmetric around μ</li>
<li><strong>Empirical Rule</strong>:</li>
<li>μ ± σ contains ≈ 68% of data</li>
<li>μ ± 2σ contains ≈ 95% of data</li>
<li>μ ± 3σ contains ≈ 99.7% of data</li>
</ol>
<h3 id="grassroots-statistics-2_probability_distributions-implementation">Implementation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-implementation" title="Permanent link">&para;</a></h3>
<p>For practical applications, we can use Python:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">stats</span>

<span class="c1"># Calculate probability between -1 and 1 standard deviations</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Probability within 1σ: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># ≈ 0.6827</span>
</code></pre></div>
<h2 id="grassroots-statistics-2_probability_distributions-students-t-distribution">Student's t-Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-students-t-distribution" title="Permanent link">&para;</a></h2>
<p>The t-distribution emerges when estimating the mean of a normally distributed population when the sample size is small and the population standard deviation is unknown.</p>
<h3 id="grassroots-statistics-2_probability_distributions-mathematical-foundation_1">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-mathematical-foundation_1" title="Permanent link">&para;</a></h3>
<p>The probability density function is:</p>
<p><span class="arithmatex">\(f(t) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})} \left(1+\frac{t^2}{\nu}\right)^{-\frac{\nu+1}{2}}\)</span></p>
<p>where ν represents the degrees of freedom.</p>
<h3 id="grassroots-statistics-2_probability_distributions-intuition">Intuition<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-intuition" title="Permanent link">&para;</a></h3>
<p>Think of the t-distribution as a "more uncertain" normal distribution. As sample size increases (ν increases), we become more certain about our estimates, and the t-distribution approaches the normal distribution.</p>
<h3 id="grassroots-statistics-2_probability_distributions-critical-values">Critical Values<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-critical-values" title="Permanent link">&para;</a></h3>
<p>For hypothesis testing, we often need critical values. The relationship between confidence levels and t-values depends on ν:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_t_critical</span><span class="p">(</span><span class="n">confidence_level</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate two-tailed critical t-value&quot;&quot;&quot;</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">confidence_level</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
</code></pre></div>
<h2 id="grassroots-statistics-2_probability_distributions-chi-square-distribution">Chi-Square Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-chi-square-distribution" title="Permanent link">&para;</a></h2>
<p>The chi-square distribution represents the sum of squared standard normal variables. It's crucial for variance-related inference and categorical data analysis.</p>
<h3 id="grassroots-statistics-2_probability_distributions-mathematical-foundation_2">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-mathematical-foundation_2" title="Permanent link">&para;</a></h3>
<p>For k degrees of freedom:</p>
<p><span class="arithmatex">\(f(x) = \frac{1}{2^{k/2}\Gamma(k/2)} x^{k/2-1}e^{-x/2}\)</span></p>
<h3 id="grassroots-statistics-2_probability_distributions-properties">Properties<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-properties" title="Permanent link">&para;</a></h3>
<p>Expected value: E(X) = k
Variance: Var(X) = 2k</p>
<p>The distribution becomes more symmetric as k increases, approaching normality.</p>
<h3 id="grassroots-statistics-2_probability_distributions-application-example-testing-variance">Application Example: Testing Variance<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-application-example-testing-variance" title="Permanent link">&para;</a></h3>
<p>To test if a sample comes from a population with a specified variance σ²₀:</p>
<p><span class="arithmatex">\(\chi^2 = \frac{(n-1)s^2}{\sigma_0^2}\)</span></p>
<p>where s² is the sample variance.</p>
<h2 id="grassroots-statistics-2_probability_distributions-f-distribution">F-Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-f-distribution" title="Permanent link">&para;</a></h2>
<p>The F-distribution represents the ratio of two chi-square distributions divided by their respective degrees of freedom. It's fundamental for comparing variances and in ANOVA.</p>
<h3 id="grassroots-statistics-2_probability_distributions-mathematical-formulation">Mathematical Formulation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-mathematical-formulation" title="Permanent link">&para;</a></h3>
<p>For a ratio of chi-square variables with d₁ and d₂ degrees of freedom:</p>
<p><span class="arithmatex">\(f(x) = \frac{\sqrt{\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}{xB(d_1/2,d_2/2)}\)</span></p>
<p>where B is the beta function.</p>
<h3 id="grassroots-statistics-2_probability_distributions-anova-application">ANOVA Application<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-anova-application" title="Permanent link">&para;</a></h3>
<p>In one-way ANOVA, the F-statistic is:</p>
<p><span class="arithmatex">\(F = \frac{\text{Between-group variability}}{\text{Within-group variability}} = \frac{MS_{\text{between}}}{MS_{\text{within}}}\)</span></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_f_statistic</span><span class="p">(</span><span class="n">groups</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate F-statistic for one-way ANOVA&quot;&quot;&quot;</span>
    <span class="n">f_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">f_oneway</span><span class="p">(</span><span class="o">*</span><span class="n">groups</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f_stat</span><span class="p">,</span> <span class="n">p_val</span>
</code></pre></div>
<h2 id="grassroots-statistics-2_probability_distributions-exponential-and-gamma-distributions">Exponential and Gamma Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-exponential-and-gamma-distributions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-2_probability_distributions-exponential-distribution">Exponential Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-exponential-distribution" title="Permanent link">&para;</a></h3>
<p>Models the time between events in a Poisson process. Its memoryless property makes it unique.</p>
<p>Mathematical form:
<span class="arithmatex">\(f(x) = \lambda e^{-\lambda x}\)</span>, x ≥ 0</p>
<p>The mean is 1/λ and variance is 1/λ².</p>
<h3 id="grassroots-statistics-2_probability_distributions-gamma-distribution">Gamma Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-gamma-distribution" title="Permanent link">&para;</a></h3>
<p>Generalizes the exponential distribution. If X₁, ..., Xₖ are independent exponential(λ), their sum follows Gamma(k,λ).</p>
<p>PDF:
<span class="arithmatex">\(f(x) = \frac{\lambda^k x^{k-1} e^{-\lambda x}}{\Gamma(k)}\)</span></p>
<h2 id="grassroots-statistics-2_probability_distributions-beta-distribution">Beta Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-beta-distribution" title="Permanent link">&para;</a></h2>
<p>The beta distribution is defined on [0,1], making it perfect for modeling probabilities and proportions.</p>
<h3 id="grassroots-statistics-2_probability_distributions-mathematical-form">Mathematical Form<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-mathematical-form" title="Permanent link">&para;</a></h3>
<p><span class="arithmatex">\(f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}\)</span></p>
<h3 id="grassroots-statistics-2_probability_distributions-shape-parameters">Shape Parameters<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-shape-parameters" title="Permanent link">&para;</a></h3>
<p>α and β control the distribution shape:
- α, β &gt; 1: unimodal
- α = β = 1: uniform
- α &lt; 1: J-shaped
- β &lt; 1: reverse J-shaped</p>
<h3 id="grassroots-statistics-2_probability_distributions-bayesian-application">Bayesian Application<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-bayesian-application" title="Permanent link">&para;</a></h3>
<p>In Bayesian inference, beta serves as a conjugate prior for binomial probability:
- Prior: Beta(α,β)
- Data: Binomial(n,p)
- Posterior: Beta(α+successes, β+failures)</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">update_beta_parameters</span><span class="p">(</span><span class="n">prior_alpha</span><span class="p">,</span> <span class="n">prior_beta</span><span class="p">,</span> <span class="n">successes</span><span class="p">,</span> <span class="n">failures</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update beta parameters with new data&quot;&quot;&quot;</span>
    <span class="n">post_alpha</span> <span class="o">=</span> <span class="n">prior_alpha</span> <span class="o">+</span> <span class="n">successes</span>
    <span class="n">post_beta</span> <span class="o">=</span> <span class="n">prior_beta</span> <span class="o">+</span> <span class="n">failures</span>
    <span class="k">return</span> <span class="n">post_alpha</span><span class="p">,</span> <span class="n">post_beta</span>
</code></pre></div>
<p>Remember:
1. Distribution choice should be guided by data properties and theoretical considerations
2. Many distributions are interconnected through transformations
3. Computational tools help, but understanding the mathematical foundations is crucial
4. Visual inspection and formal tests should complement each other in distribution analysis</p>
<h1 id="grassroots-statistics-2_probability_distributions-discrete-probability-distributions">Discrete Probability Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete-probability-distributions" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-2_probability_distributions-bernoulli-and-binomial-distributions">Bernoulli and Binomial Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-bernoulli-and-binomial-distributions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-2_probability_distributions-bernoulli-distribution">Bernoulli Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-bernoulli-distribution" title="Permanent link">&para;</a></h3>
<p>The Bernoulli distribution is the fundamental building block for many discrete distributions, modeling a single binary outcome. Think of it as a single coin flip or any yes/no experiment.</p>
<p><strong>Mathematical Formulation:</strong>
Let X be a Bernoulli random variable with parameter p. Then:</p>
<p>P(X = x) = p^x * (1-p)^(1-x), x ∈ {0,1}</p>
<p>The elegance of this formulation lies in how it captures both outcomes in a single expression:
- When x = 1: P(X = 1) = p
- When x = 0: P(X = 0) = 1-p</p>
<p><strong>Key Properties:</strong>
- E[X] = p
- Var(X) = p(1-p)
- All higher moments can be derived from p
- Moment Generating Function: M(t) = 1-p + pe^t</p>
<p>For practical implementation, we can use Python's built-in random module for simple cases:
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="k">def</span><span class="w"> </span><span class="nf">bernoulli_trial</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">p</span> <span class="k">else</span> <span class="mi">0</span>
</code></pre></div></p>
<h3 id="grassroots-statistics-2_probability_distributions-binomial-distribution">Binomial Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-binomial-distribution" title="Permanent link">&para;</a></h3>
<p>The binomial distribution naturally extends the Bernoulli to model the sum of n independent trials. Imagine flipping a coin n times and counting the total number of heads.</p>
<p><strong>Mathematical Formulation:</strong>
For n trials with success probability p:</p>
<p>P(X = k) = (n choose k) * p^k * (1-p)^(n-k)</p>
<p>where (n choose k) = n!/(k!(n-k)!)</p>
<p>The binomial coefficient (n choose k) represents the number of ways to choose k successes from n trials, making this a beautiful combination of combinatorics and probability.</p>
<p><strong>Probability Calculation Example:</strong>
For small values, we can compute this directly:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">comb</span>

<span class="k">def</span><span class="w"> </span><span class="nf">binomial_probability</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="o">**</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">))</span>
</code></pre></div></p>
<p>For larger values where numerical stability is important, we should use specialized libraries:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div></p>
<h2 id="grassroots-statistics-2_probability_distributions-poisson-distribution">Poisson Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-poisson-distribution" title="Permanent link">&para;</a></h2>
<p>The Poisson distribution models rare events occurring in a fixed interval. What makes it special is that we only need to know the average rate λ to describe the entire distribution.</p>
<p><strong>Mathematical Foundation:</strong>
P(X = k) = (λ^k * e^(-λ)) / k!</p>
<p>This elegant formula emerges as the limit of a binomial distribution when n → ∞ and p → 0 while np = λ remains constant. This limiting relationship provides deep insight into why the Poisson distribution appears so often in nature.</p>
<p><strong>Moment Properties:</strong>
- E[X] = λ
- Var(X) = λ
- All cumulants = λ</p>
<p>This equality of mean and variance is a defining characteristic that can help identify Poisson processes in real data.</p>
<h2 id="grassroots-statistics-2_probability_distributions-geometric-and-negative-binomial">Geometric and Negative Binomial<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-geometric-and-negative-binomial" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-2_probability_distributions-geometric-distribution">Geometric Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-geometric-distribution" title="Permanent link">&para;</a></h3>
<p>The geometric distribution models the waiting time until the first success in repeated trials. Its memoryless property makes it unique among discrete distributions.</p>
<p><strong>Mathematical Insight:</strong>
P(X = k) = p(1-p)^(k-1)</p>
<p>The memoryless property means:
P(X &gt; m + n | X &gt; m) = P(X &gt; n)</p>
<p>This counterintuitive property tells us that the distribution "forgets" its past attempts.</p>
<h3 id="grassroots-statistics-2_probability_distributions-negative-binomial">Negative Binomial<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-negative-binomial" title="Permanent link">&para;</a></h3>
<p>Generalizing the geometric distribution, the negative binomial models the number of trials until r successes. </p>
<p><strong>Mathematical Form:</strong>
P(X = k) = ((k-1) choose (r-1)) * p^r * (1-p)^(k-r)</p>
<p>This can be understood as waiting for the rth success, with k-r failures along the way. The combinatorial term accounts for all possible arrangements of these failures.</p>
<h2 id="grassroots-statistics-2_probability_distributions-hypergeometric-distribution">Hypergeometric Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-hypergeometric-distribution" title="Permanent link">&para;</a></h2>
<p>Unlike the binomial, the hypergeometric distribution models sampling without replacement, making each draw dependent on previous draws.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p>P(X = k) = [C(K,k) * C(N-K,n-k)] / C(N,n)</p>
<p>Where:
- N = population size
- K = success states in population
- n = sample size
- k = observed successes</p>
<p>The denominator C(N,n) represents all possible samples, while the numerator counts favorable outcomes through a clever application of the multiplication principle.</p>
<p><strong>Expected Value:</strong>
E[X] = n(K/N)</p>
<p>This intuitive result shows that the expected proportion of successes in the sample equals the proportion in the population.</p>
<h2 id="grassroots-statistics-2_probability_distributions-distribution-relationships">Distribution Relationships<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-distribution-relationships" title="Permanent link">&para;</a></h2>
<p>The relationships between these distributions reveal deep mathematical connections:</p>
<ol>
<li>
<p><strong>Binomial and Poisson:</strong>
When n is large and p is small:
Binomial(n,p) ≈ Poisson(np)</p>
</li>
<li>
<p><strong>Geometric and Negative Binomial:</strong>
Geometric(p) = NegativeBinomial(1,p)</p>
</li>
<li>
<p><strong>Binomial and Hypergeometric:</strong>
As N → ∞, Hypergeometric(N,K,n) → Binomial(n,K/N)</p>
</li>
</ol>
<p>For computational work, these relationships often suggest efficient approximations:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">approximate_large_binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use Poisson approximation for large n, small p&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mi">100</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div></p>
<p>Remember:
- The choice between mathematical and computational approaches should be guided by both theoretical considerations and practical constraints
- Understanding the mathematical foundations helps in selecting appropriate approximations
- Modern computational tools make exact calculations feasible in many cases where approximations were historically necessary
- The elegance of these distributions lies in their ability to model complex phenomena with simple parameters</p></section><section class="print-page" id="grassroots-statistics-2_probability_distributions-continuous_distributions"><h1 id="grassroots-statistics-2_probability_distributions-continuous_distributions-continuous-probability-distributions">Continuous Probability Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-continuous-probability-distributions" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-2_probability_distributions-continuous_distributions-normalgaussian-distribution">Normal/Gaussian Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-normalgaussian-distribution" title="Permanent link">&para;</a></h2>
<p>The normal distribution is fundamental to statistics, arising naturally in many phenomena due to the Central Limit Theorem. Its mathematical elegance and theoretical properties make it the cornerstone of statistical inference.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-foundation">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-foundation" title="Permanent link">&para;</a></h3>
<p>The probability density function is given by:</p>
<p><span class="arithmatex">\(f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span></p>
<p>where:
- μ determines the center (location)
- σ controls the spread (scale)</p>
<p>The standard normal distribution (μ = 0, σ = 1) simplifies this to:</p>
<p><span class="arithmatex">\(f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}\)</span></p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-key-properties">Key Properties<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-key-properties" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Symmetry</strong>: The distribution is perfectly symmetric around μ</li>
<li><strong>Empirical Rule</strong>:</li>
<li>μ ± σ contains ≈ 68% of data</li>
<li>μ ± 2σ contains ≈ 95% of data</li>
<li>μ ± 3σ contains ≈ 99.7% of data</li>
</ol>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-implementation">Implementation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-implementation" title="Permanent link">&para;</a></h3>
<p>For practical applications, we can use Python:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">stats</span>

<span class="c1"># Calculate probability between -1 and 1 standard deviations</span>
<span class="n">prob</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Probability within 1σ: </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># ≈ 0.6827</span>
</code></pre></div>
<h2 id="grassroots-statistics-2_probability_distributions-continuous_distributions-students-t-distribution">Student's t-Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-students-t-distribution" title="Permanent link">&para;</a></h2>
<p>The t-distribution emerges when estimating the mean of a normally distributed population when the sample size is small and the population standard deviation is unknown.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-foundation_1">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-foundation_1" title="Permanent link">&para;</a></h3>
<p>The probability density function is:</p>
<p><span class="arithmatex">\(f(t) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})} \left(1+\frac{t^2}{\nu}\right)^{-\frac{\nu+1}{2}}\)</span></p>
<p>where ν represents the degrees of freedom.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-intuition">Intuition<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-intuition" title="Permanent link">&para;</a></h3>
<p>Think of the t-distribution as a "more uncertain" normal distribution. As sample size increases (ν increases), we become more certain about our estimates, and the t-distribution approaches the normal distribution.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-critical-values">Critical Values<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-critical-values" title="Permanent link">&para;</a></h3>
<p>For hypothesis testing, we often need critical values. The relationship between confidence levels and t-values depends on ν:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_t_critical</span><span class="p">(</span><span class="n">confidence_level</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate two-tailed critical t-value&quot;&quot;&quot;</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">confidence_level</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
</code></pre></div>
<h2 id="grassroots-statistics-2_probability_distributions-continuous_distributions-chi-square-distribution">Chi-Square Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-chi-square-distribution" title="Permanent link">&para;</a></h2>
<p>The chi-square distribution represents the sum of squared standard normal variables. It's crucial for variance-related inference and categorical data analysis.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-foundation_2">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-foundation_2" title="Permanent link">&para;</a></h3>
<p>For k degrees of freedom:</p>
<p><span class="arithmatex">\(f(x) = \frac{1}{2^{k/2}\Gamma(k/2)} x^{k/2-1}e^{-x/2}\)</span></p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-properties">Properties<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-properties" title="Permanent link">&para;</a></h3>
<p>Expected value: E(X) = k
Variance: Var(X) = 2k</p>
<p>The distribution becomes more symmetric as k increases, approaching normality.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-application-example-testing-variance">Application Example: Testing Variance<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-application-example-testing-variance" title="Permanent link">&para;</a></h3>
<p>To test if a sample comes from a population with a specified variance σ²₀:</p>
<p><span class="arithmatex">\(\chi^2 = \frac{(n-1)s^2}{\sigma_0^2}\)</span></p>
<p>where s² is the sample variance.</p>
<h2 id="grassroots-statistics-2_probability_distributions-continuous_distributions-f-distribution">F-Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-f-distribution" title="Permanent link">&para;</a></h2>
<p>The F-distribution represents the ratio of two chi-square distributions divided by their respective degrees of freedom. It's fundamental for comparing variances and in ANOVA.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-formulation">Mathematical Formulation<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-formulation" title="Permanent link">&para;</a></h3>
<p>For a ratio of chi-square variables with d₁ and d₂ degrees of freedom:</p>
<p><span class="arithmatex">\(f(x) = \frac{\sqrt{\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}{xB(d_1/2,d_2/2)}\)</span></p>
<p>where B is the beta function.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-anova-application">ANOVA Application<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-anova-application" title="Permanent link">&para;</a></h3>
<p>In one-way ANOVA, the F-statistic is:</p>
<p><span class="arithmatex">\(F = \frac{\text{Between-group variability}}{\text{Within-group variability}} = \frac{MS_{\text{between}}}{MS_{\text{within}}}\)</span></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_f_statistic</span><span class="p">(</span><span class="n">groups</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate F-statistic for one-way ANOVA&quot;&quot;&quot;</span>
    <span class="n">f_stat</span><span class="p">,</span> <span class="n">p_val</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">f_oneway</span><span class="p">(</span><span class="o">*</span><span class="n">groups</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f_stat</span><span class="p">,</span> <span class="n">p_val</span>
</code></pre></div>
<h2 id="grassroots-statistics-2_probability_distributions-continuous_distributions-exponential-and-gamma-distributions">Exponential and Gamma Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-exponential-and-gamma-distributions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-exponential-distribution">Exponential Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-exponential-distribution" title="Permanent link">&para;</a></h3>
<p>Models the time between events in a Poisson process. Its memoryless property makes it unique.</p>
<p>Mathematical form:
<span class="arithmatex">\(f(x) = \lambda e^{-\lambda x}\)</span>, x ≥ 0</p>
<p>The mean is 1/λ and variance is 1/λ².</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-gamma-distribution">Gamma Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-gamma-distribution" title="Permanent link">&para;</a></h3>
<p>Generalizes the exponential distribution. If X₁, ..., Xₖ are independent exponential(λ), their sum follows Gamma(k,λ).</p>
<p>PDF:
<span class="arithmatex">\(f(x) = \frac{\lambda^k x^{k-1} e^{-\lambda x}}{\Gamma(k)}\)</span></p>
<h2 id="grassroots-statistics-2_probability_distributions-continuous_distributions-beta-distribution">Beta Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-beta-distribution" title="Permanent link">&para;</a></h2>
<p>The beta distribution is defined on [0,1], making it perfect for modeling probabilities and proportions.</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-form">Mathematical Form<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-mathematical-form" title="Permanent link">&para;</a></h3>
<p><span class="arithmatex">\(f(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}\)</span></p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-shape-parameters">Shape Parameters<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-shape-parameters" title="Permanent link">&para;</a></h3>
<p>α and β control the distribution shape:
- α, β &gt; 1: unimodal
- α = β = 1: uniform
- α &lt; 1: J-shaped
- β &lt; 1: reverse J-shaped</p>
<h3 id="grassroots-statistics-2_probability_distributions-continuous_distributions-bayesian-application">Bayesian Application<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-continuous_distributions-bayesian-application" title="Permanent link">&para;</a></h3>
<p>In Bayesian inference, beta serves as a conjugate prior for binomial probability:
- Prior: Beta(α,β)
- Data: Binomial(n,p)
- Posterior: Beta(α+successes, β+failures)</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">update_beta_parameters</span><span class="p">(</span><span class="n">prior_alpha</span><span class="p">,</span> <span class="n">prior_beta</span><span class="p">,</span> <span class="n">successes</span><span class="p">,</span> <span class="n">failures</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Update beta parameters with new data&quot;&quot;&quot;</span>
    <span class="n">post_alpha</span> <span class="o">=</span> <span class="n">prior_alpha</span> <span class="o">+</span> <span class="n">successes</span>
    <span class="n">post_beta</span> <span class="o">=</span> <span class="n">prior_beta</span> <span class="o">+</span> <span class="n">failures</span>
    <span class="k">return</span> <span class="n">post_alpha</span><span class="p">,</span> <span class="n">post_beta</span>
</code></pre></div>
<p>Remember:
1. Distribution choice should be guided by data properties and theoretical considerations
2. Many distributions are interconnected through transformations
3. Computational tools help, but understanding the mathematical foundations is crucial
4. Visual inspection and formal tests should complement each other in distribution analysis</p></section><section class="print-page" id="grassroots-statistics-2_probability_distributions-discrete_distributions"><h1 id="grassroots-statistics-2_probability_distributions-discrete_distributions-discrete-probability-distributions">Discrete Probability Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-discrete-probability-distributions" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-2_probability_distributions-discrete_distributions-bernoulli-and-binomial-distributions">Bernoulli and Binomial Distributions<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-bernoulli-and-binomial-distributions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-2_probability_distributions-discrete_distributions-bernoulli-distribution">Bernoulli Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-bernoulli-distribution" title="Permanent link">&para;</a></h3>
<p>The Bernoulli distribution is the fundamental building block for many discrete distributions, modeling a single binary outcome. Think of it as a single coin flip or any yes/no experiment.</p>
<p><strong>Mathematical Formulation:</strong>
Let X be a Bernoulli random variable with parameter p. Then:</p>
<p>P(X = x) = p^x * (1-p)^(1-x), x ∈ {0,1}</p>
<p>The elegance of this formulation lies in how it captures both outcomes in a single expression:
- When x = 1: P(X = 1) = p
- When x = 0: P(X = 0) = 1-p</p>
<p><strong>Key Properties:</strong>
- E[X] = p
- Var(X) = p(1-p)
- All higher moments can be derived from p
- Moment Generating Function: M(t) = 1-p + pe^t</p>
<p>For practical implementation, we can use Python's built-in random module for simple cases:
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="k">def</span><span class="w"> </span><span class="nf">bernoulli_trial</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">p</span> <span class="k">else</span> <span class="mi">0</span>
</code></pre></div></p>
<h3 id="grassroots-statistics-2_probability_distributions-discrete_distributions-binomial-distribution">Binomial Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-binomial-distribution" title="Permanent link">&para;</a></h3>
<p>The binomial distribution naturally extends the Bernoulli to model the sum of n independent trials. Imagine flipping a coin n times and counting the total number of heads.</p>
<p><strong>Mathematical Formulation:</strong>
For n trials with success probability p:</p>
<p>P(X = k) = (n choose k) * p^k * (1-p)^(n-k)</p>
<p>where (n choose k) = n!/(k!(n-k)!)</p>
<p>The binomial coefficient (n choose k) represents the number of ways to choose k successes from n trials, making this a beautiful combination of combinatorics and probability.</p>
<p><strong>Probability Calculation Example:</strong>
For small values, we can compute this directly:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">comb</span>

<span class="k">def</span><span class="w"> </span><span class="nf">binomial_probability</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">comb</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="o">**</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">))</span>
</code></pre></div></p>
<p>For larger values where numerical stability is important, we should use specialized libraries:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div></p>
<h2 id="grassroots-statistics-2_probability_distributions-discrete_distributions-poisson-distribution">Poisson Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-poisson-distribution" title="Permanent link">&para;</a></h2>
<p>The Poisson distribution models rare events occurring in a fixed interval. What makes it special is that we only need to know the average rate λ to describe the entire distribution.</p>
<p><strong>Mathematical Foundation:</strong>
P(X = k) = (λ^k * e^(-λ)) / k!</p>
<p>This elegant formula emerges as the limit of a binomial distribution when n → ∞ and p → 0 while np = λ remains constant. This limiting relationship provides deep insight into why the Poisson distribution appears so often in nature.</p>
<p><strong>Moment Properties:</strong>
- E[X] = λ
- Var(X) = λ
- All cumulants = λ</p>
<p>This equality of mean and variance is a defining characteristic that can help identify Poisson processes in real data.</p>
<h2 id="grassroots-statistics-2_probability_distributions-discrete_distributions-geometric-and-negative-binomial">Geometric and Negative Binomial<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-geometric-and-negative-binomial" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-2_probability_distributions-discrete_distributions-geometric-distribution">Geometric Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-geometric-distribution" title="Permanent link">&para;</a></h3>
<p>The geometric distribution models the waiting time until the first success in repeated trials. Its memoryless property makes it unique among discrete distributions.</p>
<p><strong>Mathematical Insight:</strong>
P(X = k) = p(1-p)^(k-1)</p>
<p>The memoryless property means:
P(X &gt; m + n | X &gt; m) = P(X &gt; n)</p>
<p>This counterintuitive property tells us that the distribution "forgets" its past attempts.</p>
<h3 id="grassroots-statistics-2_probability_distributions-discrete_distributions-negative-binomial">Negative Binomial<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-negative-binomial" title="Permanent link">&para;</a></h3>
<p>Generalizing the geometric distribution, the negative binomial models the number of trials until r successes. </p>
<p><strong>Mathematical Form:</strong>
P(X = k) = ((k-1) choose (r-1)) * p^r * (1-p)^(k-r)</p>
<p>This can be understood as waiting for the rth success, with k-r failures along the way. The combinatorial term accounts for all possible arrangements of these failures.</p>
<h2 id="grassroots-statistics-2_probability_distributions-discrete_distributions-hypergeometric-distribution">Hypergeometric Distribution<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-hypergeometric-distribution" title="Permanent link">&para;</a></h2>
<p>Unlike the binomial, the hypergeometric distribution models sampling without replacement, making each draw dependent on previous draws.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p>P(X = k) = [C(K,k) * C(N-K,n-k)] / C(N,n)</p>
<p>Where:
- N = population size
- K = success states in population
- n = sample size
- k = observed successes</p>
<p>The denominator C(N,n) represents all possible samples, while the numerator counts favorable outcomes through a clever application of the multiplication principle.</p>
<p><strong>Expected Value:</strong>
E[X] = n(K/N)</p>
<p>This intuitive result shows that the expected proportion of successes in the sample equals the proportion in the population.</p>
<h2 id="grassroots-statistics-2_probability_distributions-discrete_distributions-distribution-relationships">Distribution Relationships<a class="headerlink" href="#grassroots-statistics-2_probability_distributions-discrete_distributions-distribution-relationships" title="Permanent link">&para;</a></h2>
<p>The relationships between these distributions reveal deep mathematical connections:</p>
<ol>
<li>
<p><strong>Binomial and Poisson:</strong>
When n is large and p is small:
Binomial(n,p) ≈ Poisson(np)</p>
</li>
<li>
<p><strong>Geometric and Negative Binomial:</strong>
Geometric(p) = NegativeBinomial(1,p)</p>
</li>
<li>
<p><strong>Binomial and Hypergeometric:</strong>
As N → ∞, Hypergeometric(N,K,n) → Binomial(n,K/N)</p>
</li>
</ol>
<p>For computational work, these relationships often suggest efficient approximations:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">approximate_large_binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Use Poisson approximation for large n, small p&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mi">100</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="o">*</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div></p>
<p>Remember:
- The choice between mathematical and computational approaches should be guided by both theoretical considerations and practical constraints
- Understanding the mathematical foundations helps in selecting appropriate approximations
- Modern computational tools make exact calculations feasible in many cases where approximations were historically necessary
- The elegance of these distributions lies in their ability to model complex phenomena with simple parameters</p></section><h1 class='nav-section-title-end'>Ended: 2 probability distributions</h1>
                        <h3 class='nav-section-title' id='section-3-computational-rules'>
                            3 computational rules <a class='headerlink' href='#section-3-computational-rules' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-statistics-3_computational_rules"><h1 id="grassroots-statistics-3_computational_rules-basic-probability-computation-rules">Basic Probability Computation Rules<a class="headerlink" href="#grassroots-statistics-3_computational_rules-basic-probability-computation-rules" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-3_computational_rules-1-fundamental-concepts">1. Fundamental Concepts<a class="headerlink" href="#grassroots-statistics-3_computational_rules-1-fundamental-concepts" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-3_computational_rules-sample-space">Sample Space<a class="headerlink" href="#grassroots-statistics-3_computational_rules-sample-space" title="Permanent link">&para;</a></h3>
<p>The sample space (Ω) represents all possible outcomes of a random experiment. For example, when rolling a die, Ω = {1, 2, 3, 4, 5, 6}.</p>
<h3 id="grassroots-statistics-3_computational_rules-events">Events<a class="headerlink" href="#grassroots-statistics-3_computational_rules-events" title="Permanent link">&para;</a></h3>
<p>An event is a subset of the sample space. We typically denote events with capital letters (A, B, etc.).</p>
<h3 id="grassroots-statistics-3_computational_rules-probability-measure">Probability Measure<a class="headerlink" href="#grassroots-statistics-3_computational_rules-probability-measure" title="Permanent link">&para;</a></h3>
<p>For any event A, the probability P(A) must satisfy:
- 0 ≤ P(A) ≤ 1
- P(Ω) = 1
- P(∅) = 0</p>
<h2 id="grassroots-statistics-3_computational_rules-2-addition-rules">2. Addition Rules<a class="headerlink" href="#grassroots-statistics-3_computational_rules-2-addition-rules" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-3_computational_rules-basic-addition-rule">Basic Addition Rule<a class="headerlink" href="#grassroots-statistics-3_computational_rules-basic-addition-rule" title="Permanent link">&para;</a></h3>
<p>For mutually exclusive events A and B:
P(A ∪ B) = P(A) + P(B)</p>
<h3 id="grassroots-statistics-3_computational_rules-general-addition-rule">General Addition Rule<a class="headerlink" href="#grassroots-statistics-3_computational_rules-general-addition-rule" title="Permanent link">&para;</a></h3>
<p>For any two events A and B:
P(A ∪ B) = P(A) + P(B) - P(A ∩ B)</p>
<h3 id="grassroots-statistics-3_computational_rules-extension-to-multiple-events">Extension to Multiple Events<a class="headerlink" href="#grassroots-statistics-3_computational_rules-extension-to-multiple-events" title="Permanent link">&para;</a></h3>
<p>For three events A, B, and C:
P(A ∪ B ∪ C) = P(A) + P(B) + P(C) - P(A ∩ B) - P(A ∩ C) - P(B ∩ C) + P(A ∩ B ∩ C)</p>
<h2 id="grassroots-statistics-3_computational_rules-3-multiplication-rules">3. Multiplication Rules<a class="headerlink" href="#grassroots-statistics-3_computational_rules-3-multiplication-rules" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-3_computational_rules-independent-events">Independent Events<a class="headerlink" href="#grassroots-statistics-3_computational_rules-independent-events" title="Permanent link">&para;</a></h3>
<p>Two events A and B are independent if:
P(A ∩ B) = P(A) × P(B)</p>
<h3 id="grassroots-statistics-3_computational_rules-dependent-events">Dependent Events<a class="headerlink" href="#grassroots-statistics-3_computational_rules-dependent-events" title="Permanent link">&para;</a></h3>
<p>For dependent events, use conditional probability:
P(A ∩ B) = P(A) × P(B|A)</p>
<h3 id="grassroots-statistics-3_computational_rules-chain-rule">Chain Rule<a class="headerlink" href="#grassroots-statistics-3_computational_rules-chain-rule" title="Permanent link">&para;</a></h3>
<p>For multiple events:
P(A ∩ B ∩ C) = P(A) × P(B|A) × P(C|A ∩ B)</p>
<h2 id="grassroots-statistics-3_computational_rules-4-important-relationships">4. Important Relationships<a class="headerlink" href="#grassroots-statistics-3_computational_rules-4-important-relationships" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-3_computational_rules-mutually-exclusive-events">Mutually Exclusive Events<a class="headerlink" href="#grassroots-statistics-3_computational_rules-mutually-exclusive-events" title="Permanent link">&para;</a></h3>
<p>Events A and B are mutually exclusive if:
- A ∩ B = ∅
- P(A ∩ B) = 0</p>
<h3 id="grassroots-statistics-3_computational_rules-complement-rule">Complement Rule<a class="headerlink" href="#grassroots-statistics-3_computational_rules-complement-rule" title="Permanent link">&para;</a></h3>
<p>For any event A:
P(A') = 1 - P(A)</p>
<h3 id="grassroots-statistics-3_computational_rules-conditional-probability">Conditional Probability<a class="headerlink" href="#grassroots-statistics-3_computational_rules-conditional-probability" title="Permanent link">&para;</a></h3>
<p>P(B|A) = P(A ∩ B) / P(A), where P(A) &gt; 0</p>
<h2 id="grassroots-statistics-3_computational_rules-5-common-mistakes-and-pitfalls">5. Common Mistakes and Pitfalls<a class="headerlink" href="#grassroots-statistics-3_computational_rules-5-common-mistakes-and-pitfalls" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-3_computational_rules-independence-vs-mutual-exclusivity">Independence vs. Mutual Exclusivity<a class="headerlink" href="#grassroots-statistics-3_computational_rules-independence-vs-mutual-exclusivity" title="Permanent link">&para;</a></h3>
<ul>
<li>Independent events CAN occur together</li>
<li>Mutually exclusive events CANNOT occur together</li>
<li>Two events cannot be both independent and mutually exclusive (unless one has probability 0)</li>
</ul>
<h3 id="grassroots-statistics-3_computational_rules-addition-rule-selection">Addition Rule Selection<a class="headerlink" href="#grassroots-statistics-3_computational_rules-addition-rule-selection" title="Permanent link">&para;</a></h3>
<ul>
<li>Use basic addition rule ONLY for mutually exclusive events</li>
<li>Always use general addition rule when events can overlap</li>
</ul>
<h2 id="grassroots-statistics-3_computational_rules-6-practical-applications">6. Practical Applications<a class="headerlink" href="#grassroots-statistics-3_computational_rules-6-practical-applications" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-3_computational_rules-example-card-drawing">Example: Card Drawing<a class="headerlink" href="#grassroots-statistics-3_computational_rules-example-card-drawing" title="Permanent link">&para;</a></h3>
<p>Drawing cards from a standard deck:
- P(Drawing a King) = 4/52 = 1/13
- P(Drawing a Heart) = 13/52 = 1/4
- P(Drawing a King of Hearts) = 1/52
- P(Drawing a King OR a Heart) = 15/52 (using general addition rule)</p>
<h3 id="grassroots-statistics-3_computational_rules-example-rolling-dice">Example: Rolling Dice<a class="headerlink" href="#grassroots-statistics-3_computational_rules-example-rolling-dice" title="Permanent link">&para;</a></h3>
<p>Rolling two dice:
- P(Sum = 7) = 6/36 = 1/6
- P(First die = 6) = 1/6
- These events are independent: knowing the sum doesn't tell us the value of the first die</p>
<h2 id="grassroots-statistics-3_computational_rules-7-verification-methods">7. Verification Methods<a class="headerlink" href="#grassroots-statistics-3_computational_rules-7-verification-methods" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-3_computational_rules-testing-for-independence">Testing for Independence<a class="headerlink" href="#grassroots-statistics-3_computational_rules-testing-for-independence" title="Permanent link">&para;</a></h3>
<p>To verify if events A and B are independent, check if:
P(A ∩ B) = P(A) × P(B)</p>
<h3 id="grassroots-statistics-3_computational_rules-testing-for-mutual-exclusivity">Testing for Mutual Exclusivity<a class="headerlink" href="#grassroots-statistics-3_computational_rules-testing-for-mutual-exclusivity" title="Permanent link">&para;</a></h3>
<p>To verify if events A and B are mutually exclusive, check if:
P(A ∩ B) = 0</p>
<h2 id="grassroots-statistics-3_computational_rules-8-additional-considerations">8. Additional Considerations<a class="headerlink" href="#grassroots-statistics-3_computational_rules-8-additional-considerations" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-3_computational_rules-law-of-total-probability">Law of Total Probability<a class="headerlink" href="#grassroots-statistics-3_computational_rules-law-of-total-probability" title="Permanent link">&para;</a></h3>
<p>For a partition of events B₁, B₂, ..., Bₙ:
P(A) = P(A|B₁)P(B₁) + P(A|B₂)P(B₂) + ... + P(A|Bₙ)P(Bₙ)</p>
<h3 id="grassroots-statistics-3_computational_rules-bayes-theorem">Bayes' Theorem<a class="headerlink" href="#grassroots-statistics-3_computational_rules-bayes-theorem" title="Permanent link">&para;</a></h3>
<p>P(A|B) = P(B|A)P(A) / P(B)</p>
<h2 id="grassroots-statistics-3_computational_rules-practice-problems">Practice Problems<a class="headerlink" href="#grassroots-statistics-3_computational_rules-practice-problems" title="Permanent link">&para;</a></h2>
<ol>
<li>If P(A) = 0.3, P(B) = 0.4, and P(A ∩ B) = 0.12, calculate:</li>
<li>P(A ∪ B)</li>
<li>
<p>Are A and B independent?</p>
</li>
<li>
<p>Three cards are drawn from a deck without replacement. Calculate:</p>
</li>
<li>P(All are aces)</li>
<li>P(All are the same suit)</li>
</ol>
<p>[Solutions provided in separate section]</p></section><h1 class='nav-section-title-end'>Ended: 3 computational rules</h1>
                        <h3 class='nav-section-title' id='section-4-statistical-inference'>
                            4 statistical inference <a class='headerlink' href='#section-4-statistical-inference' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-statistics-4_statistical_inference"><h1 id="grassroots-statistics-4_statistical_inference-statistical-inference">Statistical Inference<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-statistical-inference" title="Permanent link">&para;</a></h1>
<h1 id="grassroots-statistics-4_statistical_inference-statistical-hypothesis-testing">Statistical Hypothesis Testing<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-statistical-hypothesis-testing" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-4_statistical_inference-fundamental-concepts">Fundamental Concepts<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-fundamental-concepts" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-null-and-alternative-hypotheses">Null and Alternative Hypotheses<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-null-and-alternative-hypotheses" title="Permanent link">&para;</a></h3>
<p>The foundation of hypothesis testing lies in formulating two competing claims about a population parameter:</p>
<ol>
<li><strong>Null Hypothesis (H₀)</strong>: The default position, typically expressing "no effect" or "no difference"</li>
<li><strong>Alternative Hypothesis (H₁ or Hₐ)</strong>: The research claim we want to support with evidence</li>
</ol>
<p>For a population parameter θ, these are typically expressed in one of three ways:</p>
<p><strong>Two-sided test:</strong>
<div class="highlight"><pre><span></span><code>H₀: θ = θ₀
H₁: θ ≠ θ₀
</code></pre></div></p>
<p><strong>One-sided tests:</strong>
<div class="highlight"><pre><span></span><code>Upper-tailed:          Lower-tailed:
H₀: θ ≤ θ₀            H₀: θ ≥ θ₀
H₁: θ &gt; θ₀            H₁: θ &lt; θ₀
</code></pre></div></p>
<h3 id="grassroots-statistics-4_statistical_inference-test-statistics-and-sampling-distributions">Test Statistics and Sampling Distributions<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-test-statistics-and-sampling-distributions" title="Permanent link">&para;</a></h3>
<p>Most test statistics follow the general form:</p>
<div class="highlight"><pre><span></span><code>test statistic = (sample estimate - null value) / (standard error)
</code></pre></div>
<p>For example, the z-test statistic for a population mean:</p>
<div class="highlight"><pre><span></span><code>z = (x̄ - μ₀) / (σ/√n)
</code></pre></div>
<p>where:
- x̄ is the sample mean
- μ₀ is the hypothesized population mean
- σ is the population standard deviation
- n is the sample size</p>
<p>When σ is unknown and estimated by s, we use the t-statistic:</p>
<div class="highlight"><pre><span></span><code>t = (x̄ - μ₀) / (s/√n)
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-type-i-and-type-ii-errors">Type I and Type II Errors<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-type-i-and-type-ii-errors" title="Permanent link">&para;</a></h2>
<p>The decision process in hypothesis testing can lead to two types of errors:</p>
<table>
<thead>
<tr>
<th></th>
<th>H₀ True</th>
<th>H₀ False</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reject H₀</td>
<td>Type I Error (α)</td>
<td>Correct Decision</td>
</tr>
<tr>
<td>Fail to Reject H₀</td>
<td>Correct Decision</td>
<td>Type II Error (β)</td>
</tr>
</tbody>
</table>
<p>The probability relationships:
<div class="highlight"><pre><span></span><code>P(Type I Error) = α = P(Reject H₀ | H₀ true)
P(Type II Error) = β = P(Fail to Reject H₀ | H₁ true)
Power = 1 - β = P(Reject H₀ | H₁ true)
</code></pre></div></p>
<h2 id="grassroots-statistics-4_statistical_inference-p-values-and-statistical-power">P-values and Statistical Power<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-p-values-and-statistical-power" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-p-value">P-value<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-p-value" title="Permanent link">&para;</a></h3>
<p>The p-value is defined mathematically as:</p>
<p>For a test statistic T and observed value t*:
<div class="highlight"><pre><span></span><code>Two-sided: p = 2 * P(T ≥ |t*| | H₀)
Upper-tailed: p = P(T ≥ t* | H₀)
Lower-tailed: p = P(T ≤ t* | H₀)
</code></pre></div></p>
<p>For practical computation in Python:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_p_value</span><span class="p">(</span><span class="n">test_statistic</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">two_sided</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">two_sided</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">test_statistic</span><span class="p">)))</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">test_statistic</span><span class="p">)</span>
</code></pre></div></p>
<h3 id="grassroots-statistics-4_statistical_inference-statistical-power">Statistical Power<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-statistical-power" title="Permanent link">&para;</a></h3>
<p>Power depends on four interrelated quantities:
1. Effect size (δ)
2. Sample size (n)
3. Significance level (α)
4. Power (1-β)</p>
<p>For a two-sided z-test, the power function is:</p>
<div class="highlight"><pre><span></span><code>Power = 1 - β = Φ(zα/2 + δ√n/σ) + Φ(-zα/2 + δ√n/σ)
</code></pre></div>
<p>where:
- Φ is the standard normal CDF
- zα/2 is the critical value
- δ is the true difference from null
- σ is the population standard deviation</p>
<h2 id="grassroots-statistics-4_statistical_inference-multiple-testing-problem">Multiple Testing Problem<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-multiple-testing-problem" title="Permanent link">&para;</a></h2>
<p>When conducting m independent tests at significance level α, the probability of at least one Type I error (Family-Wise Error Rate, FWER) is:</p>
<div class="highlight"><pre><span></span><code>FWER = 1 - (1-α)ᵐ
</code></pre></div>
<h3 id="grassroots-statistics-4_statistical_inference-bonferroni-correction">Bonferroni Correction<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-bonferroni-correction" title="Permanent link">&para;</a></h3>
<p>Controls FWER by adjusting the significance level:
<div class="highlight"><pre><span></span><code>α_adjusted = α/m
</code></pre></div></p>
<h3 id="grassroots-statistics-4_statistical_inference-benjamini-hochberg-procedure">Benjamini-Hochberg Procedure<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-benjamini-hochberg-procedure" title="Permanent link">&para;</a></h3>
<p>Controls False Discovery Rate (FDR). For ordered p-values p₁ ≤ p₂ ≤ ... ≤ pₘ:
1. Find largest k where p_k ≤ (k/m)α
2. Reject all hypotheses H₍ᵢ₎ for i = 1,...,k</p>
<p>Implementation combining mathematical insight with computation:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">benjamini_hochberg</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements Benjamini-Hochberg procedure</span>
<span class="sd">    Returns: boolean array of rejected null hypotheses</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span>
    <span class="n">ranked</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">)</span>
    <span class="n">critical_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">ranked</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>
    <span class="n">sorted_p_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span>

    <span class="c1"># Find largest k where p_k ≤ (k/m)α</span>
    <span class="n">significant</span> <span class="o">=</span> <span class="n">p_values</span> <span class="o">&lt;=</span> <span class="n">critical_values</span>
    <span class="k">return</span> <span class="n">significant</span>
</code></pre></div></p>
<h2 id="grassroots-statistics-4_statistical_inference-power-analysis-example">Power Analysis Example<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-power-analysis-example" title="Permanent link">&para;</a></h2>
<p>Let's combine mathematical formulation with computation for a t-test power analysis:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">power_analysis</span><span class="p">(</span><span class="n">effect_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">two_sided</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate power for two-sample t-test</span>

<span class="sd">    Parameters:</span>
<span class="sd">    effect_size (d) = (μ₁ - μ₂)/σ</span>
<span class="sd">    n = sample size per group</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Critical value</span>
    <span class="n">df</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span>  <span class="c1"># degrees of freedom</span>
    <span class="k">if</span> <span class="n">two_sided</span><span class="p">:</span>
        <span class="n">t_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">t_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>

    <span class="c1"># Non-centrality parameter</span>
    <span class="n">ncp</span> <span class="o">=</span> <span class="n">effect_size</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Power calculation</span>
    <span class="k">if</span> <span class="n">two_sided</span><span class="p">:</span>
        <span class="n">power</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">nct</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ncp</span><span class="p">)</span> <span class="o">+</span> 
                <span class="n">stats</span><span class="o">.</span><span class="n">nct</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ncp</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">power</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">nct</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ncp</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">power</span>
</code></pre></div>
<p>This balanced approach shows both the mathematical foundation and its practical implementation, helping users understand both the theory and application of hypothesis testing.</p>
<p>Remember:
1. Start with clear mathematical formulation
2. Provide intuitive explanations
3. Implement solutions efficiently
4. Consider computational aspects
5. Document assumptions and limitations</p>
<h1 id="grassroots-statistics-4_statistical_inference-statistical-estimation">Statistical Estimation<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-statistical-estimation" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-4_statistical_inference-point-estimates">Point Estimates<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-point-estimates" title="Permanent link">&para;</a></h2>
<p>A point estimate is a single value that serves as our "best guess" for an unknown population parameter. The theory behind point estimation helps us understand what makes a good estimator.</p>
<h3 id="grassroots-statistics-4_statistical_inference-properties-of-estimators">Properties of Estimators<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-properties-of-estimators" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-4_statistical_inference-unbiasedness">Unbiasedness<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-unbiasedness" title="Permanent link">&para;</a></h4>
<p>An estimator θ̂ is unbiased if its expected value equals the true parameter:</p>
<p>E[θ̂] = θ</p>
<p>For example, the sample mean X̄ is an unbiased estimator of the population mean μ:</p>
<p>E[X̄] = E[∑Xᵢ/n] = ∑E[Xᵢ]/n = μ</p>
<h4 id="grassroots-statistics-4_statistical_inference-consistency">Consistency<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-consistency" title="Permanent link">&para;</a></h4>
<p>An estimator is consistent if it converges to the true parameter as sample size increases:</p>
<p>lim(n→∞) P(|θ̂ₙ - θ| &gt; ε) = 0 for any ε &gt; 0</p>
<h4 id="grassroots-statistics-4_statistical_inference-efficiency">Efficiency<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-efficiency" title="Permanent link">&para;</a></h4>
<p>Among unbiased estimators, the most efficient one has the smallest variance. The Cramér-Rao bound gives us the theoretical minimum variance:</p>
<p>Var(θ̂) ≥ 1/I(θ)</p>
<p>where I(θ) is the Fisher Information.</p>
<h3 id="grassroots-statistics-4_statistical_inference-implementation-of-basic-estimators">Implementation of Basic Estimators<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-implementation-of-basic-estimators" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_estimators</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate common point estimates with their standard errors&quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Using n-1 for unbiased estimation</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span>
        <span class="s1">&#39;se_mean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="o">/</span><span class="n">n</span><span class="p">),</span>  <span class="c1"># Standard error of mean</span>
        <span class="s1">&#39;variance&#39;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
        <span class="s1">&#39;se_variance&#39;</span><span class="p">:</span> <span class="n">variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># SE of variance</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-confidence-intervals">Confidence Intervals<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-confidence-intervals" title="Permanent link">&para;</a></h2>
<p>A confidence interval provides a range of plausible values for a parameter, along with a measure of uncertainty. The mathematical foundation comes from the sampling distribution of the estimator.</p>
<h3 id="grassroots-statistics-4_statistical_inference-for-population-mean">For Population Mean<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-for-population-mean" title="Permanent link">&para;</a></h3>
<p>Under normality assumption, the pivotal quantity:</p>
<p>(X̄ - μ)/(s/√n) ~ t(n-1)</p>
<p>leads to the confidence interval:</p>
<p>X̄ ± t(α/2,n-1) * s/√n</p>
<p>where:
- t(α/2,n-1) is the critical value from t-distribution
- s is the sample standard deviation
- n is the sample size</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">mean_confidence_interval</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate CI for mean using t-distribution&quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">sem</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">ci</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">confidence</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">se</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">ci</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-maximum-likelihood-estimation-mle" title="Permanent link">&para;</a></h2>
<p>MLE finds parameter values that maximize the probability of observing the data. For independent observations, the likelihood function is:</p>
<p>L(θ; x₁, ..., xₙ) = ∏ᵢ f(xᵢ; θ)</p>
<p>We typically maximize the log-likelihood:</p>
<p>ℓ(θ) = ∑ᵢ log f(xᵢ; θ)</p>
<h3 id="grassroots-statistics-4_statistical_inference-example-normal-distribution">Example: Normal Distribution<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-example-normal-distribution" title="Permanent link">&para;</a></h3>
<p>For normal distribution, the log-likelihood is:</p>
<p>ℓ(μ,σ²) = -n/2 * log(2πσ²) - ∑(xᵢ - μ)²/(2σ²)</p>
<p>The MLEs are:
μ̂ = X̄
σ̂² = ∑(xᵢ - X̄)²/n</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">normal_mle_with_uncertainty</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MLE for normal distribution with standard errors&quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">sigma2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>  <span class="c1"># MLE of variance</span>

    <span class="c1"># Fisher Information Matrix derivatives</span>
    <span class="n">se_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
    <span class="n">se_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="n">mu</span><span class="p">,</span> 
        <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">),</span>
        <span class="s1">&#39;se_mu&#39;</span><span class="p">:</span> <span class="n">se_mu</span><span class="p">,</span>
        <span class="s1">&#39;se_sigma&#39;</span><span class="p">:</span> <span class="n">se_sigma</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-bias-variance-tradeoff">Bias-Variance Tradeoff<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-bias-variance-tradeoff" title="Permanent link">&para;</a></h2>
<p>The expected prediction error can be decomposed into:</p>
<p>E[(y - f̂(x))²] = (Bias[f̂(x)])² + Var[f̂(x)] + σ²</p>
<p>where:
- Bias[f̂(x)] = E[f̂(x)] - f(x)
- Var[f̂(x)] = E[(f̂(x) - E[f̂(x)])²]
- σ² is irreducible error</p>
<p>This decomposition helps understand the fundamental tradeoff in model complexity:
- Simple models: High bias, low variance
- Complex models: Low bias, high variance</p>
<h3 id="grassroots-statistics-4_statistical_inference-visual-demonstration">Visual Demonstration<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-visual-demonstration" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">plot_bias_variance_tradeoff</span><span class="p">(</span><span class="n">complexity_range</span><span class="p">,</span> <span class="n">bias_values</span><span class="p">,</span> <span class="n">variance_values</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot bias-variance tradeoff across model complexities&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexity_range</span><span class="p">,</span> <span class="n">bias_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Bias²&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexity_range</span><span class="p">,</span> <span class="n">variance_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Variance&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexity_range</span><span class="p">,</span> 
            <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bias_values</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">variance_values</span><span class="p">),</span> 
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Total Error&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Model Complexity&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Bias-Variance Tradeoff&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plt</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-key-takeaways">Key Takeaways<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-key-takeaways" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Point Estimation</strong></li>
<li>Balance between different estimator properties</li>
<li>
<p>Consider both theoretical properties and practical constraints</p>
</li>
<li>
<p><strong>Interval Estimation</strong></p>
</li>
<li>Provides measure of uncertainty</li>
<li>Based on sampling distribution theory</li>
<li>
<p>Requires careful interpretation</p>
</li>
<li>
<p><strong>Maximum Likelihood</strong></p>
</li>
<li>Powerful, general-purpose method</li>
<li>Asymptotically optimal properties</li>
<li>
<p>Can be computationally intensive</p>
</li>
<li>
<p><strong>Bias-Variance</strong></p>
</li>
<li>Fundamental tradeoff in statistical learning</li>
<li>Guides model complexity selection</li>
<li>Helps understand overfitting/underfitting</li>
</ol>
<p>Remember:
- Theory guides the choice of methods
- Practical considerations often require compromises
- Understanding uncertainty is crucial
- Multiple approaches often provide better insight</p>
<h1 id="grassroots-statistics-4_statistical_inference-sampling-theory">Sampling Theory<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling-theory" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-4_statistical_inference-population-vs-sample">Population vs Sample<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-population-vs-sample" title="Permanent link">&para;</a></h2>
<p>The foundation of statistical inference lies in the relationship between populations and samples:</p>
<ul>
<li><strong>Population</strong>: The complete set of all elements we want to study</li>
<li><strong>Sample</strong>: A subset of the population used to make inferences</li>
</ul>
<h3 id="grassroots-statistics-4_statistical_inference-mathematical-notation">Mathematical Notation<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-mathematical-notation" title="Permanent link">&para;</a></h3>
<ul>
<li>Population parameters: θ, μ, σ, π</li>
<li>Sample statistics: θ̂, x̄, s, p̂</li>
</ul>
<p>The relationship between population parameters and sample statistics can be expressed through expected values:
* E[x̄] = μ (unbiased estimator of mean)
* E[s²] = σ² (unbiased estimator of variance)</p>
<h2 id="grassroots-statistics-4_statistical_inference-sampling-distributions">Sampling Distributions<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling-distributions" title="Permanent link">&para;</a></h2>
<p>The sampling distribution describes the probability distribution of a statistic across all possible samples of size n.</p>
<h3 id="grassroots-statistics-4_statistical_inference-sample-mean">Sample Mean<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sample-mean" title="Permanent link">&para;</a></h3>
<p>For a random sample X₁, X₂, ..., Xₙ:
* Sample mean: x̄ = (1/n)∑Xᵢ
* Expected value: E[x̄] = μ
* Variance: Var(x̄) = σ²/n</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Implementation for empirical sampling distribution</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sample_mean_distribution</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">):</span>
    <span class="n">means</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">))</span> 
             <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-central-limit-theorem-clt">Central Limit Theorem (CLT)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-central-limit-theorem-clt" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-mathematical-formulation">Mathematical Formulation<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-mathematical-formulation" title="Permanent link">&para;</a></h3>
<p>For independent, identically distributed random variables X₁, X₂, ..., Xₙ with mean μ and variance σ², the standardized sample mean:</p>
<p>Z = (x̄ - μ)/(σ/√n) → N(0,1) as n → ∞</p>
<p>This means that for large n:
x̄ ∼ N(μ, σ²/n)</p>
<h3 id="grassroots-statistics-4_statistical_inference-intuition">Intuition<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-intuition" title="Permanent link">&para;</a></h3>
<p>The CLT tells us that regardless of the underlying distribution:
1. The sampling distribution of the mean becomes approximately normal
2. The spread of this distribution shrinks with √n
3. The center remains at the population mean</p>
<h2 id="grassroots-statistics-4_statistical_inference-law-of-large-numbers-lln">Law of Large Numbers (LLN)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-law-of-large-numbers-lln" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-weak-law-wlln">Weak Law (WLLN)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-weak-law-wlln" title="Permanent link">&para;</a></h3>
<p>For any ε &gt; 0:
P(|x̄ₙ - μ| &gt; ε) → 0 as n → ∞</p>
<h3 id="grassroots-statistics-4_statistical_inference-strong-law-slln">Strong Law (SLLN)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-strong-law-slln" title="Permanent link">&para;</a></h3>
<p>P(limₙ→∞ x̄ₙ = μ) = 1</p>
<h3 id="grassroots-statistics-4_statistical_inference-intuition_1">Intuition<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-intuition_1" title="Permanent link">&para;</a></h3>
<ul>
<li>WLLN: The probability of a large deviation from μ becomes small</li>
<li>SLLN: The sample mean will converge to μ with probability 1</li>
</ul>
<h2 id="grassroots-statistics-4_statistical_inference-standard-error">Standard Error<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-standard-error" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-mathematical-definition">Mathematical Definition<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-mathematical-definition" title="Permanent link">&para;</a></h3>
<p>For any statistic θ̂:
SE(θ̂) = √Var(θ̂)</p>
<h3 id="grassroots-statistics-4_statistical_inference-common-forms">Common Forms<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-common-forms" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Mean</strong>: SE(x̄) = σ/√n</li>
<li><strong>Proportion</strong>: SE(p̂) = √(p(1-p)/n)</li>
<li><strong>Difference of Means</strong>: SE(x̄₁ - x̄₂) = √(σ₁²/n₁ + σ₂²/n₂)</li>
</ol>
<p>When population parameters are unknown, we use sample estimates:
s/√n, √(p̂(1-p̂)/n), etc.</p>
<h2 id="grassroots-statistics-4_statistical_inference-practical-sampling-methods">Practical Sampling Methods<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-practical-sampling-methods" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-simple-random-sampling">Simple Random Sampling<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-simple-random-sampling" title="Permanent link">&para;</a></h3>
<p>Each subset of size n has equal probability of selection:
P(selecting specific sample) = 1/₍ₙᴺ₎</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">simple_random_sample</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-statistics-4_statistical_inference-stratified-sampling">Stratified Sampling<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-stratified-sampling" title="Permanent link">&para;</a></h3>
<p>For L strata with Nₕ units in stratum h:
* Stratum weight: Wₕ = Nₕ/N
* Stratified mean: x̄ₛₜ = ∑Wₕx̄ₕ
* Variance: Var(x̄ₛₜ) = ∑Wₕ²σₕ²/nₕ</p>
<h3 id="grassroots-statistics-4_statistical_inference-sample-size-determination">Sample Size Determination<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sample-size-determination" title="Permanent link">&para;</a></h3>
<p>For desired margin of error E and confidence level α:
* For means: n = (zα/2 × σ/E)²
* For proportions: n = (zα/2)² × p(1-p)/E²</p>
<h2 id="grassroots-statistics-4_statistical_inference-relationship-to-statistical-inference">Relationship to Statistical Inference<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-relationship-to-statistical-inference" title="Permanent link">&para;</a></h2>
<p>The theoretical foundations of sampling connect directly to inference through:</p>
<ol>
<li>
<p><strong>Confidence Intervals</strong>
θ̂ ± zα/2 × SE(θ̂)</p>
</li>
<li>
<p><strong>Hypothesis Tests</strong>
Test statistic = (θ̂ - θ₀)/SE(θ̂)</p>
</li>
</ol>
<p>These applications rely on the sampling distribution theory developed above.</p>
<h3 id="grassroots-statistics-4_statistical_inference-example-one-sample-t-test">Example: One-Sample t-test<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-example-one-sample-t-test" title="Permanent link">&para;</a></h3>
<p>Under H₀: μ = μ₀
t = (x̄ - μ₀)/(s/√n) ∼ t(n-1)</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">t_test_statistic</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">null_mean</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="n">null_mean</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)))</span>
</code></pre></div>
<p>Remember:
1. The mathematical theory provides the foundation
2. Computational methods help verify and visualize
3. Understanding both perspectives enhances statistical practice
4. Real-world applications often require both theoretical and practical tools</p></section><section class="print-page" id="grassroots-statistics-4_statistical_inference-estimation"><h1 id="grassroots-statistics-4_statistical_inference-estimation-statistical-estimation">Statistical Estimation<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-statistical-estimation" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-4_statistical_inference-estimation-point-estimates">Point Estimates<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-point-estimates" title="Permanent link">&para;</a></h2>
<p>A point estimate is a single value that serves as our "best guess" for an unknown population parameter. The theory behind point estimation helps us understand what makes a good estimator.</p>
<h3 id="grassroots-statistics-4_statistical_inference-estimation-properties-of-estimators">Properties of Estimators<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-properties-of-estimators" title="Permanent link">&para;</a></h3>
<h4 id="grassroots-statistics-4_statistical_inference-estimation-unbiasedness">Unbiasedness<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-unbiasedness" title="Permanent link">&para;</a></h4>
<p>An estimator θ̂ is unbiased if its expected value equals the true parameter:</p>
<p>E[θ̂] = θ</p>
<p>For example, the sample mean X̄ is an unbiased estimator of the population mean μ:</p>
<p>E[X̄] = E[∑Xᵢ/n] = ∑E[Xᵢ]/n = μ</p>
<h4 id="grassroots-statistics-4_statistical_inference-estimation-consistency">Consistency<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-consistency" title="Permanent link">&para;</a></h4>
<p>An estimator is consistent if it converges to the true parameter as sample size increases:</p>
<p>lim(n→∞) P(|θ̂ₙ - θ| &gt; ε) = 0 for any ε &gt; 0</p>
<h4 id="grassroots-statistics-4_statistical_inference-estimation-efficiency">Efficiency<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-efficiency" title="Permanent link">&para;</a></h4>
<p>Among unbiased estimators, the most efficient one has the smallest variance. The Cramér-Rao bound gives us the theoretical minimum variance:</p>
<p>Var(θ̂) ≥ 1/I(θ)</p>
<p>where I(θ) is the Fisher Information.</p>
<h3 id="grassroots-statistics-4_statistical_inference-estimation-implementation-of-basic-estimators">Implementation of Basic Estimators<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-implementation-of-basic-estimators" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_estimators</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate common point estimates with their standard errors&quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Using n-1 for unbiased estimation</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span>
        <span class="s1">&#39;se_mean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="o">/</span><span class="n">n</span><span class="p">),</span>  <span class="c1"># Standard error of mean</span>
        <span class="s1">&#39;variance&#39;</span><span class="p">:</span> <span class="n">variance</span><span class="p">,</span>
        <span class="s1">&#39;se_variance&#39;</span><span class="p">:</span> <span class="n">variance</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># SE of variance</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-estimation-confidence-intervals">Confidence Intervals<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-confidence-intervals" title="Permanent link">&para;</a></h2>
<p>A confidence interval provides a range of plausible values for a parameter, along with a measure of uncertainty. The mathematical foundation comes from the sampling distribution of the estimator.</p>
<h3 id="grassroots-statistics-4_statistical_inference-estimation-for-population-mean">For Population Mean<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-for-population-mean" title="Permanent link">&para;</a></h3>
<p>Under normality assumption, the pivotal quantity:</p>
<p>(X̄ - μ)/(s/√n) ~ t(n-1)</p>
<p>leads to the confidence interval:</p>
<p>X̄ ± t(α/2,n-1) * s/√n</p>
<p>where:
- t(α/2,n-1) is the critical value from t-distribution
- s is the sample standard deviation
- n is the sample size</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">mean_confidence_interval</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate CI for mean using t-distribution&quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">se</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">sem</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">ci</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">interval</span><span class="p">(</span><span class="n">confidence</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">se</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">ci</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-estimation-maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-maximum-likelihood-estimation-mle" title="Permanent link">&para;</a></h2>
<p>MLE finds parameter values that maximize the probability of observing the data. For independent observations, the likelihood function is:</p>
<p>L(θ; x₁, ..., xₙ) = ∏ᵢ f(xᵢ; θ)</p>
<p>We typically maximize the log-likelihood:</p>
<p>ℓ(θ) = ∑ᵢ log f(xᵢ; θ)</p>
<h3 id="grassroots-statistics-4_statistical_inference-estimation-example-normal-distribution">Example: Normal Distribution<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-example-normal-distribution" title="Permanent link">&para;</a></h3>
<p>For normal distribution, the log-likelihood is:</p>
<p>ℓ(μ,σ²) = -n/2 * log(2πσ²) - ∑(xᵢ - μ)²/(2σ²)</p>
<p>The MLEs are:
μ̂ = X̄
σ̂² = ∑(xᵢ - X̄)²/n</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">normal_mle_with_uncertainty</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MLE for normal distribution with standard errors&quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">sigma2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>  <span class="c1"># MLE of variance</span>

    <span class="c1"># Fisher Information Matrix derivatives</span>
    <span class="n">se_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
    <span class="n">se_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="n">mu</span><span class="p">,</span> 
        <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">),</span>
        <span class="s1">&#39;se_mu&#39;</span><span class="p">:</span> <span class="n">se_mu</span><span class="p">,</span>
        <span class="s1">&#39;se_sigma&#39;</span><span class="p">:</span> <span class="n">se_sigma</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-estimation-bias-variance-tradeoff">Bias-Variance Tradeoff<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-bias-variance-tradeoff" title="Permanent link">&para;</a></h2>
<p>The expected prediction error can be decomposed into:</p>
<p>E[(y - f̂(x))²] = (Bias[f̂(x)])² + Var[f̂(x)] + σ²</p>
<p>where:
- Bias[f̂(x)] = E[f̂(x)] - f(x)
- Var[f̂(x)] = E[(f̂(x) - E[f̂(x)])²]
- σ² is irreducible error</p>
<p>This decomposition helps understand the fundamental tradeoff in model complexity:
- Simple models: High bias, low variance
- Complex models: Low bias, high variance</p>
<h3 id="grassroots-statistics-4_statistical_inference-estimation-visual-demonstration">Visual Demonstration<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-visual-demonstration" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">plot_bias_variance_tradeoff</span><span class="p">(</span><span class="n">complexity_range</span><span class="p">,</span> <span class="n">bias_values</span><span class="p">,</span> <span class="n">variance_values</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot bias-variance tradeoff across model complexities&quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexity_range</span><span class="p">,</span> <span class="n">bias_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Bias²&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexity_range</span><span class="p">,</span> <span class="n">variance_values</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Variance&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">complexity_range</span><span class="p">,</span> 
            <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bias_values</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">variance_values</span><span class="p">),</span> 
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Total Error&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Model Complexity&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Bias-Variance Tradeoff&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plt</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-estimation-key-takeaways">Key Takeaways<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-estimation-key-takeaways" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Point Estimation</strong></li>
<li>Balance between different estimator properties</li>
<li>
<p>Consider both theoretical properties and practical constraints</p>
</li>
<li>
<p><strong>Interval Estimation</strong></p>
</li>
<li>Provides measure of uncertainty</li>
<li>Based on sampling distribution theory</li>
<li>
<p>Requires careful interpretation</p>
</li>
<li>
<p><strong>Maximum Likelihood</strong></p>
</li>
<li>Powerful, general-purpose method</li>
<li>Asymptotically optimal properties</li>
<li>
<p>Can be computationally intensive</p>
</li>
<li>
<p><strong>Bias-Variance</strong></p>
</li>
<li>Fundamental tradeoff in statistical learning</li>
<li>Guides model complexity selection</li>
<li>Helps understand overfitting/underfitting</li>
</ol>
<p>Remember:
- Theory guides the choice of methods
- Practical considerations often require compromises
- Understanding uncertainty is crucial
- Multiple approaches often provide better insight</p></section><section class="print-page" id="grassroots-statistics-4_statistical_inference-hypothesis_testing"><h1 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-statistical-hypothesis-testing">Statistical Hypothesis Testing<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-statistical-hypothesis-testing" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-fundamental-concepts">Fundamental Concepts<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-fundamental-concepts" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-null-and-alternative-hypotheses">Null and Alternative Hypotheses<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-null-and-alternative-hypotheses" title="Permanent link">&para;</a></h3>
<p>The foundation of hypothesis testing lies in formulating two competing claims about a population parameter:</p>
<ol>
<li><strong>Null Hypothesis (H₀)</strong>: The default position, typically expressing "no effect" or "no difference"</li>
<li><strong>Alternative Hypothesis (H₁ or Hₐ)</strong>: The research claim we want to support with evidence</li>
</ol>
<p>For a population parameter θ, these are typically expressed in one of three ways:</p>
<p><strong>Two-sided test:</strong>
<div class="highlight"><pre><span></span><code>H₀: θ = θ₀
H₁: θ ≠ θ₀
</code></pre></div></p>
<p><strong>One-sided tests:</strong>
<div class="highlight"><pre><span></span><code>Upper-tailed:          Lower-tailed:
H₀: θ ≤ θ₀            H₀: θ ≥ θ₀
H₁: θ &gt; θ₀            H₁: θ &lt; θ₀
</code></pre></div></p>
<h3 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-test-statistics-and-sampling-distributions">Test Statistics and Sampling Distributions<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-test-statistics-and-sampling-distributions" title="Permanent link">&para;</a></h3>
<p>Most test statistics follow the general form:</p>
<div class="highlight"><pre><span></span><code>test statistic = (sample estimate - null value) / (standard error)
</code></pre></div>
<p>For example, the z-test statistic for a population mean:</p>
<div class="highlight"><pre><span></span><code>z = (x̄ - μ₀) / (σ/√n)
</code></pre></div>
<p>where:
- x̄ is the sample mean
- μ₀ is the hypothesized population mean
- σ is the population standard deviation
- n is the sample size</p>
<p>When σ is unknown and estimated by s, we use the t-statistic:</p>
<div class="highlight"><pre><span></span><code>t = (x̄ - μ₀) / (s/√n)
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-type-i-and-type-ii-errors">Type I and Type II Errors<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-type-i-and-type-ii-errors" title="Permanent link">&para;</a></h2>
<p>The decision process in hypothesis testing can lead to two types of errors:</p>
<table>
<thead>
<tr>
<th></th>
<th>H₀ True</th>
<th>H₀ False</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reject H₀</td>
<td>Type I Error (α)</td>
<td>Correct Decision</td>
</tr>
<tr>
<td>Fail to Reject H₀</td>
<td>Correct Decision</td>
<td>Type II Error (β)</td>
</tr>
</tbody>
</table>
<p>The probability relationships:
<div class="highlight"><pre><span></span><code>P(Type I Error) = α = P(Reject H₀ | H₀ true)
P(Type II Error) = β = P(Fail to Reject H₀ | H₁ true)
Power = 1 - β = P(Reject H₀ | H₁ true)
</code></pre></div></p>
<h2 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-p-values-and-statistical-power">P-values and Statistical Power<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-p-values-and-statistical-power" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-p-value">P-value<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-p-value" title="Permanent link">&para;</a></h3>
<p>The p-value is defined mathematically as:</p>
<p>For a test statistic T and observed value t*:
<div class="highlight"><pre><span></span><code>Two-sided: p = 2 * P(T ≥ |t*| | H₀)
Upper-tailed: p = P(T ≥ t* | H₀)
Lower-tailed: p = P(T ≤ t* | H₀)
</code></pre></div></p>
<p>For practical computation in Python:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_p_value</span><span class="p">(</span><span class="n">test_statistic</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="n">two_sided</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">distribution</span> <span class="o">==</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">two_sided</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">test_statistic</span><span class="p">)))</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">test_statistic</span><span class="p">)</span>
</code></pre></div></p>
<h3 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-statistical-power">Statistical Power<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-statistical-power" title="Permanent link">&para;</a></h3>
<p>Power depends on four interrelated quantities:
1. Effect size (δ)
2. Sample size (n)
3. Significance level (α)
4. Power (1-β)</p>
<p>For a two-sided z-test, the power function is:</p>
<div class="highlight"><pre><span></span><code>Power = 1 - β = Φ(zα/2 + δ√n/σ) + Φ(-zα/2 + δ√n/σ)
</code></pre></div>
<p>where:
- Φ is the standard normal CDF
- zα/2 is the critical value
- δ is the true difference from null
- σ is the population standard deviation</p>
<h2 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-multiple-testing-problem">Multiple Testing Problem<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-multiple-testing-problem" title="Permanent link">&para;</a></h2>
<p>When conducting m independent tests at significance level α, the probability of at least one Type I error (Family-Wise Error Rate, FWER) is:</p>
<div class="highlight"><pre><span></span><code>FWER = 1 - (1-α)ᵐ
</code></pre></div>
<h3 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-bonferroni-correction">Bonferroni Correction<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-bonferroni-correction" title="Permanent link">&para;</a></h3>
<p>Controls FWER by adjusting the significance level:
<div class="highlight"><pre><span></span><code>α_adjusted = α/m
</code></pre></div></p>
<h3 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-benjamini-hochberg-procedure">Benjamini-Hochberg Procedure<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-benjamini-hochberg-procedure" title="Permanent link">&para;</a></h3>
<p>Controls False Discovery Rate (FDR). For ordered p-values p₁ ≤ p₂ ≤ ... ≤ pₘ:
1. Find largest k where p_k ≤ (k/m)α
2. Reject all hypotheses H₍ᵢ₎ for i = 1,...,k</p>
<p>Implementation combining mathematical insight with computation:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">benjamini_hochberg</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements Benjamini-Hochberg procedure</span>
<span class="sd">    Returns: boolean array of rejected null hypotheses</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span>
    <span class="n">ranked</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">p_values</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">)</span>
    <span class="n">critical_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">ranked</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>
    <span class="n">sorted_p_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">p_values</span><span class="p">)</span>

    <span class="c1"># Find largest k where p_k ≤ (k/m)α</span>
    <span class="n">significant</span> <span class="o">=</span> <span class="n">p_values</span> <span class="o">&lt;=</span> <span class="n">critical_values</span>
    <span class="k">return</span> <span class="n">significant</span>
</code></pre></div></p>
<h2 id="grassroots-statistics-4_statistical_inference-hypothesis_testing-power-analysis-example">Power Analysis Example<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-hypothesis_testing-power-analysis-example" title="Permanent link">&para;</a></h2>
<p>Let's combine mathematical formulation with computation for a t-test power analysis:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">power_analysis</span><span class="p">(</span><span class="n">effect_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">two_sided</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate power for two-sample t-test</span>

<span class="sd">    Parameters:</span>
<span class="sd">    effect_size (d) = (μ₁ - μ₂)/σ</span>
<span class="sd">    n = sample size per group</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Critical value</span>
    <span class="n">df</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span>  <span class="c1"># degrees of freedom</span>
    <span class="k">if</span> <span class="n">two_sided</span><span class="p">:</span>
        <span class="n">t_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">t_crit</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>

    <span class="c1"># Non-centrality parameter</span>
    <span class="n">ncp</span> <span class="o">=</span> <span class="n">effect_size</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Power calculation</span>
    <span class="k">if</span> <span class="n">two_sided</span><span class="p">:</span>
        <span class="n">power</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">nct</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ncp</span><span class="p">)</span> <span class="o">+</span> 
                <span class="n">stats</span><span class="o">.</span><span class="n">nct</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ncp</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">power</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">nct</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">t_crit</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ncp</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">power</span>
</code></pre></div>
<p>This balanced approach shows both the mathematical foundation and its practical implementation, helping users understand both the theory and application of hypothesis testing.</p>
<p>Remember:
1. Start with clear mathematical formulation
2. Provide intuitive explanations
3. Implement solutions efficiently
4. Consider computational aspects
5. Document assumptions and limitations</p></section><section class="print-page" id="grassroots-statistics-4_statistical_inference-sampling_theory"><h1 id="grassroots-statistics-4_statistical_inference-sampling_theory-sampling-theory">Sampling Theory<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-sampling-theory" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-4_statistical_inference-sampling_theory-population-vs-sample">Population vs Sample<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-population-vs-sample" title="Permanent link">&para;</a></h2>
<p>The foundation of statistical inference lies in the relationship between populations and samples:</p>
<ul>
<li><strong>Population</strong>: The complete set of all elements we want to study</li>
<li><strong>Sample</strong>: A subset of the population used to make inferences</li>
</ul>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-mathematical-notation">Mathematical Notation<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-mathematical-notation" title="Permanent link">&para;</a></h3>
<ul>
<li>Population parameters: θ, μ, σ, π</li>
<li>Sample statistics: θ̂, x̄, s, p̂</li>
</ul>
<p>The relationship between population parameters and sample statistics can be expressed through expected values:
* E[x̄] = μ (unbiased estimator of mean)
* E[s²] = σ² (unbiased estimator of variance)</p>
<h2 id="grassroots-statistics-4_statistical_inference-sampling_theory-sampling-distributions">Sampling Distributions<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-sampling-distributions" title="Permanent link">&para;</a></h2>
<p>The sampling distribution describes the probability distribution of a statistic across all possible samples of size n.</p>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-sample-mean">Sample Mean<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-sample-mean" title="Permanent link">&para;</a></h3>
<p>For a random sample X₁, X₂, ..., Xₙ:
* Sample mean: x̄ = (1/n)∑Xᵢ
* Expected value: E[x̄] = μ
* Variance: Var(x̄) = σ²/n</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Implementation for empirical sampling distribution</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sample_mean_distribution</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">):</span>
    <span class="n">means</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">))</span> 
             <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>
</code></pre></div>
<h2 id="grassroots-statistics-4_statistical_inference-sampling_theory-central-limit-theorem-clt">Central Limit Theorem (CLT)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-central-limit-theorem-clt" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-mathematical-formulation">Mathematical Formulation<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-mathematical-formulation" title="Permanent link">&para;</a></h3>
<p>For independent, identically distributed random variables X₁, X₂, ..., Xₙ with mean μ and variance σ², the standardized sample mean:</p>
<p>Z = (x̄ - μ)/(σ/√n) → N(0,1) as n → ∞</p>
<p>This means that for large n:
x̄ ∼ N(μ, σ²/n)</p>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-intuition">Intuition<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-intuition" title="Permanent link">&para;</a></h3>
<p>The CLT tells us that regardless of the underlying distribution:
1. The sampling distribution of the mean becomes approximately normal
2. The spread of this distribution shrinks with √n
3. The center remains at the population mean</p>
<h2 id="grassroots-statistics-4_statistical_inference-sampling_theory-law-of-large-numbers-lln">Law of Large Numbers (LLN)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-law-of-large-numbers-lln" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-weak-law-wlln">Weak Law (WLLN)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-weak-law-wlln" title="Permanent link">&para;</a></h3>
<p>For any ε &gt; 0:
P(|x̄ₙ - μ| &gt; ε) → 0 as n → ∞</p>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-strong-law-slln">Strong Law (SLLN)<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-strong-law-slln" title="Permanent link">&para;</a></h3>
<p>P(limₙ→∞ x̄ₙ = μ) = 1</p>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-intuition_1">Intuition<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-intuition_1" title="Permanent link">&para;</a></h3>
<ul>
<li>WLLN: The probability of a large deviation from μ becomes small</li>
<li>SLLN: The sample mean will converge to μ with probability 1</li>
</ul>
<h2 id="grassroots-statistics-4_statistical_inference-sampling_theory-standard-error">Standard Error<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-standard-error" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-mathematical-definition">Mathematical Definition<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-mathematical-definition" title="Permanent link">&para;</a></h3>
<p>For any statistic θ̂:
SE(θ̂) = √Var(θ̂)</p>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-common-forms">Common Forms<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-common-forms" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Mean</strong>: SE(x̄) = σ/√n</li>
<li><strong>Proportion</strong>: SE(p̂) = √(p(1-p)/n)</li>
<li><strong>Difference of Means</strong>: SE(x̄₁ - x̄₂) = √(σ₁²/n₁ + σ₂²/n₂)</li>
</ol>
<p>When population parameters are unknown, we use sample estimates:
s/√n, √(p̂(1-p̂)/n), etc.</p>
<h2 id="grassroots-statistics-4_statistical_inference-sampling_theory-practical-sampling-methods">Practical Sampling Methods<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-practical-sampling-methods" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-simple-random-sampling">Simple Random Sampling<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-simple-random-sampling" title="Permanent link">&para;</a></h3>
<p>Each subset of size n has equal probability of selection:
P(selecting specific sample) = 1/₍ₙᴺ₎</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">simple_random_sample</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-stratified-sampling">Stratified Sampling<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-stratified-sampling" title="Permanent link">&para;</a></h3>
<p>For L strata with Nₕ units in stratum h:
* Stratum weight: Wₕ = Nₕ/N
* Stratified mean: x̄ₛₜ = ∑Wₕx̄ₕ
* Variance: Var(x̄ₛₜ) = ∑Wₕ²σₕ²/nₕ</p>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-sample-size-determination">Sample Size Determination<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-sample-size-determination" title="Permanent link">&para;</a></h3>
<p>For desired margin of error E and confidence level α:
* For means: n = (zα/2 × σ/E)²
* For proportions: n = (zα/2)² × p(1-p)/E²</p>
<h2 id="grassroots-statistics-4_statistical_inference-sampling_theory-relationship-to-statistical-inference">Relationship to Statistical Inference<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-relationship-to-statistical-inference" title="Permanent link">&para;</a></h2>
<p>The theoretical foundations of sampling connect directly to inference through:</p>
<ol>
<li>
<p><strong>Confidence Intervals</strong>
θ̂ ± zα/2 × SE(θ̂)</p>
</li>
<li>
<p><strong>Hypothesis Tests</strong>
Test statistic = (θ̂ - θ₀)/SE(θ̂)</p>
</li>
</ol>
<p>These applications rely on the sampling distribution theory developed above.</p>
<h3 id="grassroots-statistics-4_statistical_inference-sampling_theory-example-one-sample-t-test">Example: One-Sample t-test<a class="headerlink" href="#grassroots-statistics-4_statistical_inference-sampling_theory-example-one-sample-t-test" title="Permanent link">&para;</a></h3>
<p>Under H₀: μ = μ₀
t = (x̄ - μ₀)/(s/√n) ∼ t(n-1)</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">t_test_statistic</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">null_mean</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="n">null_mean</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)))</span>
</code></pre></div>
<p>Remember:
1. The mathematical theory provides the foundation
2. Computational methods help verify and visualize
3. Understanding both perspectives enhances statistical practice
4. Real-world applications often require both theoretical and practical tools</p></section><h1 class='nav-section-title-end'>Ended: 4 statistical inference</h1>
                        <h3 class='nav-section-title' id='section-5-regression-analysis'>
                            5 regression analysis <a class='headerlink' href='#section-5-regression-analysis' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-statistics-5_regression_analysis"><h1 id="grassroots-statistics-5_regression_analysis-regression-analysis">Regression Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-regression-analysis" title="Permanent link">&para;</a></h1>
<h1 id="grassroots-statistics-5_regression_analysis-simple-linear-regression">Simple Linear Regression<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple-linear-regression" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-5_regression_analysis-model-fundamentals">Model Fundamentals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-model-fundamentals" title="Permanent link">&para;</a></h2>
<p>Linear regression models the relationship between a dependent variable y and a single independent variable x:
<div class="highlight"><pre><span></span><code>y = β₀ + β₁x + ε
</code></pre></div>
where:
- β₀ is the intercept
- β₁ is the slope
- ε is the error term</p>
<h2 id="grassroots-statistics-5_regression_analysis-assumptions">Assumptions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-assumptions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-1-linearity">1. Linearity<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-1-linearity" title="Permanent link">&para;</a></h3>
<ul>
<li>Relationship between x and y is linear</li>
<li>Can be checked through scatter plots</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_linearity</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot data and check linearity assumption</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Independent Variable&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Dependent Variable&#39;</span><span class="p">)</span>

    <span class="c1"># Add lowess smoother for comparison</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.nonparametric.smoothers_lowess</span><span class="w"> </span><span class="kn">import</span> <span class="n">lowess</span>
    <span class="n">smooth</span> <span class="o">=</span> <span class="n">lowess</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">frac</span><span class="o">=</span><span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smooth</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">smooth</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;LOWESS&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-2-independence">2. Independence<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-2-independence" title="Permanent link">&para;</a></h3>
<ul>
<li>Observations are independent of each other</li>
<li>No autocorrelation in residuals</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_independence</span><span class="p">(</span><span class="n">residuals</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check independence using Durbin-Watson test</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.stats.stattools</span><span class="w"> </span><span class="kn">import</span> <span class="n">durbin_watson</span>
    <span class="n">dw_stat</span> <span class="o">=</span> <span class="n">durbin_watson</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;dw_statistic&#39;</span><span class="p">:</span> <span class="n">dw_stat</span><span class="p">,</span>
        <span class="s1">&#39;is_independent&#39;</span><span class="p">:</span> <span class="mf">1.5</span> <span class="o">&lt;</span> <span class="n">dw_stat</span> <span class="o">&lt;</span> <span class="mf">2.5</span>
    <span class="p">}</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-3-homoscedasticity">3. Homoscedasticity<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-3-homoscedasticity" title="Permanent link">&para;</a></h3>
<ul>
<li>Constant variance of residuals</li>
<li>Can be checked through residual plots</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_homoscedasticity</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">residuals</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot residuals vs fitted values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fitted_values</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fitted_values</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted Values&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-4-normality-of-residuals">4. Normality of Residuals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-4-normality-of-residuals" title="Permanent link">&para;</a></h3>
<ul>
<li>Residuals should be normally distributed</li>
<li>Can be checked through QQ plots and tests</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_normality</span><span class="p">(</span><span class="n">residuals</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check normality of residuals</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

    <span class="c1"># QQ plot</span>
    <span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">)</span>

    <span class="c1"># Shapiro-Wilk test</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">shapiro</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;p_value&#39;</span><span class="p">:</span> <span class="n">p_value</span><span class="p">,</span>
        <span class="s1">&#39;is_normal&#39;</span><span class="p">:</span> <span class="n">p_value</span> <span class="o">&gt;</span> <span class="mf">0.05</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-least-squares-estimation">Least Squares Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-least-squares-estimation" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-ordinary-least-squares-ols">Ordinary Least Squares (OLS)<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-ordinary-least-squares-ols" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fit_ols</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fit simple linear regression using OLS</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;intercept&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span>
        <span class="s1">&#39;slope&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">model</span>
    <span class="p">}</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-manual-implementation">Manual Implementation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-manual-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">manual_ols</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Manual implementation of OLS</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Calculate slope</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_mean</span><span class="p">))</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="c1"># Calculate intercept</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">y_mean</span> <span class="o">-</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">X_mean</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;intercept&#39;</span><span class="p">:</span> <span class="n">intercept</span><span class="p">,</span>
        <span class="s1">&#39;slope&#39;</span><span class="p">:</span> <span class="n">slope</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-r-squared-and-adjusted-r-squared">R-squared and Adjusted R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-r-squared-and-adjusted-r-squared" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-r-squared">R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-r-squared" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_r_squared</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate R-squared</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ss_total</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_true</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ss_residual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">r_squared</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">ss_residual</span> <span class="o">/</span> <span class="n">ss_total</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r_squared</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-adjusted-r-squared">Adjusted R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-adjusted-r-squared" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_adjusted_r_squared</span><span class="p">(</span><span class="n">r_squared</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate adjusted R-squared</span>
<span class="sd">    n: number of observations</span>
<span class="sd">    p: number of predictors (1 for simple linear regression)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adjusted_r_squared</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r_squared</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adjusted_r_squared</span>
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-residual-analysis">Residual Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-residual-analysis" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-comprehensive-residual-analysis">Comprehensive Residual Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-comprehensive-residual-analysis" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ResidualAnalyzer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residuals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_residuals</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_residuals</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate residuals&quot;&quot;&quot;</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">standardized_residuals</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate standardized residuals&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">zscore</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_all_checks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run all residual diagnostics&quot;&quot;&quot;</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;normality&#39;</span><span class="p">:</span> <span class="n">check_normality</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals</span><span class="p">),</span>
            <span class="s1">&#39;independence&#39;</span><span class="p">:</span> <span class="n">check_independence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals</span><span class="p">),</span>
            <span class="s1">&#39;homoscedasticity&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_homoscedasticity</span><span class="p">(),</span>
            <span class="s1">&#39;outliers&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_outliers</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_outliers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check for outliers using standardized residuals&quot;&quot;&quot;</span>
        <span class="n">std_resid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standardized_residuals</span><span class="p">()</span>
        <span class="n">outliers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">std_resid</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">threshold</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;n_outliers&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">outliers</span><span class="p">),</span>
            <span class="s1">&#39;outlier_indices&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">outliers</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_homoscedasticity</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Breusch-Pagan test for homoscedasticity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.stats.diagnostic</span><span class="w"> </span><span class="kn">import</span> <span class="n">het_breuschpagan</span>

        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">het_breuschpagan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;p_value&#39;</span><span class="p">:</span> <span class="n">p_value</span><span class="p">,</span>
            <span class="s1">&#39;is_homoscedastic&#39;</span><span class="p">:</span> <span class="n">p_value</span> <span class="o">&gt;</span> <span class="mf">0.05</span>
        <span class="p">}</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-complete-regression-analysis">Complete Regression Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-complete-regression-analysis" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">complete_regression_analysis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform complete regression analysis</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Fit model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">fit_ols</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Get predictions</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Calculate metrics</span>
    <span class="n">r_squared</span> <span class="o">=</span> <span class="n">calculate_r_squared</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">adj_r_squared</span> <span class="o">=</span> <span class="n">calculate_adjusted_r_squared</span><span class="p">(</span><span class="n">r_squared</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Analyze residuals</span>
    <span class="n">analyzer</span> <span class="o">=</span> <span class="n">ResidualAnalyzer</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">residual_analysis</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">run_all_checks</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
        <span class="s1">&#39;r_squared&#39;</span><span class="p">:</span> <span class="n">r_squared</span><span class="p">,</span>
        <span class="s1">&#39;adj_r_squared&#39;</span><span class="p">:</span> <span class="n">adj_r_squared</span><span class="p">,</span>
        <span class="s1">&#39;residual_analysis&#39;</span><span class="p">:</span> <span class="n">residual_analysis</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-best-practices">Best Practices<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-model-building-process">Model Building Process<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-model-building-process" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Data Preparation</strong></p>
<ul>
<li>Check for missing values</li>
<li>Handle outliers</li>
<li>Scale if necessary</li>
</ul>
</li>
<li>
<p><strong>Model Fitting</strong></p>
<ul>
<li>Fit model using OLS</li>
<li>Calculate confidence intervals</li>
<li>Assess significance</li>
</ul>
</li>
<li>
<p><strong>Model Validation</strong></p>
<ul>
<li>Check assumptions</li>
<li>Analyze residuals</li>
<li>Assess fit metrics</li>
</ul>
</li>
<li>
<p><strong>Reporting</strong></p>
<ul>
<li>Coefficient estimates</li>
<li>Standard errors</li>
<li>R-squared values</li>
<li>Assumption validations</li>
</ul>
</li>
</ol>
<p>Remember:
1. Always check assumptions
2. Look for influential points
3. Consider transformations if needed
4. Report comprehensive results
5. Interpret findings in context</p>
<h1 id="grassroots-statistics-5_regression_analysis-simple-linear-regression-mathematical-foundations">Simple Linear Regression: Mathematical Foundations<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple-linear-regression-mathematical-foundations" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-5_regression_analysis-model-specification">Model Specification<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-model-specification" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-basic-form">Basic Form<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-basic-form" title="Permanent link">&para;</a></h3>
<p>The simple linear regression model is expressed as:
<div class="highlight"><pre><span></span><code>Y = β₀ + β₁X + ε
</code></pre></div>
where:
- Y is the dependent variable
- X is the independent variable
- β₀ is the y-intercept (value of Y when X = 0)
- β₁ is the slope (change in Y for one unit change in X)
- ε is the error term (random disturbance)</p>
<h2 id="grassroots-statistics-5_regression_analysis-model-assumptions">Model Assumptions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-model-assumptions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-1-linearity_1">1. Linearity<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-1-linearity_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mathematical Form</strong>: E[Y|X] = β₀ + β₁X</li>
<li><strong>Interpretation</strong>: The conditional expectation of Y given X is a linear function</li>
<li><strong>Violation Impact</strong>: Biased estimates, poor predictions</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-2-independence_1">2. Independence<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-2-independence_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mathematical Form</strong>: cov(εᵢ, εⱼ) = 0 for all i ≠ j</li>
<li><strong>Interpretation</strong>: No relationship between error terms</li>
<li><strong>Detection</strong>: Through autocorrelation function: ρₖ = cov(εₜ, εₜ₊ₖ)/var(ε)</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-3-homoscedasticity_1">3. Homoscedasticity<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-3-homoscedasticity_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mathematical Form</strong>: var(ε|X) = σ²</li>
<li><strong>Interpretation</strong>: Constant variance of errors across all X values</li>
<li><strong>Violation Impact</strong>: Inefficient estimates, invalid standard errors</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-4-normality">4. Normality<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-4-normality" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mathematical Form</strong>: ε ~ N(0, σ²)</li>
<li><strong>Interpretation</strong>: Errors follow normal distribution with mean 0 and constant variance</li>
<li><strong>Importance</strong>: Required for valid inference (t-tests, F-tests)</li>
</ul>
<h2 id="grassroots-statistics-5_regression_analysis-least-squares-estimation_1">Least Squares Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-least-squares-estimation_1" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-optimization-problem">Optimization Problem<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-optimization-problem" title="Permanent link">&para;</a></h3>
<p>Find β₀ and β₁ that minimize the sum of squared residuals:
<div class="highlight"><pre><span></span><code>min Σ(Yᵢ - β₀ - β₁Xᵢ)²
</code></pre></div></p>
<h3 id="grassroots-statistics-5_regression_analysis-solutions">Solutions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-solutions" title="Permanent link">&para;</a></h3>
<p>The optimal estimates are given by:
<div class="highlight"><pre><span></span><code>β̂₁ = Σ((Xᵢ - X̄)(Yᵢ - Ȳ)) / Σ(Xᵢ - X̄)²
β̂₀ = Ȳ - β̂₁X̄
</code></pre></div>
where X̄ and Ȳ are sample means</p>
<h3 id="grassroots-statistics-5_regression_analysis-properties-of-ols-estimators">Properties of OLS Estimators<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-properties-of-ols-estimators" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Unbiasedness</strong>: E[β̂] = β</li>
<li><strong>Consistency</strong>: β̂ → β as n → ∞</li>
<li><strong>Efficiency</strong>: Minimum variance among linear unbiased estimators (BLUE)</li>
<li><strong>Sampling Distributions</strong>:
   <div class="highlight"><pre><span></span><code>β̂₁ ~ N(β₁, σ²/Σ(Xᵢ - X̄)²)
β̂₀ ~ N(β₀, σ²(1/n + X̄²/Σ(Xᵢ - X̄)²))
</code></pre></div></li>
</ol>
<h2 id="grassroots-statistics-5_regression_analysis-measures-of-fit">Measures of Fit<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-measures-of-fit" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-r-squared_1">R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-r-squared_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Formula</strong>: R² = 1 - SSE/SST
  where:</li>
<li>SSE = Σ(Yᵢ - Ŷᵢ)² (Sum of Squared Errors)</li>
<li>
<p>SST = Σ(Yᵢ - Ȳ)² (Total Sum of Squares)</p>
</li>
<li>
<p><strong>Interpretation</strong>: Proportion of variance in Y explained by X</p>
</li>
<li><strong>Range</strong>: [0,1], with 1 indicating perfect fit</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-adjusted-r-squared_1">Adjusted R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-adjusted-r-squared_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Formula</strong>: R̄² = 1 - (1-R²)(n-1)/(n-p-1)
  where:</li>
<li>n = sample size</li>
<li>
<p>p = number of predictors (1 for simple regression)</p>
</li>
<li>
<p><strong>Interpretation</strong>: R² adjusted for model complexity</p>
</li>
<li><strong>Advantage</strong>: Penalizes unnecessary predictors</li>
</ul>
<h2 id="grassroots-statistics-5_regression_analysis-residual-analysis_1">Residual Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-residual-analysis_1" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-residual-definition">Residual Definition<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-residual-definition" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>eᵢ = Yᵢ - Ŷᵢ
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-standardized-residuals">Standardized Residuals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-standardized-residuals" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>rᵢ = eᵢ/(s√(1-hᵢᵢ))
</code></pre></div>
where:
- s = standard error of regression
- hᵢᵢ = leverage of observation i</p>
<h3 id="grassroots-statistics-5_regression_analysis-key-properties">Key Properties<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-key-properties" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Sum</strong>: Σeᵢ = 0</li>
<li><strong>Correlation</strong>: Σ(Xᵢeᵢ) = 0</li>
<li><strong>Distribution</strong>: Under assumptions, eᵢ/σ ~ N(0,1-hᵢᵢ)</li>
</ol>
<h2 id="grassroots-statistics-5_regression_analysis-statistical-inference">Statistical Inference<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-statistical-inference" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-standard-errors">Standard Errors<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-standard-errors" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>SE(β̂₁) = σ/√(Σ(Xᵢ - X̄)²)
SE(β̂₀) = σ√(1/n + X̄²/Σ(Xᵢ - X̄)²)
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-hypothesis-testing">Hypothesis Testing<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-hypothesis-testing" title="Permanent link">&para;</a></h3>
<p>For slope:
<div class="highlight"><pre><span></span><code>H₀: β₁ = 0
H₁: β₁ ≠ 0
t = β̂₁/SE(β̂₁) ~ t(n-2)
</code></pre></div></p>
<h3 id="grassroots-statistics-5_regression_analysis-confidence-intervals">Confidence Intervals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-confidence-intervals" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>β̂₁ ± t(α/2,n-2)SE(β̂₁)
β̂₀ ± t(α/2,n-2)SE(β̂₀)
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-prediction">Prediction<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-prediction" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-point-prediction">Point Prediction<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-point-prediction" title="Permanent link">&para;</a></h3>
<p>For new observation X₀:
<div class="highlight"><pre><span></span><code>Ŷ₀ = β̂₀ + β̂₁X₀
</code></pre></div></p>
<h3 id="grassroots-statistics-5_regression_analysis-prediction-interval">Prediction Interval<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-prediction-interval" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Ŷ₀ ± t(α/2,n-2)s√(1 + 1/n + (X₀-X̄)²/Σ(Xᵢ-X̄)²)
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-confidence-interval-for-mean-response">Confidence Interval for Mean Response<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-confidence-interval-for-mean-response" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Ŷ₀ ± t(α/2,n-2)s√(1/n + (X₀-X̄)²/Σ(Xᵢ-X̄)²)
</code></pre></div>
<p>Remember:
1. Assumptions are crucial for valid inference
2. R² alone is insufficient for model assessment
3. Residual analysis provides insight into model adequacy
4. Prediction intervals are wider than confidence intervals
5. Model simplicity aids interpretation</p>
<h1 id="grassroots-statistics-5_regression_analysis-generalized-linear-models">Generalized Linear Models<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized-linear-models" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-5_regression_analysis-general-framework">General Framework<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-general-framework" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-components-of-glms">Components of GLMs<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-components-of-glms" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Random Component</strong>: Response distribution from exponential family</li>
<li><strong>Systematic Component</strong>: Linear predictor η = Xβ</li>
<li><strong>Link Function</strong>: g(μ) = η, connecting mean to linear predictor</li>
</ol>
<h3 id="grassroots-statistics-5_regression_analysis-exponential-family-form">Exponential Family Form<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-exponential-family-form" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>f(y;θ,ϕ) = exp{[yθ - b(θ)]/a(ϕ) + c(y,ϕ)}
</code></pre></div>
where:
- θ is the natural parameter
- ϕ is the dispersion parameter
- b(θ) is the cumulant function
- a(ϕ) is typically ϕ/w (w is a known weight)</p>
<h2 id="grassroots-statistics-5_regression_analysis-logistic-regression">Logistic Regression<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-logistic-regression" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-model-specification_1">Model Specification<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-model-specification_1" title="Permanent link">&para;</a></h3>
<p>For binary response Y ∈ {0,1}:
<div class="highlight"><pre><span></span><code>logit(π) = log(π/(1-π)) = β₀ + β₁x₁ + ... + βₖxₖ
</code></pre></div>
where π = P(Y=1|X)</p>
<h3 id="grassroots-statistics-5_regression_analysis-properties">Properties<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-properties" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Response Distribution</strong>: Bernoulli/Binomial</li>
<li><strong>Link Function</strong>: logit(π) = log(π/(1-π))</li>
<li><strong>Mean Function</strong>: E(Y|X) = π = exp(Xβ)/(1 + exp(Xβ))</li>
<li><strong>Variance Function</strong>: var(Y|X) = π(1-π)</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-interpretation">Interpretation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-interpretation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Odds</strong>: π/(1-π)</li>
<li><strong>Odds Ratio</strong>: exp(βᵢ) for unit change in xᵢ</li>
<li><strong>Probability</strong>: π = exp(Xβ)/(1 + exp(Xβ))</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-maximum-likelihood-estimation">Maximum Likelihood Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-maximum-likelihood-estimation" title="Permanent link">&para;</a></h3>
<p>Log-likelihood:
<div class="highlight"><pre><span></span><code>l(β) = Σ[yᵢlog(πᵢ) + (1-yᵢ)log(1-πᵢ)]
</code></pre></div>
Solved iteratively using Newton-Raphson or Fisher scoring</p>
<h2 id="grassroots-statistics-5_regression_analysis-poisson-regression">Poisson Regression<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-poisson-regression" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-model-specification_2">Model Specification<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-model-specification_2" title="Permanent link">&para;</a></h3>
<p>For count response Y:
<div class="highlight"><pre><span></span><code>log(μ) = β₀ + β₁x₁ + ... + βₖxₖ
</code></pre></div>
where μ = E(Y|X)</p>
<h3 id="grassroots-statistics-5_regression_analysis-properties_1">Properties<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-properties_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Response Distribution</strong>: Poisson</li>
<li><strong>Link Function</strong>: log(μ)</li>
<li><strong>Mean Function</strong>: E(Y|X) = μ = exp(Xβ)</li>
<li><strong>Variance Function</strong>: var(Y|X) = μ</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-interpretation_1">Interpretation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-interpretation_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Rate Ratio</strong>: exp(βᵢ) for unit change in xᵢ</li>
<li><strong>Expected Count</strong>: exp(Xβ)</li>
<li><strong>Percent Change</strong>: 100(exp(βᵢ)-1)% for unit change in xᵢ</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-maximum-likelihood-estimation_1">Maximum Likelihood Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-maximum-likelihood-estimation_1" title="Permanent link">&para;</a></h3>
<p>Log-likelihood:
<div class="highlight"><pre><span></span><code>l(β) = Σ[yᵢlog(μᵢ) - μᵢ - log(yᵢ!)]
</code></pre></div></p>
<h2 id="grassroots-statistics-5_regression_analysis-link-functions">Link Functions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-link-functions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-common-link-functions">Common Link Functions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-common-link-functions" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Identity</strong>: g(μ) = μ</li>
<li>Used in linear regression</li>
<li>
<p>Range: (-∞, ∞)</p>
</li>
<li>
<p><strong>Logit</strong>: g(μ) = log(μ/(1-μ))</p>
</li>
<li>Used in logistic regression</li>
<li>
<p>Range: [0,1]</p>
</li>
<li>
<p><strong>Log</strong>: g(μ) = log(μ)</p>
</li>
<li>Used in Poisson regression</li>
<li>
<p>Range: (0, ∞)</p>
</li>
<li>
<p><strong>Probit</strong>: g(μ) = Φ⁻¹(μ)</p>
</li>
<li>Alternative for binary data</li>
<li>Range: [0,1]</li>
</ol>
<h3 id="grassroots-statistics-5_regression_analysis-canonical-links">Canonical Links<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-canonical-links" title="Permanent link">&para;</a></h3>
<p>For exponential family distributions:
* <strong>Binomial</strong>: logit
* <strong>Poisson</strong>: log
* <strong>Normal</strong>: identity
* <strong>Gamma</strong>: inverse</p>
<h2 id="grassroots-statistics-5_regression_analysis-model-assessment">Model Assessment<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-model-assessment" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-deviance">Deviance<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-deviance" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>D = 2[l(y;y) - l(β̂;y)]
</code></pre></div>
where l(y;y) is log-likelihood of saturated model</p>
<h3 id="grassroots-statistics-5_regression_analysis-residuals">Residuals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-residuals" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Pearson Residuals</strong>:
<div class="highlight"><pre><span></span><code>rₚ = (y - μ̂)/√(V(μ̂))
</code></pre></div></p>
</li>
<li>
<p><strong>Deviance Residuals</strong>:
<div class="highlight"><pre><span></span><code>rᵈ = sign(y - μ̂)√(d)
</code></pre></div>
where d is contribution to deviance</p>
</li>
<li>
<p><strong>Working Residuals</strong>:
<div class="highlight"><pre><span></span><code>rʷ = (y - μ̂)g&#39;(μ̂)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-5_regression_analysis-goodness-of-fit">Goodness of Fit<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-goodness-of-fit" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Deviance Test</strong>:</li>
<li>Compare deviance to χ² distribution</li>
<li>
<p>Degrees of freedom = n - p</p>
</li>
<li>
<p><strong>Pearson χ² Test</strong>:
<div class="highlight"><pre><span></span><code>X² = Σ(y - μ̂)²/V(μ̂)
</code></pre></div></p>
</li>
<li>
<p><strong>AIC</strong>:
<div class="highlight"><pre><span></span><code>AIC = -2l(β̂) + 2p
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-5_regression_analysis-inference">Inference<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-inference" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-parameter-estimation">Parameter Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-parameter-estimation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Standard Errors</strong>: √(diagonal of (X'WX)⁻¹)</li>
<li><strong>Confidence Intervals</strong>: β̂ᵢ ± z₁₋α/₂SE(β̂ᵢ)</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-hypothesis-testing_1">Hypothesis Testing<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-hypothesis-testing_1" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Wald Test</strong>:
<div class="highlight"><pre><span></span><code>W = β̂/SE(β̂) ~ N(0,1)
</code></pre></div></p>
</li>
<li>
<p><strong>Likelihood Ratio Test</strong>:
<div class="highlight"><pre><span></span><code>LR = 2[l(β̂₁) - l(β̂₀)] ~ χ²(df)
</code></pre></div></p>
</li>
</ul>
<h2 id="grassroots-statistics-5_regression_analysis-model-selection">Model Selection<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-model-selection" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-criteria">Criteria<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-criteria" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>AIC</strong>: -2l(β̂) + 2p</li>
<li><strong>BIC</strong>: -2l(β̂) + p log(n)</li>
<li><strong>Deviance</strong>: Measure of model fit</li>
</ol>
<h3 id="grassroots-statistics-5_regression_analysis-stepwise-procedures">Stepwise Procedures<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-stepwise-procedures" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Forward Selection</strong>: Add terms sequentially</li>
<li><strong>Backward Elimination</strong>: Remove terms sequentially</li>
<li><strong>Stepwise</strong>: Combination of forward and backward</li>
</ol>
<p>Remember:
1. Choice of link function affects interpretation
2. Check for overdispersion in count data
3. Assess model fit through residuals
4. Consider theoretical justification for model choice
5. Validate assumptions appropriate to chosen model</p></section><section class="print-page" id="grassroots-statistics-5_regression_analysis-generalized_linear_models"><h1 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-generalized-linear-models">Generalized Linear Models<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-generalized-linear-models" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-general-framework">General Framework<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-general-framework" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-components-of-glms">Components of GLMs<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-components-of-glms" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Random Component</strong>: Response distribution from exponential family</li>
<li><strong>Systematic Component</strong>: Linear predictor η = Xβ</li>
<li><strong>Link Function</strong>: g(μ) = η, connecting mean to linear predictor</li>
</ol>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-exponential-family-form">Exponential Family Form<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-exponential-family-form" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>f(y;θ,ϕ) = exp{[yθ - b(θ)]/a(ϕ) + c(y,ϕ)}
</code></pre></div>
where:
- θ is the natural parameter
- ϕ is the dispersion parameter
- b(θ) is the cumulant function
- a(ϕ) is typically ϕ/w (w is a known weight)</p>
<h2 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-logistic-regression">Logistic Regression<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-logistic-regression" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-model-specification">Model Specification<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-model-specification" title="Permanent link">&para;</a></h3>
<p>For binary response Y ∈ {0,1}:
<div class="highlight"><pre><span></span><code>logit(π) = log(π/(1-π)) = β₀ + β₁x₁ + ... + βₖxₖ
</code></pre></div>
where π = P(Y=1|X)</p>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-properties">Properties<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-properties" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Response Distribution</strong>: Bernoulli/Binomial</li>
<li><strong>Link Function</strong>: logit(π) = log(π/(1-π))</li>
<li><strong>Mean Function</strong>: E(Y|X) = π = exp(Xβ)/(1 + exp(Xβ))</li>
<li><strong>Variance Function</strong>: var(Y|X) = π(1-π)</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-interpretation">Interpretation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-interpretation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Odds</strong>: π/(1-π)</li>
<li><strong>Odds Ratio</strong>: exp(βᵢ) for unit change in xᵢ</li>
<li><strong>Probability</strong>: π = exp(Xβ)/(1 + exp(Xβ))</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-maximum-likelihood-estimation">Maximum Likelihood Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-maximum-likelihood-estimation" title="Permanent link">&para;</a></h3>
<p>Log-likelihood:
<div class="highlight"><pre><span></span><code>l(β) = Σ[yᵢlog(πᵢ) + (1-yᵢ)log(1-πᵢ)]
</code></pre></div>
Solved iteratively using Newton-Raphson or Fisher scoring</p>
<h2 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-poisson-regression">Poisson Regression<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-poisson-regression" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-model-specification_1">Model Specification<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-model-specification_1" title="Permanent link">&para;</a></h3>
<p>For count response Y:
<div class="highlight"><pre><span></span><code>log(μ) = β₀ + β₁x₁ + ... + βₖxₖ
</code></pre></div>
where μ = E(Y|X)</p>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-properties_1">Properties<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-properties_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Response Distribution</strong>: Poisson</li>
<li><strong>Link Function</strong>: log(μ)</li>
<li><strong>Mean Function</strong>: E(Y|X) = μ = exp(Xβ)</li>
<li><strong>Variance Function</strong>: var(Y|X) = μ</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-interpretation_1">Interpretation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-interpretation_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Rate Ratio</strong>: exp(βᵢ) for unit change in xᵢ</li>
<li><strong>Expected Count</strong>: exp(Xβ)</li>
<li><strong>Percent Change</strong>: 100(exp(βᵢ)-1)% for unit change in xᵢ</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-maximum-likelihood-estimation_1">Maximum Likelihood Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-maximum-likelihood-estimation_1" title="Permanent link">&para;</a></h3>
<p>Log-likelihood:
<div class="highlight"><pre><span></span><code>l(β) = Σ[yᵢlog(μᵢ) - μᵢ - log(yᵢ!)]
</code></pre></div></p>
<h2 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-link-functions">Link Functions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-link-functions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-common-link-functions">Common Link Functions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-common-link-functions" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Identity</strong>: g(μ) = μ</li>
<li>Used in linear regression</li>
<li>
<p>Range: (-∞, ∞)</p>
</li>
<li>
<p><strong>Logit</strong>: g(μ) = log(μ/(1-μ))</p>
</li>
<li>Used in logistic regression</li>
<li>
<p>Range: [0,1]</p>
</li>
<li>
<p><strong>Log</strong>: g(μ) = log(μ)</p>
</li>
<li>Used in Poisson regression</li>
<li>
<p>Range: (0, ∞)</p>
</li>
<li>
<p><strong>Probit</strong>: g(μ) = Φ⁻¹(μ)</p>
</li>
<li>Alternative for binary data</li>
<li>Range: [0,1]</li>
</ol>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-canonical-links">Canonical Links<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-canonical-links" title="Permanent link">&para;</a></h3>
<p>For exponential family distributions:
* <strong>Binomial</strong>: logit
* <strong>Poisson</strong>: log
* <strong>Normal</strong>: identity
* <strong>Gamma</strong>: inverse</p>
<h2 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-model-assessment">Model Assessment<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-model-assessment" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-deviance">Deviance<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-deviance" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>D = 2[l(y;y) - l(β̂;y)]
</code></pre></div>
where l(y;y) is log-likelihood of saturated model</p>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-residuals">Residuals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-residuals" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Pearson Residuals</strong>:
<div class="highlight"><pre><span></span><code>rₚ = (y - μ̂)/√(V(μ̂))
</code></pre></div></p>
</li>
<li>
<p><strong>Deviance Residuals</strong>:
<div class="highlight"><pre><span></span><code>rᵈ = sign(y - μ̂)√(d)
</code></pre></div>
where d is contribution to deviance</p>
</li>
<li>
<p><strong>Working Residuals</strong>:
<div class="highlight"><pre><span></span><code>rʷ = (y - μ̂)g&#39;(μ̂)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-goodness-of-fit">Goodness of Fit<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-goodness-of-fit" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Deviance Test</strong>:</li>
<li>Compare deviance to χ² distribution</li>
<li>
<p>Degrees of freedom = n - p</p>
</li>
<li>
<p><strong>Pearson χ² Test</strong>:
<div class="highlight"><pre><span></span><code>X² = Σ(y - μ̂)²/V(μ̂)
</code></pre></div></p>
</li>
<li>
<p><strong>AIC</strong>:
<div class="highlight"><pre><span></span><code>AIC = -2l(β̂) + 2p
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-inference">Inference<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-inference" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-parameter-estimation">Parameter Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-parameter-estimation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Standard Errors</strong>: √(diagonal of (X'WX)⁻¹)</li>
<li><strong>Confidence Intervals</strong>: β̂ᵢ ± z₁₋α/₂SE(β̂ᵢ)</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-hypothesis-testing">Hypothesis Testing<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-hypothesis-testing" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Wald Test</strong>:
<div class="highlight"><pre><span></span><code>W = β̂/SE(β̂) ~ N(0,1)
</code></pre></div></p>
</li>
<li>
<p><strong>Likelihood Ratio Test</strong>:
<div class="highlight"><pre><span></span><code>LR = 2[l(β̂₁) - l(β̂₀)] ~ χ²(df)
</code></pre></div></p>
</li>
</ul>
<h2 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-model-selection">Model Selection<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-model-selection" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-criteria">Criteria<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-criteria" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>AIC</strong>: -2l(β̂) + 2p</li>
<li><strong>BIC</strong>: -2l(β̂) + p log(n)</li>
<li><strong>Deviance</strong>: Measure of model fit</li>
</ol>
<h3 id="grassroots-statistics-5_regression_analysis-generalized_linear_models-stepwise-procedures">Stepwise Procedures<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-generalized_linear_models-stepwise-procedures" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Forward Selection</strong>: Add terms sequentially</li>
<li><strong>Backward Elimination</strong>: Remove terms sequentially</li>
<li><strong>Stepwise</strong>: Combination of forward and backward</li>
</ol>
<p>Remember:
1. Choice of link function affects interpretation
2. Check for overdispersion in count data
3. Assess model fit through residuals
4. Consider theoretical justification for model choice
5. Validate assumptions appropriate to chosen model</p></section><section class="print-page" id="grassroots-statistics-5_regression_analysis-multiple_linear_regression"><h1 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-simple-linear-regression-mathematical-foundations">Simple Linear Regression: Mathematical Foundations<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-simple-linear-regression-mathematical-foundations" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-model-specification">Model Specification<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-model-specification" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-basic-form">Basic Form<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-basic-form" title="Permanent link">&para;</a></h3>
<p>The simple linear regression model is expressed as:
<div class="highlight"><pre><span></span><code>Y = β₀ + β₁X + ε
</code></pre></div>
where:
- Y is the dependent variable
- X is the independent variable
- β₀ is the y-intercept (value of Y when X = 0)
- β₁ is the slope (change in Y for one unit change in X)
- ε is the error term (random disturbance)</p>
<h2 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-model-assumptions">Model Assumptions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-model-assumptions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-1-linearity">1. Linearity<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-1-linearity" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mathematical Form</strong>: E[Y|X] = β₀ + β₁X</li>
<li><strong>Interpretation</strong>: The conditional expectation of Y given X is a linear function</li>
<li><strong>Violation Impact</strong>: Biased estimates, poor predictions</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-2-independence">2. Independence<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-2-independence" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mathematical Form</strong>: cov(εᵢ, εⱼ) = 0 for all i ≠ j</li>
<li><strong>Interpretation</strong>: No relationship between error terms</li>
<li><strong>Detection</strong>: Through autocorrelation function: ρₖ = cov(εₜ, εₜ₊ₖ)/var(ε)</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-3-homoscedasticity">3. Homoscedasticity<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-3-homoscedasticity" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mathematical Form</strong>: var(ε|X) = σ²</li>
<li><strong>Interpretation</strong>: Constant variance of errors across all X values</li>
<li><strong>Violation Impact</strong>: Inefficient estimates, invalid standard errors</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-4-normality">4. Normality<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-4-normality" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mathematical Form</strong>: ε ~ N(0, σ²)</li>
<li><strong>Interpretation</strong>: Errors follow normal distribution with mean 0 and constant variance</li>
<li><strong>Importance</strong>: Required for valid inference (t-tests, F-tests)</li>
</ul>
<h2 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-least-squares-estimation">Least Squares Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-least-squares-estimation" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-optimization-problem">Optimization Problem<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-optimization-problem" title="Permanent link">&para;</a></h3>
<p>Find β₀ and β₁ that minimize the sum of squared residuals:
<div class="highlight"><pre><span></span><code>min Σ(Yᵢ - β₀ - β₁Xᵢ)²
</code></pre></div></p>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-solutions">Solutions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-solutions" title="Permanent link">&para;</a></h3>
<p>The optimal estimates are given by:
<div class="highlight"><pre><span></span><code>β̂₁ = Σ((Xᵢ - X̄)(Yᵢ - Ȳ)) / Σ(Xᵢ - X̄)²
β̂₀ = Ȳ - β̂₁X̄
</code></pre></div>
where X̄ and Ȳ are sample means</p>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-properties-of-ols-estimators">Properties of OLS Estimators<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-properties-of-ols-estimators" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Unbiasedness</strong>: E[β̂] = β</li>
<li><strong>Consistency</strong>: β̂ → β as n → ∞</li>
<li><strong>Efficiency</strong>: Minimum variance among linear unbiased estimators (BLUE)</li>
<li><strong>Sampling Distributions</strong>:
   <div class="highlight"><pre><span></span><code>β̂₁ ~ N(β₁, σ²/Σ(Xᵢ - X̄)²)
β̂₀ ~ N(β₀, σ²(1/n + X̄²/Σ(Xᵢ - X̄)²))
</code></pre></div></li>
</ol>
<h2 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-measures-of-fit">Measures of Fit<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-measures-of-fit" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-r-squared">R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-r-squared" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Formula</strong>: R² = 1 - SSE/SST
  where:</li>
<li>SSE = Σ(Yᵢ - Ŷᵢ)² (Sum of Squared Errors)</li>
<li>
<p>SST = Σ(Yᵢ - Ȳ)² (Total Sum of Squares)</p>
</li>
<li>
<p><strong>Interpretation</strong>: Proportion of variance in Y explained by X</p>
</li>
<li><strong>Range</strong>: [0,1], with 1 indicating perfect fit</li>
</ul>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-adjusted-r-squared">Adjusted R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-adjusted-r-squared" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Formula</strong>: R̄² = 1 - (1-R²)(n-1)/(n-p-1)
  where:</li>
<li>n = sample size</li>
<li>
<p>p = number of predictors (1 for simple regression)</p>
</li>
<li>
<p><strong>Interpretation</strong>: R² adjusted for model complexity</p>
</li>
<li><strong>Advantage</strong>: Penalizes unnecessary predictors</li>
</ul>
<h2 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-residual-analysis">Residual Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-residual-analysis" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-residual-definition">Residual Definition<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-residual-definition" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>eᵢ = Yᵢ - Ŷᵢ
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-standardized-residuals">Standardized Residuals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-standardized-residuals" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>rᵢ = eᵢ/(s√(1-hᵢᵢ))
</code></pre></div>
where:
- s = standard error of regression
- hᵢᵢ = leverage of observation i</p>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-key-properties">Key Properties<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-key-properties" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Sum</strong>: Σeᵢ = 0</li>
<li><strong>Correlation</strong>: Σ(Xᵢeᵢ) = 0</li>
<li><strong>Distribution</strong>: Under assumptions, eᵢ/σ ~ N(0,1-hᵢᵢ)</li>
</ol>
<h2 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-statistical-inference">Statistical Inference<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-statistical-inference" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-standard-errors">Standard Errors<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-standard-errors" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>SE(β̂₁) = σ/√(Σ(Xᵢ - X̄)²)
SE(β̂₀) = σ√(1/n + X̄²/Σ(Xᵢ - X̄)²)
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-hypothesis-testing">Hypothesis Testing<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-hypothesis-testing" title="Permanent link">&para;</a></h3>
<p>For slope:
<div class="highlight"><pre><span></span><code>H₀: β₁ = 0
H₁: β₁ ≠ 0
t = β̂₁/SE(β̂₁) ~ t(n-2)
</code></pre></div></p>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-confidence-intervals">Confidence Intervals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-confidence-intervals" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>β̂₁ ± t(α/2,n-2)SE(β̂₁)
β̂₀ ± t(α/2,n-2)SE(β̂₀)
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-prediction">Prediction<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-prediction" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-point-prediction">Point Prediction<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-point-prediction" title="Permanent link">&para;</a></h3>
<p>For new observation X₀:
<div class="highlight"><pre><span></span><code>Ŷ₀ = β̂₀ + β̂₁X₀
</code></pre></div></p>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-prediction-interval">Prediction Interval<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-prediction-interval" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Ŷ₀ ± t(α/2,n-2)s√(1 + 1/n + (X₀-X̄)²/Σ(Xᵢ-X̄)²)
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-multiple_linear_regression-confidence-interval-for-mean-response">Confidence Interval for Mean Response<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-multiple_linear_regression-confidence-interval-for-mean-response" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Ŷ₀ ± t(α/2,n-2)s√(1/n + (X₀-X̄)²/Σ(Xᵢ-X̄)²)
</code></pre></div>
<p>Remember:
1. Assumptions are crucial for valid inference
2. R² alone is insufficient for model assessment
3. Residual analysis provides insight into model adequacy
4. Prediction intervals are wider than confidence intervals
5. Model simplicity aids interpretation</p></section><section class="print-page" id="grassroots-statistics-5_regression_analysis-simple_linear_regression"><h1 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-simple-linear-regression">Simple Linear Regression<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-simple-linear-regression" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-model-fundamentals">Model Fundamentals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-model-fundamentals" title="Permanent link">&para;</a></h2>
<p>Linear regression models the relationship between a dependent variable y and a single independent variable x:
<div class="highlight"><pre><span></span><code>y = β₀ + β₁x + ε
</code></pre></div>
where:
- β₀ is the intercept
- β₁ is the slope
- ε is the error term</p>
<h2 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-assumptions">Assumptions<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-assumptions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-1-linearity">1. Linearity<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-1-linearity" title="Permanent link">&para;</a></h3>
<ul>
<li>Relationship between x and y is linear</li>
<li>Can be checked through scatter plots</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_linearity</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot data and check linearity assumption</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Independent Variable&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Dependent Variable&#39;</span><span class="p">)</span>

    <span class="c1"># Add lowess smoother for comparison</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.nonparametric.smoothers_lowess</span><span class="w"> </span><span class="kn">import</span> <span class="n">lowess</span>
    <span class="n">smooth</span> <span class="o">=</span> <span class="n">lowess</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">frac</span><span class="o">=</span><span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smooth</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">smooth</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;LOWESS&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-2-independence">2. Independence<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-2-independence" title="Permanent link">&para;</a></h3>
<ul>
<li>Observations are independent of each other</li>
<li>No autocorrelation in residuals</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_independence</span><span class="p">(</span><span class="n">residuals</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check independence using Durbin-Watson test</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.stats.stattools</span><span class="w"> </span><span class="kn">import</span> <span class="n">durbin_watson</span>
    <span class="n">dw_stat</span> <span class="o">=</span> <span class="n">durbin_watson</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;dw_statistic&#39;</span><span class="p">:</span> <span class="n">dw_stat</span><span class="p">,</span>
        <span class="s1">&#39;is_independent&#39;</span><span class="p">:</span> <span class="mf">1.5</span> <span class="o">&lt;</span> <span class="n">dw_stat</span> <span class="o">&lt;</span> <span class="mf">2.5</span>
    <span class="p">}</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-3-homoscedasticity">3. Homoscedasticity<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-3-homoscedasticity" title="Permanent link">&para;</a></h3>
<ul>
<li>Constant variance of residuals</li>
<li>Can be checked through residual plots</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_homoscedasticity</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">residuals</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot residuals vs fitted values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fitted_values</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">fitted_values</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted Values&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-4-normality-of-residuals">4. Normality of Residuals<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-4-normality-of-residuals" title="Permanent link">&para;</a></h3>
<ul>
<li>Residuals should be normally distributed</li>
<li>Can be checked through QQ plots and tests</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_normality</span><span class="p">(</span><span class="n">residuals</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check normality of residuals</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

    <span class="c1"># QQ plot</span>
    <span class="n">stats</span><span class="o">.</span><span class="n">probplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">dist</span><span class="o">=</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plt</span><span class="p">)</span>

    <span class="c1"># Shapiro-Wilk test</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">shapiro</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;p_value&#39;</span><span class="p">:</span> <span class="n">p_value</span><span class="p">,</span>
        <span class="s1">&#39;is_normal&#39;</span><span class="p">:</span> <span class="n">p_value</span> <span class="o">&gt;</span> <span class="mf">0.05</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-least-squares-estimation">Least Squares Estimation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-least-squares-estimation" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-ordinary-least-squares-ols">Ordinary Least Squares (OLS)<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-ordinary-least-squares-ols" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">fit_ols</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fit simple linear regression using OLS</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;intercept&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span>
        <span class="s1">&#39;slope&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">model</span>
    <span class="p">}</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-manual-implementation">Manual Implementation<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-manual-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">manual_ols</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Manual implementation of OLS</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Calculate slope</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_mean</span><span class="p">))</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="c1"># Calculate intercept</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">y_mean</span> <span class="o">-</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">X_mean</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;intercept&#39;</span><span class="p">:</span> <span class="n">intercept</span><span class="p">,</span>
        <span class="s1">&#39;slope&#39;</span><span class="p">:</span> <span class="n">slope</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-r-squared-and-adjusted-r-squared">R-squared and Adjusted R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-r-squared-and-adjusted-r-squared" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-r-squared">R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-r-squared" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_r_squared</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate R-squared</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ss_total</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_true</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ss_residual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">r_squared</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">ss_residual</span> <span class="o">/</span> <span class="n">ss_total</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r_squared</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-adjusted-r-squared">Adjusted R-squared<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-adjusted-r-squared" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_adjusted_r_squared</span><span class="p">(</span><span class="n">r_squared</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate adjusted R-squared</span>
<span class="sd">    n: number of observations</span>
<span class="sd">    p: number of predictors (1 for simple linear regression)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">adjusted_r_squared</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r_squared</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">p</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adjusted_r_squared</span>
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-residual-analysis">Residual Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-residual-analysis" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-comprehensive-residual-analysis">Comprehensive Residual Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-comprehensive-residual-analysis" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ResidualAnalyzer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residuals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_residuals</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_residuals</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate residuals&quot;&quot;&quot;</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">standardized_residuals</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate standardized residuals&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">stats</span><span class="o">.</span><span class="n">zscore</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run_all_checks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run all residual diagnostics&quot;&quot;&quot;</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;normality&#39;</span><span class="p">:</span> <span class="n">check_normality</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals</span><span class="p">),</span>
            <span class="s1">&#39;independence&#39;</span><span class="p">:</span> <span class="n">check_independence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals</span><span class="p">),</span>
            <span class="s1">&#39;homoscedasticity&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_homoscedasticity</span><span class="p">(),</span>
            <span class="s1">&#39;outliers&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_outliers</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_outliers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check for outliers using standardized residuals&quot;&quot;&quot;</span>
        <span class="n">std_resid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standardized_residuals</span><span class="p">()</span>
        <span class="n">outliers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">std_resid</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">threshold</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;n_outliers&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">outliers</span><span class="p">),</span>
            <span class="s1">&#39;outlier_indices&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">outliers</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_homoscedasticity</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Breusch-Pagan test for homoscedasticity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">statsmodels.stats.diagnostic</span><span class="w"> </span><span class="kn">import</span> <span class="n">het_breuschpagan</span>

        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">het_breuschpagan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">residuals</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;p_value&#39;</span><span class="p">:</span> <span class="n">p_value</span><span class="p">,</span>
            <span class="s1">&#39;is_homoscedastic&#39;</span><span class="p">:</span> <span class="n">p_value</span> <span class="o">&gt;</span> <span class="mf">0.05</span>
        <span class="p">}</span>
</code></pre></div>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-complete-regression-analysis">Complete Regression Analysis<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-complete-regression-analysis" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">complete_regression_analysis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform complete regression analysis</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Fit model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">fit_ols</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Get predictions</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Calculate metrics</span>
    <span class="n">r_squared</span> <span class="o">=</span> <span class="n">calculate_r_squared</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">adj_r_squared</span> <span class="o">=</span> <span class="n">calculate_adjusted_r_squared</span><span class="p">(</span><span class="n">r_squared</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Analyze residuals</span>
    <span class="n">analyzer</span> <span class="o">=</span> <span class="n">ResidualAnalyzer</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">residual_analysis</span> <span class="o">=</span> <span class="n">analyzer</span><span class="o">.</span><span class="n">run_all_checks</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
        <span class="s1">&#39;r_squared&#39;</span><span class="p">:</span> <span class="n">r_squared</span><span class="p">,</span>
        <span class="s1">&#39;adj_r_squared&#39;</span><span class="p">:</span> <span class="n">adj_r_squared</span><span class="p">,</span>
        <span class="s1">&#39;residual_analysis&#39;</span><span class="p">:</span> <span class="n">residual_analysis</span>
    <span class="p">}</span>
</code></pre></div>
<h2 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-best-practices">Best Practices<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-5_regression_analysis-simple_linear_regression-model-building-process">Model Building Process<a class="headerlink" href="#grassroots-statistics-5_regression_analysis-simple_linear_regression-model-building-process" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Data Preparation</strong></p>
<ul>
<li>Check for missing values</li>
<li>Handle outliers</li>
<li>Scale if necessary</li>
</ul>
</li>
<li>
<p><strong>Model Fitting</strong></p>
<ul>
<li>Fit model using OLS</li>
<li>Calculate confidence intervals</li>
<li>Assess significance</li>
</ul>
</li>
<li>
<p><strong>Model Validation</strong></p>
<ul>
<li>Check assumptions</li>
<li>Analyze residuals</li>
<li>Assess fit metrics</li>
</ul>
</li>
<li>
<p><strong>Reporting</strong></p>
<ul>
<li>Coefficient estimates</li>
<li>Standard errors</li>
<li>R-squared values</li>
<li>Assumption validations</li>
</ul>
</li>
</ol>
<p>Remember:
1. Always check assumptions
2. Look for influential points
3. Consider transformations if needed
4. Report comprehensive results
5. Interpret findings in context</p></section><h1 class='nav-section-title-end'>Ended: 5 regression analysis</h1>
                        <h3 class='nav-section-title' id='section-6-experimental-design'>
                            6 experimental design <a class='headerlink' href='#section-6-experimental-design' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-statistics-6_experimental_design"><h1 id="grassroots-statistics-6_experimental_design-experimental-design">Experimental Design<a class="headerlink" href="#grassroots-statistics-6_experimental_design-experimental-design" title="Permanent link">&para;</a></h1>
<h1 id="grassroots-statistics-6_experimental_design-basic-principles-of-experimental-design">Basic Principles of Experimental Design<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic-principles-of-experimental-design" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-6_experimental_design-randomization">Randomization<a class="headerlink" href="#grassroots-statistics-6_experimental_design-randomization" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-principle">Principle<a class="headerlink" href="#grassroots-statistics-6_experimental_design-principle" title="Permanent link">&para;</a></h3>
<p>Randomization is the random assignment of experimental units to treatments, ensuring:
- Each unit has equal probability of receiving any treatment
- Statistical independence of observations
- Validity of statistical inference</p>
<h3 id="grassroots-statistics-6_experimental_design-mathematical-foundation">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-6_experimental_design-mathematical-foundation" title="Permanent link">&para;</a></h3>
<p>For n experimental units and k treatments:
* Probability of assignment = 1/k for each treatment
* Number of possible assignments = n!/(n₁!n₂!...nₖ!)
where nᵢ is the number of units assigned to treatment i</p>
<h3 id="grassroots-statistics-6_experimental_design-key-benefits">Key Benefits<a class="headerlink" href="#grassroots-statistics-6_experimental_design-key-benefits" title="Permanent link">&para;</a></h3>
<ol>
<li>Controls for unknown confounding variables</li>
<li>Reduces selection bias</li>
<li>Allows valid statistical inference</li>
<li>Balances uncontrolled variables</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-replication">Replication<a class="headerlink" href="#grassroots-statistics-6_experimental_design-replication" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-definition">Definition<a class="headerlink" href="#grassroots-statistics-6_experimental_design-definition" title="Permanent link">&para;</a></h3>
<p>True replication involves independent observations under the same treatment conditions.</p>
<h3 id="grassroots-statistics-6_experimental_design-types-of-replication">Types of Replication<a class="headerlink" href="#grassroots-statistics-6_experimental_design-types-of-replication" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>True Replication</strong></li>
<li>Independent experimental units</li>
<li>Same treatment conditions</li>
<li>
<p>Independent measurements</p>
</li>
<li>
<p><strong>Pseudo-replication</strong></p>
</li>
<li>Multiple measurements on same unit</li>
<li>Not true independent observations</li>
<li>Limited statistical validity</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-statistical-importance">Statistical Importance<a class="headerlink" href="#grassroots-statistics-6_experimental_design-statistical-importance" title="Permanent link">&para;</a></h3>
<ul>
<li>Enables estimation of experimental error</li>
<li>Improves precision of estimates</li>
<li>Sample size determination:
<div class="highlight"><pre><span></span><code>n = 2(zα/2 + zβ)²σ²/δ²
</code></pre></div>
where:</li>
<li>σ² is variance</li>
<li>δ is minimum detectable difference</li>
<li>α is Type I error rate</li>
<li>β is Type II error rate</li>
</ul>
<h2 id="grassroots-statistics-6_experimental_design-blocking">Blocking<a class="headerlink" href="#grassroots-statistics-6_experimental_design-blocking" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-principle_1">Principle<a class="headerlink" href="#grassroots-statistics-6_experimental_design-principle_1" title="Permanent link">&para;</a></h3>
<p>Grouping experimental units into homogeneous blocks to:
* Reduce known sources of variation
* Increase precision of treatment comparisons
* Control for nuisance factors</p>
<h3 id="grassroots-statistics-6_experimental_design-mathematical-model">Mathematical Model<a class="headerlink" href="#grassroots-statistics-6_experimental_design-mathematical-model" title="Permanent link">&para;</a></h3>
<p>For randomized complete block design:
<div class="highlight"><pre><span></span><code>Yᵢⱼ = μ + τᵢ + βⱼ + εᵢⱼ
</code></pre></div>
where:
- Yᵢⱼ is response for treatment i in block j
- μ is overall mean
- τᵢ is treatment effect
- βⱼ is block effect
- εᵢⱼ is random error</p>
<h3 id="grassroots-statistics-6_experimental_design-efficiency">Efficiency<a class="headerlink" href="#grassroots-statistics-6_experimental_design-efficiency" title="Permanent link">&para;</a></h3>
<ul>
<li>Relative efficiency vs completely randomized design:
<div class="highlight"><pre><span></span><code>RE = (EMS₁/EMS₂)(df₂/df₁)
</code></pre></div>
where EMS is error mean square</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-types-of-blocks">Types of Blocks<a class="headerlink" href="#grassroots-statistics-6_experimental_design-types-of-blocks" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Physical Blocks</strong></li>
<li>Spatial grouping</li>
<li>Environmental conditions</li>
<li>
<p>Time periods</p>
</li>
<li>
<p><strong>Statistical Blocks</strong></p>
</li>
<li>Covariates</li>
<li>Matched pairs</li>
<li>Repeated measures</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-factorial-designs">Factorial Designs<a class="headerlink" href="#grassroots-statistics-6_experimental_design-factorial-designs" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-structure">Structure<a class="headerlink" href="#grassroots-statistics-6_experimental_design-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>Multiple factors studied simultaneously</li>
<li>All possible combinations of factor levels</li>
<li>Enables study of interactions</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-mathematical-model_1">Mathematical Model<a class="headerlink" href="#grassroots-statistics-6_experimental_design-mathematical-model_1" title="Permanent link">&para;</a></h3>
<p>For two-factor factorial:
<div class="highlight"><pre><span></span><code>Yᵢⱼₖ = μ + αᵢ + βⱼ + (αβ)ᵢⱼ + εᵢⱼₖ
</code></pre></div>
where:
- αᵢ is effect of factor A
- βⱼ is effect of factor B
- (αβ)ᵢⱼ is interaction effect
- εᵢⱼₖ is random error</p>
<h3 id="grassroots-statistics-6_experimental_design-properties">Properties<a class="headerlink" href="#grassroots-statistics-6_experimental_design-properties" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Main Effects</strong></li>
<li>Average effect of factor across levels of other factors</li>
<li>
<p>Marginal means comparison</p>
</li>
<li>
<p><strong>Interactions</strong></p>
</li>
<li>Non-additive effects</li>
<li>Factor effects depend on levels of other factors</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-design-efficiency">Design Efficiency<a class="headerlink" href="#grassroots-statistics-6_experimental_design-design-efficiency" title="Permanent link">&para;</a></h3>
<ul>
<li>Number of runs = product of factor levels</li>
<li>Degrees of freedom partition:
<div class="highlight"><pre><span></span><code>Total df = (a×b×r) - 1
</code></pre></div>
where:</li>
<li>a is levels of factor A</li>
<li>b is levels of factor B</li>
<li>r is replications</li>
</ul>
<h2 id="grassroots-statistics-6_experimental_design-design-considerations">Design Considerations<a class="headerlink" href="#grassroots-statistics-6_experimental_design-design-considerations" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-1-treatment-structure">1. Treatment Structure<a class="headerlink" href="#grassroots-statistics-6_experimental_design-1-treatment-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>Number of treatments</li>
<li>Factor levels</li>
<li>Interactions of interest</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-2-experimental-unit">2. Experimental Unit<a class="headerlink" href="#grassroots-statistics-6_experimental_design-2-experimental-unit" title="Permanent link">&para;</a></h3>
<ul>
<li>Definition and size</li>
<li>Independence</li>
<li>Homogeneity</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-3-design-structure">3. Design Structure<a class="headerlink" href="#grassroots-statistics-6_experimental_design-3-design-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>Randomization restrictions</li>
<li>Block size and number</li>
<li>Resource constraints</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-4-sample-size">4. Sample Size<a class="headerlink" href="#grassroots-statistics-6_experimental_design-4-sample-size" title="Permanent link">&para;</a></h3>
<ul>
<li>Power considerations</li>
<li>Resource limitations</li>
<li>Practical constraints</li>
</ul>
<h2 id="grassroots-statistics-6_experimental_design-analysis-principles">Analysis Principles<a class="headerlink" href="#grassroots-statistics-6_experimental_design-analysis-principles" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-1-anova-decomposition">1. ANOVA Decomposition<a class="headerlink" href="#grassroots-statistics-6_experimental_design-1-anova-decomposition" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>SS_Total = SS_Treatment + SS_Block + SS_Error
</code></pre></div>
<h3 id="grassroots-statistics-6_experimental_design-2-error-structure">2. Error Structure<a class="headerlink" href="#grassroots-statistics-6_experimental_design-2-error-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>Independent errors</li>
<li>Constant variance</li>
<li>Normal distribution</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-3-testing-hierarchy">3. Testing Hierarchy<a class="headerlink" href="#grassroots-statistics-6_experimental_design-3-testing-hierarchy" title="Permanent link">&para;</a></h3>
<ol>
<li>Interaction tests</li>
<li>Main effects (if interactions non-significant)</li>
<li>Simple effects (if interactions significant)</li>
</ol>
<p>Remember:
1. Randomization provides validity
2. Replication provides precision
3. Blocking increases efficiency
4. Factorial designs study interactions
5. Design choice affects analysis options</p>
<h1 id="grassroots-statistics-6_experimental_design-ab-testing">A/B Testing<a class="headerlink" href="#grassroots-statistics-6_experimental_design-ab-testing" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-6_experimental_design-sample-size-determination">Sample Size Determination<a class="headerlink" href="#grassroots-statistics-6_experimental_design-sample-size-determination" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-basic-formula">Basic Formula<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic-formula" title="Permanent link">&para;</a></h3>
<p>For comparing two proportions:
<div class="highlight"><pre><span></span><code>n = 2(zα/2 + zβ)²[p₁(1-p₁) + p₂(1-p₂)] / (p₁-p₂)²
</code></pre></div>
where:
- n is sample size per group
- α is significance level
- β is Type II error rate (1-power)
- p₁, p₂ are expected proportions
- zα/2, zβ are standard normal quantiles</p>
<h3 id="grassroots-statistics-6_experimental_design-for-continuous-outcomes">For Continuous Outcomes<a class="headerlink" href="#grassroots-statistics-6_experimental_design-for-continuous-outcomes" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>n = 2(zα/2 + zβ)²σ² / δ²
</code></pre></div>
where:
- σ² is pooled variance
- δ is minimum detectable effect</p>
<h3 id="grassroots-statistics-6_experimental_design-practical-considerations">Practical Considerations<a class="headerlink" href="#grassroots-statistics-6_experimental_design-practical-considerations" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Effect Size Specification</strong></li>
<li>Minimum Detectable Effect (MDE)</li>
<li>Business meaningful difference</li>
<li>
<p>Historical effect sizes</p>
</li>
<li>
<p><strong>Power Analysis Components</strong></p>
</li>
<li>Baseline metrics</li>
<li>Expected variance</li>
<li>Desired power (typically 0.8 or 0.9)</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-statistical-significance">Statistical Significance<a class="headerlink" href="#grassroots-statistics-6_experimental_design-statistical-significance" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-hypothesis-framework">Hypothesis Framework<a class="headerlink" href="#grassroots-statistics-6_experimental_design-hypothesis-framework" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>H₀: p₁ = p₂ (or μ₁ = μ₂)
H₁: p₁ ≠ p₂ (or μ₁ ≠ μ₂)
</code></pre></div>
<h3 id="grassroots-statistics-6_experimental_design-test-statistics">Test Statistics<a class="headerlink" href="#grassroots-statistics-6_experimental_design-test-statistics" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>For Proportions (z-test)</strong>:
<div class="highlight"><pre><span></span><code>z = (p̂₁ - p̂₂) / √[p̂(1-p̂)(1/n₁ + 1/n₂)]
</code></pre></div>
where p̂ is pooled proportion</p>
</li>
<li>
<p><strong>For Means (t-test)</strong>:
<div class="highlight"><pre><span></span><code>t = (x̄₁ - x̄₂) / √(s²₁/n₁ + s²₂/n₂)
</code></pre></div>
with Welch's correction for unequal variances</p>
</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-sequential-analysis">Sequential Analysis<a class="headerlink" href="#grassroots-statistics-6_experimental_design-sequential-analysis" title="Permanent link">&para;</a></h3>
<p>For continuous monitoring:
* <strong>O'Brien-Fleming Boundaries</strong>:
<div class="highlight"><pre><span></span><code>z_k = C/√(k/K)
</code></pre></div>
where:
- k is current look
- K is total looks
- C is critical value</p>
<h2 id="grassroots-statistics-6_experimental_design-effect-size">Effect Size<a class="headerlink" href="#grassroots-statistics-6_experimental_design-effect-size" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-standardized-measures">Standardized Measures<a class="headerlink" href="#grassroots-statistics-6_experimental_design-standardized-measures" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Cohen's d</strong>:
<div class="highlight"><pre><span></span><code>d = (μ₁ - μ₂) / σₚ
</code></pre></div>
where σₚ is pooled standard deviation</p>
</li>
<li>
<p><strong>Relative Difference</strong>:
<div class="highlight"><pre><span></span><code>Δ% = (μ₁ - μ₂) / μ₂ × 100
</code></pre></div></p>
</li>
<li>
<p><strong>Risk Ratio</strong>:
<div class="highlight"><pre><span></span><code>RR = p₁/p₂
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-confidence-intervals">Confidence Intervals<a class="headerlink" href="#grassroots-statistics-6_experimental_design-confidence-intervals" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>For Difference in Proportions</strong>:
<div class="highlight"><pre><span></span><code>(p̂₁ - p̂₂) ± zα/2√[p̂₁(1-p̂₁)/n₁ + p̂₂(1-p̂₂)/n₂]
</code></pre></div></p>
</li>
<li>
<p><strong>For Difference in Means</strong>:
<div class="highlight"><pre><span></span><code>(x̄₁ - x̄₂) ± tα/2√(s²₁/n₁ + s²₂/n₂)
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-multiple-testing-corrections">Multiple Testing Corrections<a class="headerlink" href="#grassroots-statistics-6_experimental_design-multiple-testing-corrections" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-family-wise-error-rate-fwer">Family-Wise Error Rate (FWER)<a class="headerlink" href="#grassroots-statistics-6_experimental_design-family-wise-error-rate-fwer" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Bonferroni Correction</strong>:
<div class="highlight"><pre><span></span><code>α&#39; = α/m
</code></pre></div>
where m is number of tests</p>
</li>
<li>
<p><strong>Šidák Correction</strong>:
<div class="highlight"><pre><span></span><code>α&#39; = 1 - (1-α)^(1/m)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-false-discovery-rate-fdr">False Discovery Rate (FDR)<a class="headerlink" href="#grassroots-statistics-6_experimental_design-false-discovery-rate-fdr" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Benjamini-Hochberg Procedure</strong>:</li>
<li>Order p-values: p₁ ≤ p₂ ≤ ... ≤ pₘ</li>
<li>
<p>Find largest k where:
<div class="highlight"><pre><span></span><code>pₖ ≤ (k/m)α
</code></pre></div></p>
</li>
<li>
<p><strong>Critical Value Function</strong>:
<div class="highlight"><pre><span></span><code>α(i) = (i/m)α
</code></pre></div>
where i is rank of p-value</p>
</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-sequential-testing">Sequential Testing<a class="headerlink" href="#grassroots-statistics-6_experimental_design-sequential-testing" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Alpha Spending Function</strong>:
<div class="highlight"><pre><span></span><code>α*(t) = α × t^ψ
</code></pre></div>
where:</li>
<li>t is information fraction</li>
<li>
<p>ψ is spending parameter</p>
</li>
<li>
<p><strong>Error Spending Boundaries</strong>:
<div class="highlight"><pre><span></span><code>b(t) = √[2log(log(1/t))]
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-practical-implementation">Practical Implementation<a class="headerlink" href="#grassroots-statistics-6_experimental_design-practical-implementation" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-design-considerations_1">Design Considerations<a class="headerlink" href="#grassroots-statistics-6_experimental_design-design-considerations_1" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Pre-experiment</strong></li>
<li>Define success metrics</li>
<li>Specify MDE</li>
<li>Calculate duration</li>
<li>
<p>Set stopping rules</p>
</li>
<li>
<p><strong>During Experiment</strong></p>
</li>
<li>Monitor implementation</li>
<li>Check randomization</li>
<li>Track sample sizes</li>
<li>
<p>Watch for validity threats</p>
</li>
<li>
<p><strong>Post-experiment</strong></p>
</li>
<li>Check assumptions</li>
<li>Apply corrections</li>
<li>Document findings</li>
<li>Make recommendations</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-common-pitfalls">Common Pitfalls<a class="headerlink" href="#grassroots-statistics-6_experimental_design-common-pitfalls" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Statistical</strong></li>
<li>Peeking at results</li>
<li>Insufficient power</li>
<li>Multiple testing</li>
<li>
<p>Simpson's paradox</p>
</li>
<li>
<p><strong>Practical</strong></p>
</li>
<li>Selection bias</li>
<li>Network effects</li>
<li>Seasonality</li>
<li>Novelty effects</li>
</ol>
<p>Remember:
1. Power analysis before testing
2. Clear success criteria
3. Appropriate corrections for multiple tests
4. Context-appropriate effect size measures
5. Document all decisions and assumptions</p></section><section class="print-page" id="grassroots-statistics-6_experimental_design-a_b_testing"><h1 id="grassroots-statistics-6_experimental_design-a_b_testing-ab-testing">A/B Testing<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-ab-testing" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-6_experimental_design-a_b_testing-sample-size-determination">Sample Size Determination<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-sample-size-determination" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-basic-formula">Basic Formula<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-basic-formula" title="Permanent link">&para;</a></h3>
<p>For comparing two proportions:
<div class="highlight"><pre><span></span><code>n = 2(zα/2 + zβ)²[p₁(1-p₁) + p₂(1-p₂)] / (p₁-p₂)²
</code></pre></div>
where:
- n is sample size per group
- α is significance level
- β is Type II error rate (1-power)
- p₁, p₂ are expected proportions
- zα/2, zβ are standard normal quantiles</p>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-for-continuous-outcomes">For Continuous Outcomes<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-for-continuous-outcomes" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>n = 2(zα/2 + zβ)²σ² / δ²
</code></pre></div>
where:
- σ² is pooled variance
- δ is minimum detectable effect</p>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-practical-considerations">Practical Considerations<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-practical-considerations" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Effect Size Specification</strong></li>
<li>Minimum Detectable Effect (MDE)</li>
<li>Business meaningful difference</li>
<li>
<p>Historical effect sizes</p>
</li>
<li>
<p><strong>Power Analysis Components</strong></p>
</li>
<li>Baseline metrics</li>
<li>Expected variance</li>
<li>Desired power (typically 0.8 or 0.9)</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-a_b_testing-statistical-significance">Statistical Significance<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-statistical-significance" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-hypothesis-framework">Hypothesis Framework<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-hypothesis-framework" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>H₀: p₁ = p₂ (or μ₁ = μ₂)
H₁: p₁ ≠ p₂ (or μ₁ ≠ μ₂)
</code></pre></div>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-test-statistics">Test Statistics<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-test-statistics" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>For Proportions (z-test)</strong>:
<div class="highlight"><pre><span></span><code>z = (p̂₁ - p̂₂) / √[p̂(1-p̂)(1/n₁ + 1/n₂)]
</code></pre></div>
where p̂ is pooled proportion</p>
</li>
<li>
<p><strong>For Means (t-test)</strong>:
<div class="highlight"><pre><span></span><code>t = (x̄₁ - x̄₂) / √(s²₁/n₁ + s²₂/n₂)
</code></pre></div>
with Welch's correction for unequal variances</p>
</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-sequential-analysis">Sequential Analysis<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-sequential-analysis" title="Permanent link">&para;</a></h3>
<p>For continuous monitoring:
* <strong>O'Brien-Fleming Boundaries</strong>:
<div class="highlight"><pre><span></span><code>z_k = C/√(k/K)
</code></pre></div>
where:
- k is current look
- K is total looks
- C is critical value</p>
<h2 id="grassroots-statistics-6_experimental_design-a_b_testing-effect-size">Effect Size<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-effect-size" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-standardized-measures">Standardized Measures<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-standardized-measures" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Cohen's d</strong>:
<div class="highlight"><pre><span></span><code>d = (μ₁ - μ₂) / σₚ
</code></pre></div>
where σₚ is pooled standard deviation</p>
</li>
<li>
<p><strong>Relative Difference</strong>:
<div class="highlight"><pre><span></span><code>Δ% = (μ₁ - μ₂) / μ₂ × 100
</code></pre></div></p>
</li>
<li>
<p><strong>Risk Ratio</strong>:
<div class="highlight"><pre><span></span><code>RR = p₁/p₂
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-confidence-intervals">Confidence Intervals<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-confidence-intervals" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>For Difference in Proportions</strong>:
<div class="highlight"><pre><span></span><code>(p̂₁ - p̂₂) ± zα/2√[p̂₁(1-p̂₁)/n₁ + p̂₂(1-p̂₂)/n₂]
</code></pre></div></p>
</li>
<li>
<p><strong>For Difference in Means</strong>:
<div class="highlight"><pre><span></span><code>(x̄₁ - x̄₂) ± tα/2√(s²₁/n₁ + s²₂/n₂)
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-a_b_testing-multiple-testing-corrections">Multiple Testing Corrections<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-multiple-testing-corrections" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-family-wise-error-rate-fwer">Family-Wise Error Rate (FWER)<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-family-wise-error-rate-fwer" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Bonferroni Correction</strong>:
<div class="highlight"><pre><span></span><code>α&#39; = α/m
</code></pre></div>
where m is number of tests</p>
</li>
<li>
<p><strong>Šidák Correction</strong>:
<div class="highlight"><pre><span></span><code>α&#39; = 1 - (1-α)^(1/m)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-false-discovery-rate-fdr">False Discovery Rate (FDR)<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-false-discovery-rate-fdr" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Benjamini-Hochberg Procedure</strong>:</li>
<li>Order p-values: p₁ ≤ p₂ ≤ ... ≤ pₘ</li>
<li>
<p>Find largest k where:
<div class="highlight"><pre><span></span><code>pₖ ≤ (k/m)α
</code></pre></div></p>
</li>
<li>
<p><strong>Critical Value Function</strong>:
<div class="highlight"><pre><span></span><code>α(i) = (i/m)α
</code></pre></div>
where i is rank of p-value</p>
</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-sequential-testing">Sequential Testing<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-sequential-testing" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Alpha Spending Function</strong>:
<div class="highlight"><pre><span></span><code>α*(t) = α × t^ψ
</code></pre></div>
where:</li>
<li>t is information fraction</li>
<li>
<p>ψ is spending parameter</p>
</li>
<li>
<p><strong>Error Spending Boundaries</strong>:
<div class="highlight"><pre><span></span><code>b(t) = √[2log(log(1/t))]
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-a_b_testing-practical-implementation">Practical Implementation<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-practical-implementation" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-design-considerations">Design Considerations<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-design-considerations" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Pre-experiment</strong></li>
<li>Define success metrics</li>
<li>Specify MDE</li>
<li>Calculate duration</li>
<li>
<p>Set stopping rules</p>
</li>
<li>
<p><strong>During Experiment</strong></p>
</li>
<li>Monitor implementation</li>
<li>Check randomization</li>
<li>Track sample sizes</li>
<li>
<p>Watch for validity threats</p>
</li>
<li>
<p><strong>Post-experiment</strong></p>
</li>
<li>Check assumptions</li>
<li>Apply corrections</li>
<li>Document findings</li>
<li>Make recommendations</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-a_b_testing-common-pitfalls">Common Pitfalls<a class="headerlink" href="#grassroots-statistics-6_experimental_design-a_b_testing-common-pitfalls" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Statistical</strong></li>
<li>Peeking at results</li>
<li>Insufficient power</li>
<li>Multiple testing</li>
<li>
<p>Simpson's paradox</p>
</li>
<li>
<p><strong>Practical</strong></p>
</li>
<li>Selection bias</li>
<li>Network effects</li>
<li>Seasonality</li>
<li>Novelty effects</li>
</ol>
<p>Remember:
1. Power analysis before testing
2. Clear success criteria
3. Appropriate corrections for multiple tests
4. Context-appropriate effect size measures
5. Document all decisions and assumptions</p></section><section class="print-page" id="grassroots-statistics-6_experimental_design-basic_principles"><h1 id="grassroots-statistics-6_experimental_design-basic_principles-basic-principles-of-experimental-design">Basic Principles of Experimental Design<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-basic-principles-of-experimental-design" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-6_experimental_design-basic_principles-randomization">Randomization<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-randomization" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-principle">Principle<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-principle" title="Permanent link">&para;</a></h3>
<p>Randomization is the random assignment of experimental units to treatments, ensuring:
- Each unit has equal probability of receiving any treatment
- Statistical independence of observations
- Validity of statistical inference</p>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-mathematical-foundation">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-mathematical-foundation" title="Permanent link">&para;</a></h3>
<p>For n experimental units and k treatments:
* Probability of assignment = 1/k for each treatment
* Number of possible assignments = n!/(n₁!n₂!...nₖ!)
where nᵢ is the number of units assigned to treatment i</p>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-key-benefits">Key Benefits<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-key-benefits" title="Permanent link">&para;</a></h3>
<ol>
<li>Controls for unknown confounding variables</li>
<li>Reduces selection bias</li>
<li>Allows valid statistical inference</li>
<li>Balances uncontrolled variables</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-basic_principles-replication">Replication<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-replication" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-definition">Definition<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-definition" title="Permanent link">&para;</a></h3>
<p>True replication involves independent observations under the same treatment conditions.</p>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-types-of-replication">Types of Replication<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-types-of-replication" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>True Replication</strong></li>
<li>Independent experimental units</li>
<li>Same treatment conditions</li>
<li>
<p>Independent measurements</p>
</li>
<li>
<p><strong>Pseudo-replication</strong></p>
</li>
<li>Multiple measurements on same unit</li>
<li>Not true independent observations</li>
<li>Limited statistical validity</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-statistical-importance">Statistical Importance<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-statistical-importance" title="Permanent link">&para;</a></h3>
<ul>
<li>Enables estimation of experimental error</li>
<li>Improves precision of estimates</li>
<li>Sample size determination:
<div class="highlight"><pre><span></span><code>n = 2(zα/2 + zβ)²σ²/δ²
</code></pre></div>
where:</li>
<li>σ² is variance</li>
<li>δ is minimum detectable difference</li>
<li>α is Type I error rate</li>
<li>β is Type II error rate</li>
</ul>
<h2 id="grassroots-statistics-6_experimental_design-basic_principles-blocking">Blocking<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-blocking" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-principle_1">Principle<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-principle_1" title="Permanent link">&para;</a></h3>
<p>Grouping experimental units into homogeneous blocks to:
* Reduce known sources of variation
* Increase precision of treatment comparisons
* Control for nuisance factors</p>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-mathematical-model">Mathematical Model<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-mathematical-model" title="Permanent link">&para;</a></h3>
<p>For randomized complete block design:
<div class="highlight"><pre><span></span><code>Yᵢⱼ = μ + τᵢ + βⱼ + εᵢⱼ
</code></pre></div>
where:
- Yᵢⱼ is response for treatment i in block j
- μ is overall mean
- τᵢ is treatment effect
- βⱼ is block effect
- εᵢⱼ is random error</p>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-efficiency">Efficiency<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-efficiency" title="Permanent link">&para;</a></h3>
<ul>
<li>Relative efficiency vs completely randomized design:
<div class="highlight"><pre><span></span><code>RE = (EMS₁/EMS₂)(df₂/df₁)
</code></pre></div>
where EMS is error mean square</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-types-of-blocks">Types of Blocks<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-types-of-blocks" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Physical Blocks</strong></li>
<li>Spatial grouping</li>
<li>Environmental conditions</li>
<li>
<p>Time periods</p>
</li>
<li>
<p><strong>Statistical Blocks</strong></p>
</li>
<li>Covariates</li>
<li>Matched pairs</li>
<li>Repeated measures</li>
</ol>
<h2 id="grassroots-statistics-6_experimental_design-basic_principles-factorial-designs">Factorial Designs<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-factorial-designs" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-structure">Structure<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>Multiple factors studied simultaneously</li>
<li>All possible combinations of factor levels</li>
<li>Enables study of interactions</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-mathematical-model_1">Mathematical Model<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-mathematical-model_1" title="Permanent link">&para;</a></h3>
<p>For two-factor factorial:
<div class="highlight"><pre><span></span><code>Yᵢⱼₖ = μ + αᵢ + βⱼ + (αβ)ᵢⱼ + εᵢⱼₖ
</code></pre></div>
where:
- αᵢ is effect of factor A
- βⱼ is effect of factor B
- (αβ)ᵢⱼ is interaction effect
- εᵢⱼₖ is random error</p>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-properties">Properties<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-properties" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Main Effects</strong></li>
<li>Average effect of factor across levels of other factors</li>
<li>
<p>Marginal means comparison</p>
</li>
<li>
<p><strong>Interactions</strong></p>
</li>
<li>Non-additive effects</li>
<li>Factor effects depend on levels of other factors</li>
</ol>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-design-efficiency">Design Efficiency<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-design-efficiency" title="Permanent link">&para;</a></h3>
<ul>
<li>Number of runs = product of factor levels</li>
<li>Degrees of freedom partition:
<div class="highlight"><pre><span></span><code>Total df = (a×b×r) - 1
</code></pre></div>
where:</li>
<li>a is levels of factor A</li>
<li>b is levels of factor B</li>
<li>r is replications</li>
</ul>
<h2 id="grassroots-statistics-6_experimental_design-basic_principles-design-considerations">Design Considerations<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-design-considerations" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-1-treatment-structure">1. Treatment Structure<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-1-treatment-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>Number of treatments</li>
<li>Factor levels</li>
<li>Interactions of interest</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-2-experimental-unit">2. Experimental Unit<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-2-experimental-unit" title="Permanent link">&para;</a></h3>
<ul>
<li>Definition and size</li>
<li>Independence</li>
<li>Homogeneity</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-3-design-structure">3. Design Structure<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-3-design-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>Randomization restrictions</li>
<li>Block size and number</li>
<li>Resource constraints</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-4-sample-size">4. Sample Size<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-4-sample-size" title="Permanent link">&para;</a></h3>
<ul>
<li>Power considerations</li>
<li>Resource limitations</li>
<li>Practical constraints</li>
</ul>
<h2 id="grassroots-statistics-6_experimental_design-basic_principles-analysis-principles">Analysis Principles<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-analysis-principles" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-1-anova-decomposition">1. ANOVA Decomposition<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-1-anova-decomposition" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>SS_Total = SS_Treatment + SS_Block + SS_Error
</code></pre></div>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-2-error-structure">2. Error Structure<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-2-error-structure" title="Permanent link">&para;</a></h3>
<ul>
<li>Independent errors</li>
<li>Constant variance</li>
<li>Normal distribution</li>
</ul>
<h3 id="grassroots-statistics-6_experimental_design-basic_principles-3-testing-hierarchy">3. Testing Hierarchy<a class="headerlink" href="#grassroots-statistics-6_experimental_design-basic_principles-3-testing-hierarchy" title="Permanent link">&para;</a></h3>
<ol>
<li>Interaction tests</li>
<li>Main effects (if interactions non-significant)</li>
<li>Simple effects (if interactions significant)</li>
</ol>
<p>Remember:
1. Randomization provides validity
2. Replication provides precision
3. Blocking increases efficiency
4. Factorial designs study interactions
5. Design choice affects analysis options</p></section><h1 class='nav-section-title-end'>Ended: 6 experimental design</h1>
                        <h3 class='nav-section-title' id='section-7-bayesian-statistics'>
                            7 bayesian statistics <a class='headerlink' href='#section-7-bayesian-statistics' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-statistics-7_bayesian_statistics"><h1 id="grassroots-statistics-7_bayesian_statistics-bayesian-statistics">Bayesian Statistics<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-bayesian-statistics" title="Permanent link">&para;</a></h1>
<h1 id="grassroots-statistics-7_bayesian_statistics-fundamentals-of-bayesian-statistics">Fundamentals of Bayesian Statistics<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-of-bayesian-statistics" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-7_bayesian_statistics-bayes-theorem">Bayes' Theorem<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-bayes-theorem" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-basic-form">Basic Form<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-basic-form" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>P(θ|X) = P(X|θ)P(θ) / P(X)
</code></pre></div>
where:
- P(θ|X) is posterior probability
- P(X|θ) is likelihood
- P(θ) is prior probability
- P(X) is marginal likelihood (normalizing constant)</p>
<h3 id="grassroots-statistics-7_bayesian_statistics-expanded-form">Expanded Form<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-expanded-form" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>P(X) = ∫P(X|θ)P(θ)dθ
</code></pre></div>
or for discrete case:
<div class="highlight"><pre><span></span><code>P(X) = ΣP(X|θ)P(θ)
</code></pre></div></p>
<h3 id="grassroots-statistics-7_bayesian_statistics-alternative-expression">Alternative Expression<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-alternative-expression" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>Posterior ∝ Likelihood × Prior
</code></pre></div>
avoiding computation of normalizing constant</p>
<h2 id="grassroots-statistics-7_bayesian_statistics-prior-and-posterior-distributions">Prior and Posterior Distributions<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-prior-and-posterior-distributions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-prior-distributions">Prior Distributions<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-prior-distributions" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Informative Priors</strong></li>
<li>Based on previous knowledge</li>
<li>Expert opinion</li>
<li>Historical data</li>
<li>
<p>Example: N(μ₀, σ₀²) for known mean</p>
</li>
<li>
<p><strong>Non-informative Priors</strong></p>
</li>
<li>Uniform distribution</li>
<li>Jeffreys prior: √|I(θ)|</li>
<li>Reference priors</li>
<li>
<p>Example: P(θ) ∝ 1 for location parameter</p>
</li>
<li>
<p><strong>Hierarchical Priors</strong></p>
</li>
<li>Parameters have their own priors</li>
<li>Hyperparameters
   <div class="highlight"><pre><span></span><code>P(θ) = ∫P(θ|η)P(η)dη
</code></pre></div></li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-posterior-distributions">Posterior Distributions<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-posterior-distributions" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Point Estimates</strong></li>
<li>Posterior mean: E[θ|X]</li>
<li>Posterior median</li>
<li>
<p>Maximum a posteriori (MAP)</p>
</li>
<li>
<p><strong>Interval Estimates</strong></p>
</li>
<li>Credible intervals</li>
<li>
<p>Highest posterior density (HPD)
   <div class="highlight"><pre><span></span><code>P(a ≤ θ ≤ b|X) = 1-α
</code></pre></div></p>
</li>
<li>
<p><strong>Posterior Predictive</strong>
   <div class="highlight"><pre><span></span><code>P(X̃|X) = ∫P(X̃|θ)P(θ|X)dθ
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-conjugate-priors">Conjugate Priors<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-conjugate-priors" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-definition">Definition<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-definition" title="Permanent link">&para;</a></h3>
<p>Prior and posterior from same family of distributions</p>
<h3 id="grassroots-statistics-7_bayesian_statistics-common-conjugate-pairs">Common Conjugate Pairs<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-common-conjugate-pairs" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Binomial-Beta</strong></li>
<li>Likelihood: Binomial(n,θ)</li>
<li>Prior: Beta(α,β)</li>
<li>
<p>Posterior: Beta(α+x, β+n-x)
   where x is number of successes</p>
</li>
<li>
<p><strong>Normal-Normal</strong></p>
</li>
<li>Likelihood: N(θ,σ²)</li>
<li>Prior: N(μ₀,σ₀²)</li>
<li>
<p>Posterior: N(μₙ,σₙ²)
   where:
   <div class="highlight"><pre><span></span><code>μₙ = (σ⁻²X̄n + σ₀⁻²μ₀)/(σ⁻²n + σ₀⁻²)
σₙ² = 1/(σ⁻²n + σ₀⁻²)
</code></pre></div></p>
</li>
<li>
<p><strong>Poisson-Gamma</strong></p>
</li>
<li>Likelihood: Poisson(θ)</li>
<li>Prior: Gamma(α,β)</li>
<li>Posterior: Gamma(α+Σx, β+n)</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-bayesian-inference">Bayesian Inference<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-bayesian-inference" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-parameter-estimation">Parameter Estimation<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-parameter-estimation" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Point Estimation</strong>
   <div class="highlight"><pre><span></span><code>θ̂ = E[θ|X] = ∫θP(θ|X)dθ
</code></pre></div></p>
</li>
<li>
<p><strong>Interval Estimation</strong></p>
</li>
<li>Equal-tailed interval:
   <div class="highlight"><pre><span></span><code>[θₗ,θᵤ]: P(θ &lt; θₗ|X) = P(θ &gt; θᵤ|X) = α/2
</code></pre></div></li>
<li>HPD interval:
   <div class="highlight"><pre><span></span><code>P(θ∈[θₗ,θᵤ]|X) = 1-α
</code></pre></div>
   minimizing θᵤ-θₗ</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-hypothesis-testing">Hypothesis Testing<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-hypothesis-testing" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Bayes Factor</strong>:
<div class="highlight"><pre><span></span><code>BF₁₀ = P(X|H₁)/P(X|H₀)
</code></pre></div>
Interpretation:</li>
<li>BF₁₀ &gt; 1: Evidence for H₁</li>
<li>
<p>BF₁₀ &lt; 1: Evidence for H₀</p>
</li>
<li>
<p><strong>Posterior Probability</strong>:
<div class="highlight"><pre><span></span><code>P(H₁|X) = P(X|H₁)P(H₁)/P(X)
</code></pre></div></p>
</li>
<li>
<p><strong>Decision Theory</strong>:
<div class="highlight"><pre><span></span><code>d* = argmin_d ∫L(d,θ)P(θ|X)dθ
</code></pre></div>
where L is loss function</p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-model-comparison">Model Comparison<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-model-comparison" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Bayesian Model Averaging</strong>:
<div class="highlight"><pre><span></span><code>P(θ|X) = ΣP(θ|M_k,X)P(M_k|X)
</code></pre></div></p>
</li>
<li>
<p><strong>DIC (Deviance Information Criterion)</strong>:
<div class="highlight"><pre><span></span><code>DIC = D̄ + pD
</code></pre></div>
where:</p>
</li>
<li>D̄ is expected deviance</li>
<li>pD is effective number of parameters</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-practical-considerations">Practical Considerations<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-practical-considerations" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-prior-selection">Prior Selection<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-prior-selection" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Sensitivity Analysis</strong></li>
<li>Multiple priors</li>
<li>Impact on conclusions</li>
<li>
<p>Robustness checks</p>
</li>
<li>
<p><strong>Elicitation</strong></p>
</li>
<li>Expert knowledge</li>
<li>Historical data</li>
<li>Meta-analysis</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-computation">Computation<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-computation" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Analytical Solutions</strong></li>
<li>Conjugate priors</li>
<li>
<p>Simple models</p>
</li>
<li>
<p><strong>Numerical Methods</strong></p>
</li>
<li>MCMC</li>
<li>Variational inference</li>
<li>Laplace approximation</li>
</ol>
<p>Remember:
1. Prior specification is crucial
2. Conjugate priors simplify computation
3. Interpretation differs from frequentist
4. Model checking is important
5. Consider computational feasibility</p>
<h1 id="grassroots-statistics-7_bayesian_statistics-bayesian-statistical-applications">Bayesian Statistical Applications<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-bayesian-statistical-applications" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-7_bayesian_statistics-bayesian-ab-testing">Bayesian A/B Testing<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-bayesian-ab-testing" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-model-formulation">Model Formulation<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-model-formulation" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Binomial Model</strong>
For conversion rates:
<div class="highlight"><pre><span></span><code>Group A: yₐ ~ Binomial(nₐ, θₐ)
Group B: yᵦ ~ Binomial(nᵦ, θᵦ)
Priors: θₐ, θᵦ ~ Beta(α, β)
</code></pre></div></p>
</li>
<li>
<p><strong>Normal Model</strong>
For continuous metrics:
<div class="highlight"><pre><span></span><code>Group A: yₐ ~ N(μₐ, σ²)
Group B: yᵦ ~ N(μᵦ, σ²)
Priors: μₐ, μᵦ ~ N(μ₀, σ₀²)
        σ² ~ InvGamma(α, β)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-inference">Inference<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-inference" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Probability of Improvement</strong>
<div class="highlight"><pre><span></span><code>P(θᵦ &gt; θₐ|data) = ∫∫I(θᵦ &gt; θₐ)p(θₐ,θᵦ|data)dθₐdθᵦ
</code></pre></div></p>
</li>
<li>
<p><strong>Expected Lift</strong>
<div class="highlight"><pre><span></span><code>E[θᵦ - θₐ|data] = E[θᵦ|data] - E[θₐ|data]
</code></pre></div></p>
</li>
<li>
<p><strong>Risk Assessment</strong>
<div class="highlight"><pre><span></span><code>P(θᵦ - θₐ &gt; δ|data)  # Probability of meaningful difference
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-bayesian-regression">Bayesian Regression<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-bayesian-regression" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-linear-regression-model">Linear Regression Model<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-linear-regression-model" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>y = Xβ + ε
ε ~ N(0, σ²I)
</code></pre></div>
<h3 id="grassroots-statistics-7_bayesian_statistics-prior-specifications">Prior Specifications<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-prior-specifications" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Coefficients</strong>
<div class="highlight"><pre><span></span><code>β ~ N(μ₀, Σ₀)  # Multivariate normal prior
</code></pre></div></p>
</li>
<li>
<p><strong>Variance</strong>
<div class="highlight"><pre><span></span><code>σ² ~ InvGamma(α, β)  # Inverse gamma prior
</code></pre></div></p>
</li>
<li>
<p><strong>Joint Posterior</strong>
<div class="highlight"><pre><span></span><code>p(β,σ²|y) ∝ p(y|β,σ²)p(β)p(σ²)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-posterior-inference">Posterior Inference<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-posterior-inference" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Parameter Estimation</strong>
<div class="highlight"><pre><span></span><code>E[β|y] = (X&#39;X + σ²Σ₀⁻¹)⁻¹(X&#39;y + σ²Σ₀⁻¹μ₀)
Var(β|y) = σ²(X&#39;X + σ²Σ₀⁻¹)⁻¹
</code></pre></div></p>
</li>
<li>
<p><strong>Prediction</strong>
<div class="highlight"><pre><span></span><code>p(ỹ|x̃,y) = ∫∫p(ỹ|x̃,β,σ²)p(β,σ²|y)dβdσ²
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-hierarchical-models">Hierarchical Models<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-hierarchical-models" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-general-structure">General Structure<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-general-structure" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Level 1 (Data): y_i ~ p(y|θᵢ)
Level 2 (Parameters): θᵢ ~ p(θ|η)
Level 3 (Hyperparameters): η ~ p(η)
</code></pre></div>
<h3 id="grassroots-statistics-7_bayesian_statistics-hierarchical-linear-model">Hierarchical Linear Model<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-hierarchical-linear-model" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Model Specification</strong>
<div class="highlight"><pre><span></span><code>yᵢⱼ = β₀ⱼ + β₁ⱼxᵢⱼ + εᵢⱼ
β₀ⱼ = γ₀₀ + γ₀₁wⱼ + u₀ⱼ
β₁ⱼ = γ₁₀ + γ₁₁wⱼ + u₁ⱼ
</code></pre></div></p>
</li>
<li>
<p><strong>Distribution Assumptions</strong>
<div class="highlight"><pre><span></span><code>εᵢⱼ ~ N(0, σ²)
[u₀ⱼ, u₁ⱼ]&#39; ~ N(0, Σ)
</code></pre></div></p>
</li>
<li>
<p><strong>Prior Specifications</strong>
<div class="highlight"><pre><span></span><code>γ ~ N(μγ, Σγ)
σ² ~ InvGamma(α₁, β₁)
Σ ~ InvWishart(ν, S)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-hierarchical-logistic-regression">Hierarchical Logistic Regression<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-hierarchical-logistic-regression" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Model Structure</strong>
<div class="highlight"><pre><span></span><code>yᵢⱼ ~ Bernoulli(pᵢⱼ)
logit(pᵢⱼ) = β₀ⱼ + β₁ⱼxᵢⱼ
β₀ⱼ ~ N(μ₀, τ₀²)
β₁ⱼ ~ N(μ₁, τ₁²)
</code></pre></div></p>
</li>
<li>
<p><strong>Hyperpriors</strong>
<div class="highlight"><pre><span></span><code>μ₀,μ₁ ~ N(0, σ²)
τ₀²,τ₁² ~ InvGamma(α, β)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-implementation-considerations">Implementation Considerations<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-implementation-considerations" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Model Building</strong></li>
<li>Start simple</li>
<li>Add complexity gradually</li>
<li>Check convergence</li>
<li>
<p>Assess model fit</p>
</li>
<li>
<p><strong>Prior Selection</strong></p>
</li>
<li>Weakly informative</li>
<li>Domain knowledge</li>
<li>
<p>Sensitivity analysis</p>
</li>
<li>
<p><strong>Computational Methods</strong></p>
</li>
<li>MCMC (Gibbs, Metropolis-Hastings)</li>
<li>Hamiltonian Monte Carlo</li>
<li>Variational inference</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-diagnostics-and-model-checking">Diagnostics and Model Checking<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-diagnostics-and-model-checking" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>MCMC Diagnostics</strong>
<div class="highlight"><pre><span></span><code>R̂ (Gelman-Rubin statistic)
Effective sample size
Trace plots
Autocorrelation
</code></pre></div></p>
</li>
<li>
<p><strong>Posterior Predictive Checks</strong>
<div class="highlight"><pre><span></span><code>yrep ~ p(y|θ)  # Simulated data
T(yrep) vs T(y)  # Test statistics comparison
</code></pre></div></p>
</li>
<li>
<p><strong>Model Comparison</strong>
<div class="highlight"><pre><span></span><code>WAIC = -2(lppd - pWAIC)
DIC = D̄ + pD
LOO-CV = Σᵢlog p(yᵢ|y₋ᵢ)
</code></pre></div></p>
</li>
</ol>
<p>Remember:
1. Model complexity matches data structure
2. Check convergence and mixing
3. Validate assumptions
4. Consider computational efficiency
5. Use appropriate diagnostics</p></section><section class="print-page" id="grassroots-statistics-7_bayesian_statistics-applications"><h1 id="grassroots-statistics-7_bayesian_statistics-applications-bayesian-statistical-applications">Bayesian Statistical Applications<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-bayesian-statistical-applications" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-7_bayesian_statistics-applications-bayesian-ab-testing">Bayesian A/B Testing<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-bayesian-ab-testing" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-model-formulation">Model Formulation<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-model-formulation" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Binomial Model</strong>
For conversion rates:
<div class="highlight"><pre><span></span><code>Group A: yₐ ~ Binomial(nₐ, θₐ)
Group B: yᵦ ~ Binomial(nᵦ, θᵦ)
Priors: θₐ, θᵦ ~ Beta(α, β)
</code></pre></div></p>
</li>
<li>
<p><strong>Normal Model</strong>
For continuous metrics:
<div class="highlight"><pre><span></span><code>Group A: yₐ ~ N(μₐ, σ²)
Group B: yᵦ ~ N(μᵦ, σ²)
Priors: μₐ, μᵦ ~ N(μ₀, σ₀²)
        σ² ~ InvGamma(α, β)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-inference">Inference<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-inference" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Probability of Improvement</strong>
<div class="highlight"><pre><span></span><code>P(θᵦ &gt; θₐ|data) = ∫∫I(θᵦ &gt; θₐ)p(θₐ,θᵦ|data)dθₐdθᵦ
</code></pre></div></p>
</li>
<li>
<p><strong>Expected Lift</strong>
<div class="highlight"><pre><span></span><code>E[θᵦ - θₐ|data] = E[θᵦ|data] - E[θₐ|data]
</code></pre></div></p>
</li>
<li>
<p><strong>Risk Assessment</strong>
<div class="highlight"><pre><span></span><code>P(θᵦ - θₐ &gt; δ|data)  # Probability of meaningful difference
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-applications-bayesian-regression">Bayesian Regression<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-bayesian-regression" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-linear-regression-model">Linear Regression Model<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-linear-regression-model" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>y = Xβ + ε
ε ~ N(0, σ²I)
</code></pre></div>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-prior-specifications">Prior Specifications<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-prior-specifications" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Coefficients</strong>
<div class="highlight"><pre><span></span><code>β ~ N(μ₀, Σ₀)  # Multivariate normal prior
</code></pre></div></p>
</li>
<li>
<p><strong>Variance</strong>
<div class="highlight"><pre><span></span><code>σ² ~ InvGamma(α, β)  # Inverse gamma prior
</code></pre></div></p>
</li>
<li>
<p><strong>Joint Posterior</strong>
<div class="highlight"><pre><span></span><code>p(β,σ²|y) ∝ p(y|β,σ²)p(β)p(σ²)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-posterior-inference">Posterior Inference<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-posterior-inference" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Parameter Estimation</strong>
<div class="highlight"><pre><span></span><code>E[β|y] = (X&#39;X + σ²Σ₀⁻¹)⁻¹(X&#39;y + σ²Σ₀⁻¹μ₀)
Var(β|y) = σ²(X&#39;X + σ²Σ₀⁻¹)⁻¹
</code></pre></div></p>
</li>
<li>
<p><strong>Prediction</strong>
<div class="highlight"><pre><span></span><code>p(ỹ|x̃,y) = ∫∫p(ỹ|x̃,β,σ²)p(β,σ²|y)dβdσ²
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-applications-hierarchical-models">Hierarchical Models<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-hierarchical-models" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-general-structure">General Structure<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-general-structure" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code>Level 1 (Data): y_i ~ p(y|θᵢ)
Level 2 (Parameters): θᵢ ~ p(θ|η)
Level 3 (Hyperparameters): η ~ p(η)
</code></pre></div>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-hierarchical-linear-model">Hierarchical Linear Model<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-hierarchical-linear-model" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Model Specification</strong>
<div class="highlight"><pre><span></span><code>yᵢⱼ = β₀ⱼ + β₁ⱼxᵢⱼ + εᵢⱼ
β₀ⱼ = γ₀₀ + γ₀₁wⱼ + u₀ⱼ
β₁ⱼ = γ₁₀ + γ₁₁wⱼ + u₁ⱼ
</code></pre></div></p>
</li>
<li>
<p><strong>Distribution Assumptions</strong>
<div class="highlight"><pre><span></span><code>εᵢⱼ ~ N(0, σ²)
[u₀ⱼ, u₁ⱼ]&#39; ~ N(0, Σ)
</code></pre></div></p>
</li>
<li>
<p><strong>Prior Specifications</strong>
<div class="highlight"><pre><span></span><code>γ ~ N(μγ, Σγ)
σ² ~ InvGamma(α₁, β₁)
Σ ~ InvWishart(ν, S)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-hierarchical-logistic-regression">Hierarchical Logistic Regression<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-hierarchical-logistic-regression" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Model Structure</strong>
<div class="highlight"><pre><span></span><code>yᵢⱼ ~ Bernoulli(pᵢⱼ)
logit(pᵢⱼ) = β₀ⱼ + β₁ⱼxᵢⱼ
β₀ⱼ ~ N(μ₀, τ₀²)
β₁ⱼ ~ N(μ₁, τ₁²)
</code></pre></div></p>
</li>
<li>
<p><strong>Hyperpriors</strong>
<div class="highlight"><pre><span></span><code>μ₀,μ₁ ~ N(0, σ²)
τ₀²,τ₁² ~ InvGamma(α, β)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-implementation-considerations">Implementation Considerations<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-implementation-considerations" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Model Building</strong></li>
<li>Start simple</li>
<li>Add complexity gradually</li>
<li>Check convergence</li>
<li>
<p>Assess model fit</p>
</li>
<li>
<p><strong>Prior Selection</strong></p>
</li>
<li>Weakly informative</li>
<li>Domain knowledge</li>
<li>
<p>Sensitivity analysis</p>
</li>
<li>
<p><strong>Computational Methods</strong></p>
</li>
<li>MCMC (Gibbs, Metropolis-Hastings)</li>
<li>Hamiltonian Monte Carlo</li>
<li>Variational inference</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-applications-diagnostics-and-model-checking">Diagnostics and Model Checking<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-applications-diagnostics-and-model-checking" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>MCMC Diagnostics</strong>
<div class="highlight"><pre><span></span><code>R̂ (Gelman-Rubin statistic)
Effective sample size
Trace plots
Autocorrelation
</code></pre></div></p>
</li>
<li>
<p><strong>Posterior Predictive Checks</strong>
<div class="highlight"><pre><span></span><code>yrep ~ p(y|θ)  # Simulated data
T(yrep) vs T(y)  # Test statistics comparison
</code></pre></div></p>
</li>
<li>
<p><strong>Model Comparison</strong>
<div class="highlight"><pre><span></span><code>WAIC = -2(lppd - pWAIC)
DIC = D̄ + pD
LOO-CV = Σᵢlog p(yᵢ|y₋ᵢ)
</code></pre></div></p>
</li>
</ol>
<p>Remember:
1. Model complexity matches data structure
2. Check convergence and mixing
3. Validate assumptions
4. Consider computational efficiency
5. Use appropriate diagnostics</p></section><section class="print-page" id="grassroots-statistics-7_bayesian_statistics-fundamentals"><h1 id="grassroots-statistics-7_bayesian_statistics-fundamentals-fundamentals-of-bayesian-statistics">Fundamentals of Bayesian Statistics<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-fundamentals-of-bayesian-statistics" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-7_bayesian_statistics-fundamentals-bayes-theorem">Bayes' Theorem<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-bayes-theorem" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-basic-form">Basic Form<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-basic-form" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>P(θ|X) = P(X|θ)P(θ) / P(X)
</code></pre></div>
where:
- P(θ|X) is posterior probability
- P(X|θ) is likelihood
- P(θ) is prior probability
- P(X) is marginal likelihood (normalizing constant)</p>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-expanded-form">Expanded Form<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-expanded-form" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>P(X) = ∫P(X|θ)P(θ)dθ
</code></pre></div>
or for discrete case:
<div class="highlight"><pre><span></span><code>P(X) = ΣP(X|θ)P(θ)
</code></pre></div></p>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-alternative-expression">Alternative Expression<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-alternative-expression" title="Permanent link">&para;</a></h3>
<p><div class="highlight"><pre><span></span><code>Posterior ∝ Likelihood × Prior
</code></pre></div>
avoiding computation of normalizing constant</p>
<h2 id="grassroots-statistics-7_bayesian_statistics-fundamentals-prior-and-posterior-distributions">Prior and Posterior Distributions<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-prior-and-posterior-distributions" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-prior-distributions">Prior Distributions<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-prior-distributions" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Informative Priors</strong></li>
<li>Based on previous knowledge</li>
<li>Expert opinion</li>
<li>Historical data</li>
<li>
<p>Example: N(μ₀, σ₀²) for known mean</p>
</li>
<li>
<p><strong>Non-informative Priors</strong></p>
</li>
<li>Uniform distribution</li>
<li>Jeffreys prior: √|I(θ)|</li>
<li>Reference priors</li>
<li>
<p>Example: P(θ) ∝ 1 for location parameter</p>
</li>
<li>
<p><strong>Hierarchical Priors</strong></p>
</li>
<li>Parameters have their own priors</li>
<li>Hyperparameters
   <div class="highlight"><pre><span></span><code>P(θ) = ∫P(θ|η)P(η)dη
</code></pre></div></li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-posterior-distributions">Posterior Distributions<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-posterior-distributions" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Point Estimates</strong></li>
<li>Posterior mean: E[θ|X]</li>
<li>Posterior median</li>
<li>
<p>Maximum a posteriori (MAP)</p>
</li>
<li>
<p><strong>Interval Estimates</strong></p>
</li>
<li>Credible intervals</li>
<li>
<p>Highest posterior density (HPD)
   <div class="highlight"><pre><span></span><code>P(a ≤ θ ≤ b|X) = 1-α
</code></pre></div></p>
</li>
<li>
<p><strong>Posterior Predictive</strong>
   <div class="highlight"><pre><span></span><code>P(X̃|X) = ∫P(X̃|θ)P(θ|X)dθ
</code></pre></div></p>
</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-fundamentals-conjugate-priors">Conjugate Priors<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-conjugate-priors" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-definition">Definition<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-definition" title="Permanent link">&para;</a></h3>
<p>Prior and posterior from same family of distributions</p>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-common-conjugate-pairs">Common Conjugate Pairs<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-common-conjugate-pairs" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Binomial-Beta</strong></li>
<li>Likelihood: Binomial(n,θ)</li>
<li>Prior: Beta(α,β)</li>
<li>
<p>Posterior: Beta(α+x, β+n-x)
   where x is number of successes</p>
</li>
<li>
<p><strong>Normal-Normal</strong></p>
</li>
<li>Likelihood: N(θ,σ²)</li>
<li>Prior: N(μ₀,σ₀²)</li>
<li>
<p>Posterior: N(μₙ,σₙ²)
   where:
   <div class="highlight"><pre><span></span><code>μₙ = (σ⁻²X̄n + σ₀⁻²μ₀)/(σ⁻²n + σ₀⁻²)
σₙ² = 1/(σ⁻²n + σ₀⁻²)
</code></pre></div></p>
</li>
<li>
<p><strong>Poisson-Gamma</strong></p>
</li>
<li>Likelihood: Poisson(θ)</li>
<li>Prior: Gamma(α,β)</li>
<li>Posterior: Gamma(α+Σx, β+n)</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-fundamentals-bayesian-inference">Bayesian Inference<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-bayesian-inference" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-parameter-estimation">Parameter Estimation<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-parameter-estimation" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Point Estimation</strong>
   <div class="highlight"><pre><span></span><code>θ̂ = E[θ|X] = ∫θP(θ|X)dθ
</code></pre></div></p>
</li>
<li>
<p><strong>Interval Estimation</strong></p>
</li>
<li>Equal-tailed interval:
   <div class="highlight"><pre><span></span><code>[θₗ,θᵤ]: P(θ &lt; θₗ|X) = P(θ &gt; θᵤ|X) = α/2
</code></pre></div></li>
<li>HPD interval:
   <div class="highlight"><pre><span></span><code>P(θ∈[θₗ,θᵤ]|X) = 1-α
</code></pre></div>
   minimizing θᵤ-θₗ</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-hypothesis-testing">Hypothesis Testing<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-hypothesis-testing" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Bayes Factor</strong>:
<div class="highlight"><pre><span></span><code>BF₁₀ = P(X|H₁)/P(X|H₀)
</code></pre></div>
Interpretation:</li>
<li>BF₁₀ &gt; 1: Evidence for H₁</li>
<li>
<p>BF₁₀ &lt; 1: Evidence for H₀</p>
</li>
<li>
<p><strong>Posterior Probability</strong>:
<div class="highlight"><pre><span></span><code>P(H₁|X) = P(X|H₁)P(H₁)/P(X)
</code></pre></div></p>
</li>
<li>
<p><strong>Decision Theory</strong>:
<div class="highlight"><pre><span></span><code>d* = argmin_d ∫L(d,θ)P(θ|X)dθ
</code></pre></div>
where L is loss function</p>
</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-model-comparison">Model Comparison<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-model-comparison" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Bayesian Model Averaging</strong>:
<div class="highlight"><pre><span></span><code>P(θ|X) = ΣP(θ|M_k,X)P(M_k|X)
</code></pre></div></p>
</li>
<li>
<p><strong>DIC (Deviance Information Criterion)</strong>:
<div class="highlight"><pre><span></span><code>DIC = D̄ + pD
</code></pre></div>
where:</p>
</li>
<li>D̄ is expected deviance</li>
<li>pD is effective number of parameters</li>
</ol>
<h2 id="grassroots-statistics-7_bayesian_statistics-fundamentals-practical-considerations">Practical Considerations<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-practical-considerations" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-prior-selection">Prior Selection<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-prior-selection" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Sensitivity Analysis</strong></li>
<li>Multiple priors</li>
<li>Impact on conclusions</li>
<li>
<p>Robustness checks</p>
</li>
<li>
<p><strong>Elicitation</strong></p>
</li>
<li>Expert knowledge</li>
<li>Historical data</li>
<li>Meta-analysis</li>
</ol>
<h3 id="grassroots-statistics-7_bayesian_statistics-fundamentals-computation">Computation<a class="headerlink" href="#grassroots-statistics-7_bayesian_statistics-fundamentals-computation" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Analytical Solutions</strong></li>
<li>Conjugate priors</li>
<li>
<p>Simple models</p>
</li>
<li>
<p><strong>Numerical Methods</strong></p>
</li>
<li>MCMC</li>
<li>Variational inference</li>
<li>Laplace approximation</li>
</ol>
<p>Remember:
1. Prior specification is crucial
2. Conjugate priors simplify computation
3. Interpretation differs from frequentist
4. Model checking is important
5. Consider computational feasibility</p></section><h1 class='nav-section-title-end'>Ended: 7 bayesian statistics</h1>
                        <h3 class='nav-section-title' id='section-8-advanced-topics'>
                            8 advanced topics <a class='headerlink' href='#section-8-advanced-topics' title='Permanent link'>↵</a>
                        </h3>
                        <section class="print-page" id="grassroots-statistics-8_advanced_topics"><h1 id="grassroots-statistics-8_advanced_topics-advanced-topics">Advanced topics<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-advanced-topics" title="Permanent link">&para;</a></h1>
<h1 id="grassroots-statistics-8_advanced_topics-dimensionality-reduction">Dimensionality Reduction<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality-reduction" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-8_advanced_topics-principal-component-analysis-pca">Principal Component Analysis (PCA)<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-principal-component-analysis-pca" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-mathematical-foundation">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-mathematical-foundation" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Objective</strong></li>
<li>Find orthogonal directions maximizing variance</li>
<li>Linear transformation of data</li>
<li>
<p>Minimize reconstruction error</p>
</li>
<li>
<p><strong>Formulation</strong>
<div class="highlight"><pre><span></span><code>X = UΣV&#39;
</code></pre></div>
where:</p>
</li>
<li>X is centered data matrix (n × p)</li>
<li>U is left singular vectors (n × p)</li>
<li>Σ is diagonal matrix of singular values</li>
<li>V is right singular vectors (p × p)</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-properties">Properties<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-properties" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Principal Components</strong></li>
<li>First PC: w₁ = argmax ||w||=1 Var(Xw)</li>
<li>Subsequent PCs: orthogonal to previous</li>
<li>Loading vector: eigenvectors of X'X</li>
<li>
<p>Scores: Xw</p>
</li>
<li>
<p><strong>Variance Explained</strong>
<div class="highlight"><pre><span></span><code>λᵢ/Σλᵢ
</code></pre></div>
where λᵢ are eigenvalues of covariance matrix</p>
</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-implementation-steps">Implementation Steps<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-implementation-steps" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Data Preprocessing</strong></li>
<li>Center: X̃ = X - μ</li>
<li>
<p>(Optional) Scale: X̃ = (X - μ)/σ</p>
</li>
<li>
<p><strong>Computation</strong></p>
</li>
<li>Covariance matrix: S = X̃'X̃/n</li>
<li>Eigendecomposition: S = VΛV'</li>
<li>PC scores: Z = X̃V</li>
</ol>
<h2 id="grassroots-statistics-8_advanced_topics-factor-analysis">Factor Analysis<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-factor-analysis" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-model-specification">Model Specification<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-model-specification" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Basic Model</strong>
<div class="highlight"><pre><span></span><code>X = ΛF + ε
</code></pre></div>
where:</li>
<li>X is p-dimensional observed variables</li>
<li>Λ is p × k loading matrix</li>
<li>F is k-dimensional factors</li>
<li>
<p>ε is unique factors</p>
</li>
<li>
<p><strong>Assumptions</strong>
<div class="highlight"><pre><span></span><code>F ~ N(0, I)
ε ~ N(0, Ψ)
</code></pre></div>
where Ψ is diagonal</p>
</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-estimation-methods">Estimation Methods<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-estimation-methods" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Maximum Likelihood</strong></li>
<li>Iterative procedure</li>
<li>
<p>Likelihood:
<div class="highlight"><pre><span></span><code>L(Λ,Ψ|X) = -½[log|ΛΛ&#39;+Ψ| + tr((ΛΛ&#39;+Ψ)⁻¹S)]
</code></pre></div></p>
</li>
<li>
<p><strong>Principal Factor</strong></p>
</li>
<li>Initial estimate: Ψ = diag(1 - h²)</li>
<li>h² are communalities</li>
<li>Iterate until convergence</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-factor-rotation">Factor Rotation<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-factor-rotation" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Orthogonal Rotation</strong></li>
<li>Varimax: maximize variance of squared loadings</li>
<li>Quartimax: simplify variables</li>
<li>
<p>Equamax: compromise between varimax and quartimax</p>
</li>
<li>
<p><strong>Oblique Rotation</strong></p>
</li>
<li>Promax: start with varimax, allow correlation</li>
<li>Direct oblimin: minimize cross-products of loadings</li>
</ol>
<h2 id="grassroots-statistics-8_advanced_topics-t-sne">t-SNE<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-t-sne" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-algorithm">Algorithm<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-algorithm" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Similarity Computation</strong></li>
<li>
<p>High-dimensional similarities:
<div class="highlight"><pre><span></span><code>pⱼ|ᵢ = exp(-||xᵢ-xⱼ||²/2σᵢ²)/Σₖexp(-||xᵢ-xₖ||²/2σᵢ²)
pᵢⱼ = (pⱼ|ᵢ + pᵢ|ⱼ)/2n
</code></pre></div></p>
</li>
<li>
<p><strong>Low-dimensional Similarities</strong>
<div class="highlight"><pre><span></span><code>qᵢⱼ = (1 + ||yᵢ-yⱼ||²)⁻¹/Σₖ,ₗ(1 + ||yₖ-yₗ||²)⁻¹
</code></pre></div></p>
</li>
<li>
<p><strong>Objective Function</strong>
<div class="highlight"><pre><span></span><code>KL(P||Q) = ΣᵢΣⱼpᵢⱼlog(pᵢⱼ/qᵢⱼ)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-implementation-details">Implementation Details<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-implementation-details" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Perplexity</strong></li>
<li>Controls effective number of neighbors</li>
<li>Typically between 5 and 50</li>
<li>
<p>Adaptive σᵢ selection</p>
</li>
<li>
<p><strong>Optimization</strong></p>
</li>
<li>Gradient descent with momentum</li>
<li>Early exaggeration</li>
<li>Learning rate annealing</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-advantageslimitations">Advantages/Limitations<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-advantageslimitations" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Advantages</strong></li>
<li>Preserves local structure</li>
<li>Handles non-linear relationships</li>
<li>
<p>Reveals clusters</p>
</li>
<li>
<p><strong>Limitations</strong></p>
</li>
<li>Non-parametric (no out-of-sample extension)</li>
<li>Computationally intensive</li>
<li>Non-convex optimization</li>
</ol>
<h2 id="grassroots-statistics-8_advanced_topics-comparison-and-selection">Comparison and Selection<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-comparison-and-selection" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-method-selection-criteria">Method Selection Criteria<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-method-selection-criteria" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Data Characteristics</strong></li>
<li>Sample size</li>
<li>Dimensionality</li>
<li>Linear vs non-linear relationships</li>
<li>
<p>Sparsity</p>
</li>
<li>
<p><strong>Objectives</strong></p>
</li>
<li>Visualization</li>
<li>Feature extraction</li>
<li>Data compression</li>
<li>Structure discovery</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-performance-metrics">Performance Metrics<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-performance-metrics" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Reconstruction Error</strong>
<div class="highlight"><pre><span></span><code>||X - X̂||²
</code></pre></div></p>
</li>
<li>
<p><strong>Explained Variance</strong>
<div class="highlight"><pre><span></span><code>R² = 1 - SS_res/SS_tot
</code></pre></div></p>
</li>
<li>
<p><strong>Structure Preservation</strong></p>
</li>
<li>Trustworthiness</li>
<li>Continuity</li>
<li>Local structure preservation</li>
</ol>
<h2 id="grassroots-statistics-8_advanced_topics-best-practices">Best Practices<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-implementation-guidelines">Implementation Guidelines<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-implementation-guidelines" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Data Preprocessing</strong></li>
<li>Scaling/standardization</li>
<li>Missing value handling</li>
<li>
<p>Outlier detection</p>
</li>
<li>
<p><strong>Dimensionality Selection</strong></p>
</li>
<li>Scree plot (PCA)</li>
<li>Parallel analysis (FA)</li>
<li>
<p>Perplexity tuning (t-SNE)</p>
</li>
<li>
<p><strong>Validation</strong></p>
</li>
<li>Cross-validation</li>
<li>Stability analysis</li>
<li>Visual inspection</li>
</ol>
<p>Remember:
1. Choose method based on objectives
2. Consider computational resources
3. Validate results
4. Understand assumptions
5. Document decisions</p></section><section class="print-page" id="grassroots-statistics-8_advanced_topics-dimensionality_reduction"><h1 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-dimensionality-reduction">Dimensionality Reduction<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-dimensionality-reduction" title="Permanent link">&para;</a></h1>
<h2 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-principal-component-analysis-pca">Principal Component Analysis (PCA)<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-principal-component-analysis-pca" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-mathematical-foundation">Mathematical Foundation<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-mathematical-foundation" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Objective</strong></li>
<li>Find orthogonal directions maximizing variance</li>
<li>Linear transformation of data</li>
<li>
<p>Minimize reconstruction error</p>
</li>
<li>
<p><strong>Formulation</strong>
<div class="highlight"><pre><span></span><code>X = UΣV&#39;
</code></pre></div>
where:</p>
</li>
<li>X is centered data matrix (n × p)</li>
<li>U is left singular vectors (n × p)</li>
<li>Σ is diagonal matrix of singular values</li>
<li>V is right singular vectors (p × p)</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-properties">Properties<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-properties" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Principal Components</strong></li>
<li>First PC: w₁ = argmax ||w||=1 Var(Xw)</li>
<li>Subsequent PCs: orthogonal to previous</li>
<li>Loading vector: eigenvectors of X'X</li>
<li>
<p>Scores: Xw</p>
</li>
<li>
<p><strong>Variance Explained</strong>
<div class="highlight"><pre><span></span><code>λᵢ/Σλᵢ
</code></pre></div>
where λᵢ are eigenvalues of covariance matrix</p>
</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-implementation-steps">Implementation Steps<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-implementation-steps" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Data Preprocessing</strong></li>
<li>Center: X̃ = X - μ</li>
<li>
<p>(Optional) Scale: X̃ = (X - μ)/σ</p>
</li>
<li>
<p><strong>Computation</strong></p>
</li>
<li>Covariance matrix: S = X̃'X̃/n</li>
<li>Eigendecomposition: S = VΛV'</li>
<li>PC scores: Z = X̃V</li>
</ol>
<h2 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-factor-analysis">Factor Analysis<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-factor-analysis" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-model-specification">Model Specification<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-model-specification" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Basic Model</strong>
<div class="highlight"><pre><span></span><code>X = ΛF + ε
</code></pre></div>
where:</li>
<li>X is p-dimensional observed variables</li>
<li>Λ is p × k loading matrix</li>
<li>F is k-dimensional factors</li>
<li>
<p>ε is unique factors</p>
</li>
<li>
<p><strong>Assumptions</strong>
<div class="highlight"><pre><span></span><code>F ~ N(0, I)
ε ~ N(0, Ψ)
</code></pre></div>
where Ψ is diagonal</p>
</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-estimation-methods">Estimation Methods<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-estimation-methods" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Maximum Likelihood</strong></li>
<li>Iterative procedure</li>
<li>
<p>Likelihood:
<div class="highlight"><pre><span></span><code>L(Λ,Ψ|X) = -½[log|ΛΛ&#39;+Ψ| + tr((ΛΛ&#39;+Ψ)⁻¹S)]
</code></pre></div></p>
</li>
<li>
<p><strong>Principal Factor</strong></p>
</li>
<li>Initial estimate: Ψ = diag(1 - h²)</li>
<li>h² are communalities</li>
<li>Iterate until convergence</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-factor-rotation">Factor Rotation<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-factor-rotation" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Orthogonal Rotation</strong></li>
<li>Varimax: maximize variance of squared loadings</li>
<li>Quartimax: simplify variables</li>
<li>
<p>Equamax: compromise between varimax and quartimax</p>
</li>
<li>
<p><strong>Oblique Rotation</strong></p>
</li>
<li>Promax: start with varimax, allow correlation</li>
<li>Direct oblimin: minimize cross-products of loadings</li>
</ol>
<h2 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-t-sne">t-SNE<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-t-sne" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-algorithm">Algorithm<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-algorithm" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Similarity Computation</strong></li>
<li>
<p>High-dimensional similarities:
<div class="highlight"><pre><span></span><code>pⱼ|ᵢ = exp(-||xᵢ-xⱼ||²/2σᵢ²)/Σₖexp(-||xᵢ-xₖ||²/2σᵢ²)
pᵢⱼ = (pⱼ|ᵢ + pᵢ|ⱼ)/2n
</code></pre></div></p>
</li>
<li>
<p><strong>Low-dimensional Similarities</strong>
<div class="highlight"><pre><span></span><code>qᵢⱼ = (1 + ||yᵢ-yⱼ||²)⁻¹/Σₖ,ₗ(1 + ||yₖ-yₗ||²)⁻¹
</code></pre></div></p>
</li>
<li>
<p><strong>Objective Function</strong>
<div class="highlight"><pre><span></span><code>KL(P||Q) = ΣᵢΣⱼpᵢⱼlog(pᵢⱼ/qᵢⱼ)
</code></pre></div></p>
</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-implementation-details">Implementation Details<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-implementation-details" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Perplexity</strong></li>
<li>Controls effective number of neighbors</li>
<li>Typically between 5 and 50</li>
<li>
<p>Adaptive σᵢ selection</p>
</li>
<li>
<p><strong>Optimization</strong></p>
</li>
<li>Gradient descent with momentum</li>
<li>Early exaggeration</li>
<li>Learning rate annealing</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-advantageslimitations">Advantages/Limitations<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-advantageslimitations" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Advantages</strong></li>
<li>Preserves local structure</li>
<li>Handles non-linear relationships</li>
<li>
<p>Reveals clusters</p>
</li>
<li>
<p><strong>Limitations</strong></p>
</li>
<li>Non-parametric (no out-of-sample extension)</li>
<li>Computationally intensive</li>
<li>Non-convex optimization</li>
</ol>
<h2 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-comparison-and-selection">Comparison and Selection<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-comparison-and-selection" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-method-selection-criteria">Method Selection Criteria<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-method-selection-criteria" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Data Characteristics</strong></li>
<li>Sample size</li>
<li>Dimensionality</li>
<li>Linear vs non-linear relationships</li>
<li>
<p>Sparsity</p>
</li>
<li>
<p><strong>Objectives</strong></p>
</li>
<li>Visualization</li>
<li>Feature extraction</li>
<li>Data compression</li>
<li>Structure discovery</li>
</ol>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-performance-metrics">Performance Metrics<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-performance-metrics" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Reconstruction Error</strong>
<div class="highlight"><pre><span></span><code>||X - X̂||²
</code></pre></div></p>
</li>
<li>
<p><strong>Explained Variance</strong>
<div class="highlight"><pre><span></span><code>R² = 1 - SS_res/SS_tot
</code></pre></div></p>
</li>
<li>
<p><strong>Structure Preservation</strong></p>
</li>
<li>Trustworthiness</li>
<li>Continuity</li>
<li>Local structure preservation</li>
</ol>
<h2 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-best-practices">Best Practices<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-best-practices" title="Permanent link">&para;</a></h2>
<h3 id="grassroots-statistics-8_advanced_topics-dimensionality_reduction-implementation-guidelines">Implementation Guidelines<a class="headerlink" href="#grassroots-statistics-8_advanced_topics-dimensionality_reduction-implementation-guidelines" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Data Preprocessing</strong></li>
<li>Scaling/standardization</li>
<li>Missing value handling</li>
<li>
<p>Outlier detection</p>
</li>
<li>
<p><strong>Dimensionality Selection</strong></p>
</li>
<li>Scree plot (PCA)</li>
<li>Parallel analysis (FA)</li>
<li>
<p>Perplexity tuning (t-SNE)</p>
</li>
<li>
<p><strong>Validation</strong></p>
</li>
<li>Cross-validation</li>
<li>Stability analysis</li>
<li>Visual inspection</li>
</ol>
<p>Remember:
1. Choose method based on objectives
2. Consider computational resources
3. Validate results
4. Understand assumptions
5. Document decisions</p></section><h1 class='nav-section-title-end'>Ended: 8 advanced topics</h1><h1 class='nav-section-title-end'>Ended: Statistics</h1><h1 class='nav-section-title-end'>Ended: Grassroots</h1>
                        <h1 class='nav-section-title' id='section-research'>
                            Research <a class='headerlink' href='#section-research' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="research"><h1 id="research-research">Research<a class="headerlink" href="#research-research" title="Permanent link">&para;</a></h1>
<ol>
<li><a href="#research-0_definition">Definition</a></li>
<li><a href="#research-1_classification">Hallucination classification</a></li>
<li><a href="#research-2_detection">Detection</a></li>
<li><a href="#research-3_benchmarks">Benchmarks</a></li>
</ol></section>
                        <h2 class='nav-section-title' id='section-0-definition'>
                            0 definition <a class='headerlink' href='#section-0-definition' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="research-0_definition"><h1 id="research-0_definition-hallucination-definition">Hallucination Definition<a class="headerlink" href="#research-0_definition-hallucination-definition" title="Permanent link">&para;</a></h1>
<h3 id="research-0_definition-definition-origin">Definition origin<a class="headerlink" href="#research-0_definition-definition-origin" title="Permanent link">&para;</a></h3>
<p>Some important papers <sup id="fnref:farquharDetectingHallucinationsLarge2024"><a class="footnote-ref" href="#research-0_definition-fn:farquharDetectingHallucinationsLarge2024">1</a></sup> <sup id="fnref:filippovaControlledHallucinationsLearning2020"><a class="footnote-ref" href="#research-0_definition-fn:filippovaControlledHallucinationsLearning2020">2</a></sup>  and meta studies <sup id="fnref:jiSurveyHallucinationNatural2023"><a class="footnote-ref" href="#research-0_definition-fn:jiSurveyHallucinationNatural2023">3</a></sup> <sup id="fnref:dongMultiFactCorrectionAbstractive2020"><a class="footnote-ref" href="#research-0_definition-fn:dongMultiFactCorrectionAbstractive2020">4</a></sup>   seems to point to the definition from a summarisation paper from Google <sup id="fnref:maynezFaithfulnessFactualityAbstractive2020"><a class="footnote-ref" href="#research-0_definition-fn:maynezFaithfulnessFactualityAbstractive2020">5</a></sup> (~900 citations)</p>
<h3 id="research-0_definition-definition">Definition<a class="headerlink" href="#research-0_definition-definition" title="Permanent link">&para;</a></h3>
<p><strong>Hallucinations</strong> can be defined in the following way: "hallucination is typically referred to as a phenomenon in which the generated content appears nonsensical <em>(=! untruthful)</em> or unfaithful to the provided source content" <sup id="fnref2:maynezFaithfulnessFactualityAbstractive2020"><a class="footnote-ref" href="#research-0_definition-fn:maynezFaithfulnessFactualityAbstractive2020">5</a></sup> </p>
<h3 id="research-0_definition-faithfulness-and-factuality">Faithfulness and Factuality<a class="headerlink" href="#research-0_definition-faithfulness-and-factuality" title="Permanent link">&para;</a></h3>
<p><strong>faithfulness</strong>: "a faithful model will generate a summary that only has information that is supported by its document <sup id="fnref3:maynezFaithfulnessFactualityAbstractive2020"><a class="footnote-ref" href="#research-0_definition-fn:maynezFaithfulnessFactualityAbstractive2020">5</a></sup> ."</p>
<p><strong>Factuality</strong>: "A summary S of a document D contains a factual hallucination if it contains information not found in D that is factually correct. Factual hallucinations may be composed of intrinsic hallucinations or extrinsic hallucinations" <sup id="fnref4:maynezFaithfulnessFactualityAbstractive2020"><a class="footnote-ref" href="#research-0_definition-fn:maynezFaithfulnessFactualityAbstractive2020">5</a></sup> </p>
<p>Faithfulness and Factuality are independent from one another.</p>
<p>==What about Truthfulness==</p>
<h4 id="research-0_definition-examples">Examples :<a class="headerlink" href="#research-0_definition-examples" title="Permanent link">&para;</a></h4>
<p>Doc: "In the latest Microsoft keynote, windows 98 has just been released" 
The summary "windows 11 has just been released" is unfaithful but factual</p>
<p>==The summary "windows 98 has been released by Microsoft" is unfaithful but factual== adding external implicit knowledge leads to unfaithful content, here we dont know that microsoft is the company realising windows as it is not clear by the context. The statement is nonetheless factual</p>
<p>The summary "windows 98 has just been released" is faithful but untruthful (outdated)</p>
<h3 id="research-0_definition-intrinsic-vs-extrinsic">Intrinsic vs Extrinsic<a class="headerlink" href="#research-0_definition-intrinsic-vs-extrinsic" title="Permanent link">&para;</a></h3>
<p>in the context of summarisation, <strong>intrinsic</strong> hallucination are miss-representation of the source document while <strong>extrinsic</strong> hallucinations, are the generation of facts that are not present in the input document</p>
<p>These two concept are mutually exclusive.</p>
<p>Doc: "French prime minister just used the 49.3 to pass the budget" 
The summary "Dutch prime minister just used the 49.3 to pass the budget" is an intrinsic hallucination
The summary "The prime minister is very unpopular due to its use of article 49.3"  is an extrinsic hallucination</p>
<h3 id="research-0_definition-the-point-for-hallucinations">The point for hallucinations:<a class="headerlink" href="#research-0_definition-the-point-for-hallucinations" title="Permanent link">&para;</a></h3>
<p>Hallucination != bad : </p>
<p>“hallucinations in summarisation are acceptable if they lead to better summaries that are factual with respect to the document and the associated background knowledge."</p>
<h3 id="research-0_definition-problem-specific-definition">Problem specific definition<a class="headerlink" href="#research-0_definition-problem-specific-definition" title="Permanent link">&para;</a></h3>
<p>Add task specific definition</p>
<h3 id="research-0_definition-interrogations">Interrogations<a class="headerlink" href="#research-0_definition-interrogations" title="Permanent link">&para;</a></h3>
<p>At which point do we consider that two things are the same ? </p>
<p>"Earth is a Round" =&gt; "Earth is a Sphere" ? this implication is considered an extrinsic hallucination by my definition</p>
<p>How do we define factuality ?</p>
<p>an alternative definition of the problem from Dong et al. <sup id="fnref2:dongMultiFactCorrectionAbstractive2020"><a class="footnote-ref" href="#research-0_definition-fn:dongMultiFactCorrectionAbstractive2020">4</a></sup> (where facts are interpreted as the input context, making faithfulness == factuality)</p>
<div class="footnote">
<hr />
<ol>
<li id="research-0_definition-fn:farquharDetectingHallucinationsLarge2024">
<p>Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. <em>Nature</em>, 630(8017):625–630, June 2024. <a href="https://doi.org/10.1038/s41586-024-07421-0">doi:10.1038/s41586-024-07421-0</a>.&#160;<a class="footnote-backref" href="#research-0_definition-fnref:farquharDetectingHallucinationsLarge2024" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="research-0_definition-fn:filippovaControlledHallucinationsLearning2020">
<p>Katja Filippova. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. In <em>Findings of the Association for Computational Linguistics: EMNLP 2020</em>, 864–870. Online, 2020. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.findings-emnlp.76">doi:10.18653/v1/2020.findings-emnlp.76</a>.&#160;<a class="footnote-backref" href="#research-0_definition-fnref:filippovaControlledHallucinationsLearning2020" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="research-0_definition-fn:jiSurveyHallucinationNatural2023">
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of Hallucination in Natural Language Generation. <em>ACM Comput. Surv.</em>, 55(12):248:1–248:38, March 2023. <a href="https://doi.org/10.1145/3571730">doi:10.1145/3571730</a>.&#160;<a class="footnote-backref" href="#research-0_definition-fnref:jiSurveyHallucinationNatural2023" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="research-0_definition-fn:dongMultiFactCorrectionAbstractive2020">
<p>Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, and Jingjing Liu. Multi-Fact Correction in Abstractive Text Summarization. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 9320–9331. Online, November 2020. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.749">doi:10.18653/v1/2020.emnlp-main.749</a>.&#160;<a class="footnote-backref" href="#research-0_definition-fnref:dongMultiFactCorrectionAbstractive2020" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#research-0_definition-fnref2:dongMultiFactCorrectionAbstractive2020" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="research-0_definition-fn:maynezFaithfulnessFactualityAbstractive2020">
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On Faithfulness and Factuality in Abstractive Summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 1906–1919. Online, July 2020. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.173">doi:10.18653/v1/2020.acl-main.173</a>.&#160;<a class="footnote-backref" href="#research-0_definition-fnref:maynezFaithfulnessFactualityAbstractive2020" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#research-0_definition-fnref2:maynezFaithfulnessFactualityAbstractive2020" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#research-0_definition-fnref3:maynezFaithfulnessFactualityAbstractive2020" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#research-0_definition-fnref4:maynezFaithfulnessFactualityAbstractive2020" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
</ol>
</div></section><h1 class='nav-section-title-end'>Ended: 0 definition</h1>
                        <h2 class='nav-section-title' id='section-1-classification'>
                            1 classification <a class='headerlink' href='#section-1-classification' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="research-1_classification"><h1 id="research-1_classification-index">Index</h1><h2 id="hallucination-classification">Hallucination classification<a class="headerlink" href="#research-1_classification-hallucination-classification" title="Permanent link">&para;</a></h2>
<p>Reference :  <a href="https://arxiv.org/abs/2311.05232">A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</a></p>
<h3 id="research-1_classification-definition">Definition<a class="headerlink" href="#research-1_classification-definition" title="Permanent link">&para;</a></h3>
<p>Hallucinations in large language model are in some sense analogous to what they mean in the context of a human being, but there are some nuances. The oxford dictionary define them as "<em>an experience involving the apparent perception of something not present</em>", in the context of NLP, <em>hallucination</em> refers to a phenomena where the generated content appears to be non sensical or unfaithful to the provided context. This materialise in a way that is sort of similar to human hallucination but this, as if the model was dreaming.</p>
<p>We distinguish two kind of hallucination: intrinsic and extrinsic (unfaithful and untruthful).</p>
<h4 id="research-1_classification-faithfulness-hallucination">Faithfulness hallucination<a class="headerlink" href="#research-1_classification-faithfulness-hallucination" title="Permanent link">&para;</a></h4>
<p>Faithfulness hallucinations happen when the model fails to follow or conflict with the provided context when generate new token.</p>
<h4 id="research-1_classification-truthfulness-hallucination">Truthfulness hallucination<a class="headerlink" href="#research-1_classification-truthfulness-hallucination" title="Permanent link">&para;</a></h4>
<p>Truthfulness hallucinations happen when the model statements cannot be verified in the context or a knowledge base.</p>
<p><img alt="Pasted image 20250124112926.png" src="../research/1_classification/attachments/Pasted%20image%2020250124112926.png" title="Pasted image 20250124112926.png" /></p></section><h1 class='nav-section-title-end'>Ended: 1 classification</h1>
                        <h2 class='nav-section-title' id='section-2-detection'>
                            2 detection <a class='headerlink' href='#section-2-detection' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="research-2_detection"><h1 id="research-2_detection-index">Index</h1><p>Based on the following meta study <sup id="fnref:huangSurveyHallucinationLarge2024"><a class="footnote-ref" href="#research-2_detection-fn:huangSurveyHallucinationLarge2024">1</a></sup>, what are the methods for detecting hallucinations for a model.</p>
<h2 id="research-2_detection-detection">Detection<a class="headerlink" href="#research-2_detection-detection" title="Permanent link">&para;</a></h2>
<h3 id="research-2_detection-detection-strategies-taxonomy">Detection strategies taxonomy<a class="headerlink" href="#research-2_detection-detection-strategies-taxonomy" title="Permanent link">&para;</a></h3>
<ul>
<li>Fact checking<ul>
<li>External retrieval</li>
<li>Internal retrieval</li>
</ul>
</li>
<li>Incertitude quantification<ul>
<li>LLM's internal states</li>
<li>LLM's behaviour</li>
</ul>
</li>
</ul>
<h4 id="research-2_detection-external-retrieval">External retrieval<a class="headerlink" href="#research-2_detection-external-retrieval" title="Permanent link">&para;</a></h4>
<p><em>Using an external knowledge base to compare with the model facts</em></p>
<p>Example with <strong>FActScore</strong> <sup id="fnref:minFActScoreFinegrainedAtomic2023"><a class="footnote-ref" href="#research-2_detection-fn:minFActScoreFinegrainedAtomic2023">2</a></sup></p>
<p><img alt="452x367" src="../research/2_detection/attachments/Pasted%20image%2020250125220307.png" title="452x367" /></p>
<ul>
<li>Step 1: Model generates biographies of public figures</li>
<li>Step 2: Leverage InstructGPT to turn biographies into atomic facts</li>
<li>Step 3: Use instruct Llama to label the atomic fact (“irrelevant”, ”supported”, “not supported”) based on a retrieved (Generalizable T5-based Retrievers) knowledge base (wikipedia)</li>
<li>Step 4: Evaluate the proportion of fact that are supported</li>
</ul>
<h4 id="research-2_detection-internal-retrieval">Internal retrieval<a class="headerlink" href="#research-2_detection-internal-retrieval" title="Permanent link">&para;</a></h4>
<p><em>Use the model to check its own replies</em></p>
<p>Example: <strong>Chain-of-Verification (CoVe)</strong> <sup id="fnref:dhuliawalaChainofVerificationReducesHallucination2023"><a class="footnote-ref" href="#research-2_detection-fn:dhuliawalaChainofVerificationReducesHallucination2023">3</a></sup></p>
<p>Prompt the model to:
1. Generate a draft
2. Plan verification questions (not templated)
3. Answer verification questions
4. Generate a final response</p>
<p><img alt="523x370" src="../research/2_detection/attachments/Pasted%20image%2020250127160439.png" title="523x370" /></p>
<h4 id="research-2_detection-internal-states">Internal states<a class="headerlink" href="#research-2_detection-internal-states" title="Permanent link">&para;</a></h4>
<p>Leveraging LLM's internal states such as the entropy of a given token or the perplexity of a sentence to infer on the hallucination state.</p>
<p>Example of an end to end implementation :  <em>A Stitch in Time Saves Nine</em>: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation <sup id="fnref:varshneyStitchTimeSaves2023"><a class="footnote-ref" href="#research-2_detection-fn:varshneyStitchTimeSaves2023">4</a></sup></p>
<ol>
<li>Generating sentense tokens</li>
<li>Key concept identification (using AI)</li>
<li>Uncertainty quantification (Using the lowest probability of any token in a given concept)</li>
<li>Generating validation questions (using AI)</li>
<li>Knowledge retrieval (external)</li>
<li>Question answering</li>
<li>Sentence fix</li>
</ol>
<p>Authors achieved a recall of 88% and a mitigation rate of 56%
The false positive didn't have any impact on performances.</p>
<p><img alt="453x302" src="../research/2_detection/attachments/Pasted%20image%2020250127161356.png" title="453x302" /></p>
<h4 id="research-2_detection-behavioural-approach">Behavioural approach<a class="headerlink" href="#research-2_detection-behavioural-approach" title="Permanent link">&para;</a></h4>
<p>Use behavioural tools to implement fact verification approaches. This set of tools is usually used when no other technique is applicable (for instance, when the API doesn't give access to token probability).</p>
<p>SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models <sup id="fnref:manakulSelfCheckGPTZeroResourceBlackBox2023"><a class="footnote-ref" href="#research-2_detection-fn:manakulSelfCheckGPTZeroResourceBlackBox2023">5</a></sup></p>
<p>Uses multi sampling techniques.</p>
<p>Relies on the idea that if an LLM has been trained on a concept, the sampled ideas should be similar and contain consistent facts.</p>
<ol>
<li>multi sampling</li>
<li>Use LLMs to check the consistency between prompts</li>
<li>Use the results to assert the probability of the sequence to be hallucinated</li>
</ol>
<p><img alt="450x366" src="../research/2_detection/attachments/Pasted%20image%2020250127164920.png" title="450x366" /></p>
<div class="footnote">
<hr />
<ol>
<li id="research-2_detection-fn:huangSurveyHallucinationLarge2024">
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. <em>ACM Transactions on Information Systems</em>, pages 3703155, November 2024. <a href="https://arxiv.org/abs/2311.05232">arXiv:2311.05232</a>, <a href="https://doi.org/10.1145/3703155">doi:10.1145/3703155</a>.&#160;<a class="footnote-backref" href="#research-2_detection-fnref:huangSurveyHallucinationLarge2024" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="research-2_detection-fn:minFActScoreFinegrainedAtomic2023">
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. October 2023. <a href="https://arxiv.org/abs/2305.14251">arXiv:2305.14251</a>, <a href="https://doi.org/10.48550/arXiv.2305.14251">doi:10.48550/arXiv.2305.14251</a>.&#160;<a class="footnote-backref" href="#research-2_detection-fnref:minFActScoreFinegrainedAtomic2023" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="research-2_detection-fn:dhuliawalaChainofVerificationReducesHallucination2023">
<p>Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-Verification Reduces Hallucination in Large Language Models. September 2023. <a href="https://arxiv.org/abs/2309.11495">arXiv:2309.11495</a>, <a href="https://doi.org/10.48550/arXiv.2309.11495">doi:10.48550/arXiv.2309.11495</a>.&#160;<a class="footnote-backref" href="#research-2_detection-fnref:dhuliawalaChainofVerificationReducesHallucination2023" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="research-2_detection-fn:varshneyStitchTimeSaves2023">
<p>Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. August 2023. <a href="https://arxiv.org/abs/2307.03987">arXiv:2307.03987</a>, <a href="https://doi.org/10.48550/arXiv.2307.03987">doi:10.48550/arXiv.2307.03987</a>.&#160;<a class="footnote-backref" href="#research-2_detection-fnref:varshneyStitchTimeSaves2023" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="research-2_detection-fn:manakulSelfCheckGPTZeroResourceBlackBox2023">
<p>Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. October 2023. <a href="https://arxiv.org/abs/2303.08896">arXiv:2303.08896</a>, <a href="https://doi.org/10.48550/arXiv.2303.08896">doi:10.48550/arXiv.2303.08896</a>.&#160;<a class="footnote-backref" href="#research-2_detection-fnref:manakulSelfCheckGPTZeroResourceBlackBox2023" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
</ol>
</div></section><h1 class='nav-section-title-end'>Ended: 2 detection</h1>
                        <h2 class='nav-section-title' id='section-3-benchmarks'>
                            3 benchmarks <a class='headerlink' href='#section-3-benchmarks' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="research-3_benchmarks"><h1 id="research-3_benchmarks-index">Index</h1><h2 id="benchmarks">Benchmarks<a class="headerlink" href="#research-3_benchmarks-benchmarks" title="Permanent link">&para;</a></h2>
<p>Most of the existing benchmark relies on extensive dataset and simple metrics.</p>
<p>One recent approach  from Hugging Face <sup id="fnref:huangSurveyHallucinationLarge2024"><a class="footnote-ref" href="#research-3_benchmarks-fn:huangSurveyHallucinationLarge2024">1</a></sup> has been to aggregate those benchmark into a more <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/">unified leaderboard</a>.</p>
<p>We will discuss individual benchmarks and their functioning in this page. </p>
<h3 id="research-3_benchmarks-truthfulness-benchmarks">Truthfulness Benchmarks<a class="headerlink" href="#research-3_benchmarks-truthfulness-benchmarks" title="Permanent link">&para;</a></h3>
<h4 id="research-3_benchmarks-closed-book-qa">Closed book QA:<a class="headerlink" href="#research-3_benchmarks-closed-book-qa" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>NQ open</strong>, Google, 2019: derived from Natural Questions <sup id="fnref:kwiatkowskiNaturalQuestionsBenchmark2019"><a class="footnote-ref" href="#research-3_benchmarks-fn:kwiatkowskiNaturalQuestionsBenchmark2019">2</a></sup> the first large publicly available data set to pair real (google) user queries with high-quality annotations of answers in (wikipedia) documents.
  The dataset is composed of quadruples question-evidence-longAnswer-shortAnswer. Scores on the benchmark at the time present a high upper bound, with good human performance.
  End up being a leaderboard where RAG model perform best, but most haven't been tested.</p>
</li>
<li>
<p><strong>TriviaQA</strong> <sup id="fnref:joshiTriviaQALargeScale2017"><a class="footnote-ref" href="#research-3_benchmarks-fn:joshiTriviaQALargeScale2017">3</a></sup>, University of Washington (+ a non profit founded by a Microsoft co-founder), 2017: Triples question-excerpt-answer with rich syntax  and complexe phrasing of non important/relatively funny facts (taken in part from online Trivia Quizz).
  The issue with this dataset compared to NQ is that those facts are generally niche/useless and the metrics on this benchmark are less relevant to everyday tasks.</p>
</li>
<li>
<p><strong>PopQA</strong> <sup id="fnref:mallenWhenNotTrust2023"><a class="footnote-ref" href="#research-3_benchmarks-fn:mallenWhenNotTrust2023">4</a></sup>, University of Washington (+ a non profit founded by a Microsoft co-founder), ACL 2023.
  The goal of the study was to highlight the need for RAG when using long tail knowledge (rarely occurring information within the training data) in comparison with popular knowledge (Pop in PopQA).
  PopQA is supposed to cover such long tail knowledge. Wikipedia views are used to determine how popular is a question.
  The study highlights 3 main findings: </p>
<ul>
<li>With those dataset, scaling up models does not significantly improve the performance.</li>
<li>Retrieval-augmented LMs are particularly competitive when subject entities are not popular. Surprisingly, retrieval augmentation can hurt the performance of  LLMs on questions about popular entities as the retrieved context can be misleading."</li>
<li>We can chose wether or not to use RAG based on their provided popularity metric, increasing performances on PopQA by up to 10%.</li>
</ul>
</li>
<li>
<p><strong>TruthfulQA</strong> <sup id="fnref:linTruthfulQAMeasuringHow2022"><a class="footnote-ref" href="#research-3_benchmarks-fn:linTruthfulQAMeasuringHow2022">5</a></sup>, University of Oxford/Open AI 2022 : benchmark composed of question that some human would answer falsely due to false beliefs or misconceptions. Judges the ability of a model to avoid replicating human misconceptions. 
  The performance difference between audited human and best model is 94% vs 58% respectively.
  Interesting findings: <strong>the largest model were generally the least truthful</strong> and fine tuning is a better approach to enhance performances rather than learning more text coming from internet.</p>
</li>
</ul>
<p>Missing: SimpleQA (openAI)</p>
<h4 id="research-3_benchmarks-fact-checking">Fact-Checking<a class="headerlink" href="#research-3_benchmarks-fact-checking" title="Permanent link">&para;</a></h4>
<ul>
<li>FEVER <sup id="fnref:thorneFEVERLargescaleDataset2018"><a class="footnote-ref" href="#research-3_benchmarks-fn:thorneFEVERLargescaleDataset2018">6</a></sup>, University of Sheffield, Amazon, 2018: Fact Extraction and VERification. Taking claims from wikipedia passages and verifying wether or not the claims are supported by the document (using human annotation).  Challenging test as the model successfully achieved only 32%.
  The reliance on human annotation make it so it is hard to really evaluate the inherent quality of the dataset (most humans agree on 70% of the labels).</li>
</ul>
<h4 id="research-3_benchmarks-hallucination-detection">Hallucination detection<a class="headerlink" href="#research-3_benchmarks-hallucination-detection" title="Permanent link">&para;</a></h4>
<ul>
<li>True-False <sup id="fnref:azariaInternalStateLLM2023"><a class="footnote-ref" href="#research-3_benchmarks-fn:azariaInternalStateLLM2023">7</a></sup> ACL 2023: based on a set of sentences and the LLM internal states as it predicts the sentence, train a classifier that output the probability of a statements truthfulness. Classifier achieve 71~83% accuracy based on the model.</li>
</ul>
<h3 id="research-3_benchmarks-faithfulness-benchmarks">Faithfulness Benchmarks<a class="headerlink" href="#research-3_benchmarks-faithfulness-benchmarks" title="Permanent link">&para;</a></h3>
<h4 id="research-3_benchmarks-summarisation">Summarisation<a class="headerlink" href="#research-3_benchmarks-summarisation" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>XSum <sup id="fnref:narayanDontGiveMe2018"><a class="footnote-ref" href="#research-3_benchmarks-fn:narayanDontGiveMe2018">8</a></sup> University of Edinburgh, 2018: Requires the model to resonate on the document instead of simply extracting facts.
  Since it was written in 2018, it propose a CNN to work on the task associated with the Dataset.
  The dataset is composed of article from the BBC and single sentence summaries. The summary sentence is typically written by the author of the article.
  This benchmark can be used to evaluate how well a model perform on long context comprehension <sup id="research-3_benchmarks-fnref:gaoEmpowerYourModel2023"><a class="footnote-ref" href="#research-3_benchmarks-fn:gaoEmpowerYourModel2023">9</a></sup>. It is also used in the paper defining hallucination in a modern way <sup id="fnref:maynezFaithfulnessFactualityAbstractive2020"><a class="footnote-ref" href="#research-3_benchmarks-fn:maynezFaithfulnessFactualityAbstractive2020">10</a></sup>.</p>
</li>
<li>
<p>CNN/Daily Mail <sup id="fnref:hermannTeachingMachinesRead2015"><a class="footnote-ref" href="#research-3_benchmarks-fn:hermannTeachingMachinesRead2015">11</a></sup> <sup id="fnref:chenThoroughExaminationCNN2016"><a class="footnote-ref" href="#research-3_benchmarks-fn:chenThoroughExaminationCNN2016">12</a></sup> , Google, NeurIPS 2015, Stanford, 2016 :
  Proposing a supervised learning dataset for reading comprehension. The dataset is structured in context-query-answer triples.
  1M corpus &amp; queries.
  The author tackle the issue with an  attention/LSTM model, unsuccessfully so.
  The next paper ups the accuracy from. ~7% to ~70%. It is a nice complement to the first paper as they dive much deeper into the dataset.
  (metric: ROUGE-L, which assesses n-gram overlap with reference summaries)
  ==SHOULDN'T THIS BE IN READING COMPREHENSION== ?</p>
</li>
<li>
<p>NQ-Swap (derived from Natural Questions, exact match)</p>
</li>
</ul>
<h4 id="research-3_benchmarks-reading-comprehension">Reading comprehension<a class="headerlink" href="#research-3_benchmarks-reading-comprehension" title="Permanent link">&para;</a></h4>
<ul>
<li>RACE (accuracy)</li>
<li>SQuAD 2.0 (Exact Match)</li>
</ul>
<h4 id="research-3_benchmarks-instruction-following">Instruction following<a class="headerlink" href="#research-3_benchmarks-instruction-following" title="Permanent link">&para;</a></h4>
<ul>
<li>MemoTrap</li>
<li>IFEval</li>
</ul>
<h4 id="research-3_benchmarks-hallucination-detection_1">Hallucination Detection<a class="headerlink" href="#research-3_benchmarks-hallucination-detection_1" title="Permanent link">&para;</a></h4>
<ul>
<li>FaithDial</li>
<li>HaluEval</li>
<li>HotpotQA</li>
</ul>
<p>Models are evaluated by accuracy</p>
<p>Insights:
- LLM size has beneficial impact on reducing faithfulness and truthfulness hallucination. It has to be underlined that truthfulness benefits from a better uplift.</p>
<p>Fallback:
- these benchmark don't optimise the prompt for each individual LLM, meaning that the comparison might be unfair and benefits some LLMs better than others
- Benchmarks come and go in terms of popularity and it is hard to truly assess the progress of LLMs over time as they are rarely compared to the same metrics (MMLU aside)
- some benchmarks are classified as closed book when they are developed as open book. others are classified as summarisation benchmark when they are reading comprehension, indicating a blurry line between these subjects
- Some problematic about benchmark leakage should be taken in consideration <sup id="research-3_benchmarks-fnref:xuBenchmarkingBenchmarkLeakage2024"><a class="footnote-ref" href="#research-3_benchmarks-fn:xuBenchmarkingBenchmarkLeakage2024">13</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="research-3_benchmarks-fn:huangSurveyHallucinationLarge2024">
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. <em>ACM Transactions on Information Systems</em>, pages 3703155, November 2024. <a href="https://arxiv.org/abs/2311.05232">arXiv:2311.05232</a>, <a href="https://doi.org/10.1145/3703155">doi:10.1145/3703155</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:huangSurveyHallucinationLarge2024" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:kwiatkowskiNaturalQuestionsBenchmark2019">
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A Benchmark for Question Answering Research. <em>Transactions of the Association for Computational Linguistics</em>, 7:453–466, August 2019. <a href="https://doi.org/10.1162/tacl_a_00276">doi:10.1162/tacl_a_00276</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:kwiatkowskiNaturalQuestionsBenchmark2019" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:joshiTriviaQALargeScale2017">
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. May 2017. <a href="https://arxiv.org/abs/1705.03551">arXiv:1705.03551</a>, <a href="https://doi.org/10.48550/arXiv.1705.03551">doi:10.48550/arXiv.1705.03551</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:joshiTriviaQALargeScale2017" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:mallenWhenNotTrust2023">
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. July 2023. <a href="https://arxiv.org/abs/2212.10511">arXiv:2212.10511</a>, <a href="https://doi.org/10.48550/arXiv.2212.10511">doi:10.48550/arXiv.2212.10511</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:mallenWhenNotTrust2023" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:linTruthfulQAMeasuringHow2022">
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring How Models Mimic Human Falsehoods. May 2022. <a href="https://arxiv.org/abs/2109.07958">arXiv:2109.07958</a>, <a href="https://doi.org/10.48550/arXiv.2109.07958">doi:10.48550/arXiv.2109.07958</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:linTruthfulQAMeasuringHow2022" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:thorneFEVERLargescaleDataset2018">
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for Fact Extraction and VERification. December 2018. <a href="https://arxiv.org/abs/1803.05355">arXiv:1803.05355</a>, <a href="https://doi.org/10.48550/arXiv.1803.05355">doi:10.48550/arXiv.1803.05355</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:thorneFEVERLargescaleDataset2018" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:azariaInternalStateLLM2023">
<p>Amos Azaria and Tom Mitchell. The Internal State of an LLM Knows When It's Lying. October 2023. <a href="https://arxiv.org/abs/2304.13734">arXiv:2304.13734</a>, <a href="https://doi.org/10.48550/arXiv.2304.13734">doi:10.48550/arXiv.2304.13734</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:azariaInternalStateLLM2023" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:narayanDontGiveMe2018">
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. August 2018. <a href="https://arxiv.org/abs/1808.08745">arXiv:1808.08745</a>, <a href="https://doi.org/10.48550/arXiv.1808.08745">doi:10.48550/arXiv.1808.08745</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:narayanDontGiveMe2018" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:gaoEmpowerYourModel2023">
<p>Yifei Gao, Lei Wang, Jun Fang, Longhua Hu, and Jun Cheng. Empower Your Model with Longer and Better Context Comprehension. July 2023. <a href="https://arxiv.org/abs/2307.13365">arXiv:2307.13365</a>, <a href="https://doi.org/10.48550/arXiv.2307.13365">doi:10.48550/arXiv.2307.13365</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:gaoEmpowerYourModel2023" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:maynezFaithfulnessFactualityAbstractive2020">
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On Faithfulness and Factuality in Abstractive Summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 1906–1919. Online, July 2020. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.173">doi:10.18653/v1/2020.acl-main.173</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:maynezFaithfulnessFactualityAbstractive2020" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:hermannTeachingMachinesRead2015">
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching Machines to Read and Comprehend. In <em>Advances in Neural Information Processing Systems</em>, volume 28. Curran Associates, Inc., 2015.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:hermannTeachingMachinesRead2015" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:chenThoroughExaminationCNN2016">
<p>Danqi Chen, Jason Bolton, and Christopher D. Manning. A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task. August 2016. <a href="https://arxiv.org/abs/1606.02858">arXiv:1606.02858</a>, <a href="https://doi.org/10.48550/arXiv.1606.02858">doi:10.48550/arXiv.1606.02858</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:chenThoroughExaminationCNN2016" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="research-3_benchmarks-fn:xuBenchmarkingBenchmarkLeakage2024">
<p>Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking Benchmark Leakage in Large Language Models. April 2024. <a href="https://arxiv.org/abs/2404.18824">arXiv:2404.18824</a>, <a href="https://doi.org/10.48550/arXiv.2404.18824">doi:10.48550/arXiv.2404.18824</a>.&#160;<a class="footnote-backref" href="#research-3_benchmarks-fnref:xuBenchmarkingBenchmarkLeakage2024" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
</ol>
</div></section><h1 class='nav-section-title-end'>Ended: 3 benchmarks</h1>
                        <h2 class='nav-section-title' id='section-misc'>
                            Misc <a class='headerlink' href='#section-misc' title='Permanent link'>↵</a>
                        </h2>
                        <section class="print-page" id="research-misc-deepseek"><h1 id="research-misc-deepseek-what-makes-deepseek-v3-r1-so-special">What makes DeepSeek V3 / R1 so ✨ special ✨<a class="headerlink" href="#research-misc-deepseek-what-makes-deepseek-v3-r1-so-special" title="Permanent link">&para;</a></h1>
<p>The breakthroughs are taking place over the year 2024, through the release of the V2, V3 and R1 papers.</p>
<h2 id="research-misc-deepseek-v2">V2<a class="headerlink" href="#research-misc-deepseek-v2" title="Permanent link">&para;</a></h2>
<p>The <a href="https://arxiv.org/abs/2405.04434">V2 paper</a> introduce the architectural changes, in particular they leverage 2 powerful changes : a house baked Mixture of Expert implementation (deepSeekMoE) and Multi Head Latent Attention (MHA) (DeepSeek developed as well).</p>
<p>Those two changes intervene on two distinct layers of the transformer block:
- MoE takes place in the FFN
- MHA takes place in the attention block</p>
<p>Architecture : </p>
<p><img alt="Pasted image 20250128161241.png" src="../research/misc/attachments/Pasted%20image%2020250128161241.png" title="Pasted image 20250128161241.png" /></p>
<ol>
<li>Multi head latent attention
    Low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference.
    Queries are also cached on the v3 paper</li>
<li>MoE
    Cross node MoE training - optimisation to significantly enhance training efficiency.
    Basically there are a set of <strong>shared</strong> experts that are active at all stages, as well as <strong>routed</strong> experts that can be routed on any given tokens.</li>
</ol>
<h4 id="research-misc-deepseek-multi-head-latent-attention">Multi head latent attention<a class="headerlink" href="#research-misc-deepseek-multi-head-latent-attention" title="Permanent link">&para;</a></h4>
<p>While Llama3 uses Grouped-querry, attention</p>
<p><img alt="Pasted image 20250128170832.png" src="../research/misc/attachments/Pasted%20image%2020250128170832.png" title="Pasted image 20250128170832.png" /></p>
<p>This leads to a compressed space that caches into 5% of the original size (20x saving in memory)</p>
<p>In the attention layer, MLHA is implemented, where they project key, values but ALSO queries (as NOT shown in the following graph).</p>
<p>This technique seems to be very close to <a href="https://arxiv.org/abs/2106.09685">LoRA</a>.</p>
<h4 id="research-misc-deepseek-deepseekmoe">DeepSeekMoE<a class="headerlink" href="#research-misc-deepseek-deepseekmoe" title="Permanent link">&para;</a></h4>
<p>Then you have the DeepSeekMoE architecture, with use fine(er)-grained experts and isolate/group them.</p>
<p><img alt="Pasted image 20250128175719.png" src="../research/misc/attachments/Pasted%20image%2020250128175719.png" title="Pasted image 20250128175719.png" /></p>
<p>The innovation here seems to be the fact that we have some shared expert (which are all activated at every token) and routed experts, which are selected at every token by the router.</p>
<p>These changes, when implemented together, provide a significan bump in performance, as is shown by deepseek v2 paper:</p>
<p><img alt="Pasted image 20250130173714.png" src="../research/misc/attachments/Pasted%20image%2020250130173714.png" title="Pasted image 20250130173714.png" /></p>
<p>Some insights about these graphs, Flash Attention was (is) a very big deal when it came out and had throughput benefits of 5 to 9 folds, this architecture gives us a speed boost of 5 fold with the added benefits of saving 93.3% of the KV cache on top of saving for the training cost.</p>
<p>Here is a graph showing the speedup of FlashAttention</p>
<p><img alt="Pasted image 20250129110228.png" src="../research/misc/attachments/Pasted%20image%2020250129110228.png" title="Pasted image 20250129110228.png" /></p>
<p>Some theory (TO BE CHECKED): FlashAttention is basically bypassing the old way to compute attention, a low level optimisation if you will. There have been rumours about similar practice from DeepSeek where they used PTX to rewrite some functions for optimisation.</p>
<p><a href="https://x.com/Jukanlosreve/status/1883304958432624881">Source</a>, <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead">Source 2</a></p>
<h2 id="research-misc-deepseek-v3">V3<a class="headerlink" href="#research-misc-deepseek-v3" title="Permanent link">&para;</a></h2>
<p>V3 takes the ideas from v2, add some new pipeline tweaks and scales the model to get new performance. The <a href="https://arxiv.org/abs/2412.19437">V3 paper</a> detail the following changes: </p>
<ol>
<li>Parameter and Data Scaling:
    236B to 671B parameters (21 to 37B active at every token)
    14.8T token for pre-training, very stable
    1.5M samples for fine tuning</li>
<li>Auxiliary-loss-free strategy for load balancing
     Goal: minimising performance degradation due to load balancing </li>
<li>Multi-token prediction training objective
    Beneficial to model performance + can be used for speculative decoding and inference acceleration</li>
<li>FP8 mixed precision</li>
<li>"DualPipe Algorithm" </li>
<li>New training process:
     (2 stages) Context extension : original -&gt; 32K -&gt; 128K
     Supervised fine tuning over 1.5M examples</li>
</ol>
<p>Impressively cheap cost:</p>
<p><img alt="Pasted image 20250130174008.png" src="../research/misc/attachments/Pasted%20image%2020250130174008.png" title="Pasted image 20250130174008.png" /></p>
<p>Compared to Llama 3 training, this is very impressive:</p>
<p><img alt="Pasted image 20250201210443.png" src="../research/misc/attachments/Pasted%20image%2020250201210443.png" title="Pasted image 20250201210443.png" /></p>
<p><a href="https://apxml.com/posts/training-cost-deepseek-v3-vs-llama-3">Source</a> </p>
<p>Though this doesn't account for ablation studies, data generation cost etc.
Also note that this table is for V3, not R1, CoT reasoning training comes at further cost.</p>
<hr />
<p>The <a href="https://arxiv.org/abs/2501.12948">R1 paper</a> develop 2 training strategies, yielding both R1-Zero and R1</p>
<h2 id="research-misc-deepseek-r1-zero">R1-zero<a class="headerlink" href="#research-misc-deepseek-r1-zero" title="Permanent link">&para;</a></h2>
<p>R1-zero is their RL only approach to CoT reasoning emergence.</p>
<p>Basically they use a lot of complexe reasoning task that are close problems (which have one correct output such as the code compiling etc.) and they just reward the model when it produce an output between the \<think> \</think> markups, as well as the result being correct.</p>
<p>This improved the capabilities quite impressively, reaching performance levels close to o1 
<img alt="Pasted image 20250130173029.png" src="../research/misc/attachments/Pasted%20image%2020250130173029.png" title="Pasted image 20250130173029.png" /></p>
<h2 id="research-misc-deepseek-r1">R1<a class="headerlink" href="#research-misc-deepseek-r1" title="Permanent link">&para;</a></h2>
<p>R1-zero had some limitation which motivated some refinements. The authors claim the output had poor readability and the CoT reasoning suffered from language mixing.</p>
<p>Adding SFT as well as another step of RL in the pipeline to get better behaviour and performance</p>
<p>Training is done in 4 steps:</p>
<ol>
<li>SFT to improve stability instead of cold start</li>
<li>R1-Zero RL pipeline</li>
<li>Data generation through rejection sampling -&gt; SFT</li>
<li>RL for Helpfulness and harmlessness</li>
</ol>
<p>R1 ends up with very solid performance:</p>
<p><img alt="Pasted image 20250130173405.png" src="../research/misc/attachments/Pasted%20image%2020250130173405.png" title="Pasted image 20250130173405.png" /></p>
<p>The R1 base model end up being 27x cheaper than contemporary o1 pricing per million token output</p>
<h3 id="research-misc-deepseek-whats-next">What's next ?<a class="headerlink" href="#research-misc-deepseek-whats-next" title="Permanent link">&para;</a></h3>
<p>Distillation from DeepSeek R1 (chain of thought model)</p>
<p>And the distillations are great too:</p>
<p><img alt="Pasted image 20250130173423.png" src="../research/misc/attachments/Pasted%20image%2020250130173423.png" title="Pasted image 20250130173423.png" /></p>
<p>Another impressive step is that the distillation process enable levels of performance that can't be attained on the RL pipeline of R1-zero only.
<img alt="Pasted image 20250130173520.png" src="../research/misc/attachments/Pasted%20image%2020250130173520.png" title="Pasted image 20250130173520.png" /></p>
<h5 id="research-misc-deepseek-running-r1-on-a-potato">Running R1 on a potato ?<a class="headerlink" href="#research-misc-deepseek-running-r1-on-a-potato" title="Permanent link">&para;</a></h5>
<p>This is a common misconception that has been circulating online</p>
<p>No. R1 is the size of V3, ~670B parameters. But <a href="https://ollama.com/library/deepseek-r1">they offer distilled version</a> on Llama and Qwen.</p>
<p><img alt="Pasted image 20250129120314.png" src="../research/misc/attachments/Pasted%20image%2020250129120314.png" title="Pasted image 20250129120314.png" /></p></section><h1 class='nav-section-title-end'>Ended: Misc</h1><h1 class='nav-section-title-end'>Ended: Research</h1></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "/handbook/", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.indexes", "toc.follow", "content.code.copy", "search.suggest"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../js/print-site.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>