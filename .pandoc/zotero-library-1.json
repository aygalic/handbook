[{"id":"abdinPhi4TechnicalReport2024","abstract":"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size – especially on reasoning-focused benchmarks – due to improved data, training curriculum, and innovations in the post-training scheme.","accessed":{"date-parts":[["2025",2,4]]},"author":[{"family":"Abdin","given":"Marah"},{"family":"Aneja","given":"Jyoti"},{"family":"Behl","given":"Harkirat"},{"family":"Bubeck","given":"Sébastien"},{"family":"Eldan","given":"Ronen"},{"family":"Gunasekar","given":"Suriya"},{"family":"Harrison","given":"Michael"},{"family":"Hewett","given":"Russell J."},{"family":"Javaheripi","given":"Mojan"},{"family":"Kauffmann","given":"Piero"},{"family":"Lee","given":"James R."},{"family":"Lee","given":"Yin Tat"},{"family":"Li","given":"Yuanzhi"},{"family":"Liu","given":"Weishung"},{"family":"Mendes","given":"Caio C. T."},{"family":"Nguyen","given":"Anh"},{"family":"Price","given":"Eric"},{"family":"Rosa","given":"Gustavo","dropping-particle":"de"},{"family":"Saarikivi","given":"Olli"},{"family":"Salim","given":"Adil"},{"family":"Shah","given":"Shital"},{"family":"Wang","given":"Xin"},{"family":"Ward","given":"Rachel"},{"family":"Wu","given":"Yue"},{"family":"Yu","given":"Dingli"},{"family":"Zhang","given":"Cyril"},{"family":"Zhang","given":"Yi"}],"citation-key":"abdinPhi4TechnicalReport2024","DOI":"10.48550/arXiv.2412.08905","issued":{"date-parts":[["2024",12,12]]},"language":"en","number":"arXiv:2412.08905","publisher":"arXiv","source":"arXiv.org","title":"Phi-4 Technical Report","type":"article","URL":"http://arxiv.org/abs/2412.08905"},{"id":"azariaInternalStateLLM2023","abstract":"While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\% to 83\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.","accessed":{"date-parts":[["2025",2,10]]},"author":[{"family":"Azaria","given":"Amos"},{"family":"Mitchell","given":"Tom"}],"citation-key":"azariaInternalStateLLM2023","DOI":"10.48550/arXiv.2304.13734","issued":{"date-parts":[["2023",10,17]]},"number":"arXiv:2304.13734","publisher":"arXiv","source":"arXiv.org","title":"The Internal State of an LLM Knows When It's Lying","type":"article","URL":"http://arxiv.org/abs/2304.13734"},{"id":"behnamghaderLLM2VecLargeLanguage2024","abstract":"Large decoder-only language models (LLMs) are the state-of-the-art models on most of today’s NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"BehnamGhader","given":"Parishad"},{"family":"Adlakha","given":"Vaibhav"},{"family":"Mosbach","given":"Marius"},{"family":"Bahdanau","given":"Dzmitry"},{"family":"Chapados","given":"Nicolas"},{"family":"Reddy","given":"Siva"}],"citation-key":"behnamghaderLLM2VecLargeLanguage2024","DOI":"10.48550/arXiv.2404.05961","issued":{"date-parts":[["2024",8,21]]},"language":"en","number":"arXiv:2404.05961","publisher":"arXiv","source":"arXiv.org","title":"LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders","title-short":"LLM2Vec","type":"article","URL":"http://arxiv.org/abs/2404.05961"},{"id":"chengThinkMoreHallucinate2025","abstract":"Large language models (LLMs) demonstrate exceptional capabilities, yet still face the hallucination issue. Typical text generation approaches adopt an auto-regressive generation without deliberate reasoning, which often results in untrustworthy and factually inaccurate responses. In this paper, we propose HaluSearch, a novel framework that incorporates tree search-based algorithms (e.g., MCTS) to enable an explicit slow thinking generation process for mitigating hallucinations of LLMs during inference. Specifically, HaluSearch frames text generation as a step-by-step reasoning process, using a self-evaluation reward model to score each generation step and guide the tree search towards the most reliable generation pathway for fully exploiting the internal knowledge of LLMs. To balance efficiency and quality, we introduce a hierarchical thinking system switch mechanism inspired by the dual process theory in cognitive science, which dynamically alternates between fast and slow thinking modes at both the instance and step levels, adapting to the complexity of questions and reasoning states. We conduct extensive experiments on both English and Chinese datasets and the results show that our approach significantly outperforms baseline approaches.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Cheng","given":"Xiaoxue"},{"family":"Li","given":"Junyi"},{"family":"Zhao","given":"Wayne Xin"},{"family":"Wen","given":"Ji-Rong"}],"citation-key":"chengThinkMoreHallucinate2025","DOI":"10.48550/arXiv.2501.01306","issued":{"date-parts":[["2025",1,3]]},"language":"en","number":"arXiv:2501.01306","publisher":"arXiv","source":"arXiv.org","title":"Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking","title-short":"Think More, Hallucinate Less","type":"article","URL":"http://arxiv.org/abs/2501.01306"},{"id":"daoFlashAttention2FasterAttention2023","abstract":"Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).","accessed":{"date-parts":[["2025",1,29]]},"author":[{"family":"Dao","given":"Tri"}],"citation-key":"daoFlashAttention2FasterAttention2023","DOI":"10.48550/arXiv.2307.08691","issued":{"date-parts":[["2023",7,17]]},"language":"en","number":"arXiv:2307.08691","publisher":"arXiv","source":"arXiv.org","title":"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning","title-short":"FlashAttention-2","type":"article","URL":"http://arxiv.org/abs/2307.08691"},{"id":"daoFlashAttentionFastMemoryEfficient2022","abstract":"Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading oﬀ model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IOaware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 speedup on GPT-2 (seq. length 1K), and 2.4 speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classiﬁcation) and entirely new capabilities: the ﬁrst Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).","accessed":{"date-parts":[["2025",1,29]]},"author":[{"family":"Dao","given":"Tri"},{"family":"Fu","given":"Daniel Y."},{"family":"Ermon","given":"Stefano"},{"family":"Rudra","given":"Atri"},{"family":"Ré","given":"Christopher"}],"citation-key":"daoFlashAttentionFastMemoryEfficient2022","DOI":"10.48550/arXiv.2205.14135","issued":{"date-parts":[["2022",6,23]]},"language":"en","number":"arXiv:2205.14135","publisher":"arXiv","source":"arXiv.org","title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness","title-short":"FlashAttention","type":"article","URL":"http://arxiv.org/abs/2205.14135"},{"id":"deepseek-aiDeepSeekR1IncentivizingReasoning2025","abstract":"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","accessed":{"date-parts":[["2025",1,29]]},"author":[{"family":"DeepSeek-AI","given":""},{"family":"Guo","given":"Daya"},{"family":"Yang","given":"Dejian"},{"family":"Zhang","given":"Haowei"},{"family":"Song","given":"Junxiao"},{"family":"Zhang","given":"Ruoyu"},{"family":"Xu","given":"Runxin"},{"family":"Zhu","given":"Qihao"},{"family":"Ma","given":"Shirong"},{"family":"Wang","given":"Peiyi"},{"family":"Bi","given":"Xiao"},{"family":"Zhang","given":"Xiaokang"},{"family":"Yu","given":"Xingkai"},{"family":"Wu","given":"Yu"},{"family":"Wu","given":"Z. F."},{"family":"Gou","given":"Zhibin"},{"family":"Shao","given":"Zhihong"},{"family":"Li","given":"Zhuoshu"},{"family":"Gao","given":"Ziyi"},{"family":"Liu","given":"Aixin"},{"family":"Xue","given":"Bing"},{"family":"Wang","given":"Bingxuan"},{"family":"Wu","given":"Bochao"},{"family":"Feng","given":"Bei"},{"family":"Lu","given":"Chengda"},{"family":"Zhao","given":"Chenggang"},{"family":"Deng","given":"Chengqi"},{"family":"Zhang","given":"Chenyu"},{"family":"Ruan","given":"Chong"},{"family":"Dai","given":"Damai"},{"family":"Chen","given":"Deli"},{"family":"Ji","given":"Dongjie"},{"family":"Li","given":"Erhang"},{"family":"Lin","given":"Fangyun"},{"family":"Dai","given":"Fucong"},{"family":"Luo","given":"Fuli"},{"family":"Hao","given":"Guangbo"},{"family":"Chen","given":"Guanting"},{"family":"Li","given":"Guowei"},{"family":"Zhang","given":"H."},{"family":"Bao","given":"Han"},{"family":"Xu","given":"Hanwei"},{"family":"Wang","given":"Haocheng"},{"family":"Ding","given":"Honghui"},{"family":"Xin","given":"Huajian"},{"family":"Gao","given":"Huazuo"},{"family":"Qu","given":"Hui"},{"family":"Li","given":"Hui"},{"family":"Guo","given":"Jianzhong"},{"family":"Li","given":"Jiashi"},{"family":"Wang","given":"Jiawei"},{"family":"Chen","given":"Jingchang"},{"family":"Yuan","given":"Jingyang"},{"family":"Qiu","given":"Junjie"},{"family":"Li","given":"Junlong"},{"family":"Cai","given":"J. L."},{"family":"Ni","given":"Jiaqi"},{"family":"Liang","given":"Jian"},{"family":"Chen","given":"Jin"},{"family":"Dong","given":"Kai"},{"family":"Hu","given":"Kai"},{"family":"Gao","given":"Kaige"},{"family":"Guan","given":"Kang"},{"family":"Huang","given":"Kexin"},{"family":"Yu","given":"Kuai"},{"family":"Wang","given":"Lean"},{"family":"Zhang","given":"Lecong"},{"family":"Zhao","given":"Liang"},{"family":"Wang","given":"Litong"},{"family":"Zhang","given":"Liyue"},{"family":"Xu","given":"Lei"},{"family":"Xia","given":"Leyi"},{"family":"Zhang","given":"Mingchuan"},{"family":"Zhang","given":"Minghua"},{"family":"Tang","given":"Minghui"},{"family":"Li","given":"Meng"},{"family":"Wang","given":"Miaojun"},{"family":"Li","given":"Mingming"},{"family":"Tian","given":"Ning"},{"family":"Huang","given":"Panpan"},{"family":"Zhang","given":"Peng"},{"family":"Wang","given":"Qiancheng"},{"family":"Chen","given":"Qinyu"},{"family":"Du","given":"Qiushi"},{"family":"Ge","given":"Ruiqi"},{"family":"Zhang","given":"Ruisong"},{"family":"Pan","given":"Ruizhe"},{"family":"Wang","given":"Runji"},{"family":"Chen","given":"R. J."},{"family":"Jin","given":"R. L."},{"family":"Chen","given":"Ruyi"},{"family":"Lu","given":"Shanghao"},{"family":"Zhou","given":"Shangyan"},{"family":"Chen","given":"Shanhuang"},{"family":"Ye","given":"Shengfeng"},{"family":"Wang","given":"Shiyu"},{"family":"Yu","given":"Shuiping"},{"family":"Zhou","given":"Shunfeng"},{"family":"Pan","given":"Shuting"},{"family":"Li","given":"S. S."},{"family":"Zhou","given":"Shuang"},{"family":"Wu","given":"Shaoqing"},{"family":"Ye","given":"Shengfeng"},{"family":"Yun","given":"Tao"},{"family":"Pei","given":"Tian"},{"family":"Sun","given":"Tianyu"},{"family":"Wang","given":"T."},{"family":"Zeng","given":"Wangding"},{"family":"Zhao","given":"Wanjia"},{"family":"Liu","given":"Wen"},{"family":"Liang","given":"Wenfeng"},{"family":"Gao","given":"Wenjun"},{"family":"Yu","given":"Wenqin"},{"family":"Zhang","given":"Wentao"},{"family":"Xiao","given":"W. L."},{"family":"An","given":"Wei"},{"family":"Liu","given":"Xiaodong"},{"family":"Wang","given":"Xiaohan"},{"family":"Chen","given":"Xiaokang"},{"family":"Nie","given":"Xiaotao"},{"family":"Cheng","given":"Xin"},{"family":"Liu","given":"Xin"},{"family":"Xie","given":"Xin"},{"family":"Liu","given":"Xingchao"},{"family":"Yang","given":"Xinyu"},{"family":"Li","given":"Xinyuan"},{"family":"Su","given":"Xuecheng"},{"family":"Lin","given":"Xuheng"},{"family":"Li","given":"X. Q."},{"family":"Jin","given":"Xiangyue"},{"family":"Shen","given":"Xiaojin"},{"family":"Chen","given":"Xiaosha"},{"family":"Sun","given":"Xiaowen"},{"family":"Wang","given":"Xiaoxiang"},{"family":"Song","given":"Xinnan"},{"family":"Zhou","given":"Xinyi"},{"family":"Wang","given":"Xianzu"},{"family":"Shan","given":"Xinxia"},{"family":"Li","given":"Y. K."},{"family":"Wang","given":"Y. Q."},{"family":"Wei","given":"Y. X."},{"family":"Zhang","given":"Yang"},{"family":"Xu","given":"Yanhong"},{"family":"Li","given":"Yao"},{"family":"Zhao","given":"Yao"},{"family":"Sun","given":"Yaofeng"},{"family":"Wang","given":"Yaohui"},{"family":"Yu","given":"Yi"},{"family":"Zhang","given":"Yichao"},{"family":"Shi","given":"Yifan"},{"family":"Xiong","given":"Yiliang"},{"family":"He","given":"Ying"},{"family":"Piao","given":"Yishi"},{"family":"Wang","given":"Yisong"},{"family":"Tan","given":"Yixuan"},{"family":"Ma","given":"Yiyang"},{"family":"Liu","given":"Yiyuan"},{"family":"Guo","given":"Yongqiang"},{"family":"Ou","given":"Yuan"},{"family":"Wang","given":"Yuduan"},{"family":"Gong","given":"Yue"},{"family":"Zou","given":"Yuheng"},{"family":"He","given":"Yujia"},{"family":"Xiong","given":"Yunfan"},{"family":"Luo","given":"Yuxiang"},{"family":"You","given":"Yuxiang"},{"family":"Liu","given":"Yuxuan"},{"family":"Zhou","given":"Yuyang"},{"family":"Zhu","given":"Y. X."},{"family":"Xu","given":"Yanhong"},{"family":"Huang","given":"Yanping"},{"family":"Li","given":"Yaohui"},{"family":"Zheng","given":"Yi"},{"family":"Zhu","given":"Yuchen"},{"family":"Ma","given":"Yunxian"},{"family":"Tang","given":"Ying"},{"family":"Zha","given":"Yukun"},{"family":"Yan","given":"Yuting"},{"family":"Ren","given":"Z. Z."},{"family":"Ren","given":"Zehui"},{"family":"Sha","given":"Zhangli"},{"family":"Fu","given":"Zhe"},{"family":"Xu","given":"Zhean"},{"family":"Xie","given":"Zhenda"},{"family":"Zhang","given":"Zhengyan"},{"family":"Hao","given":"Zhewen"},{"family":"Ma","given":"Zhicheng"},{"family":"Yan","given":"Zhigang"},{"family":"Wu","given":"Zhiyu"},{"family":"Gu","given":"Zihui"},{"family":"Zhu","given":"Zijia"},{"family":"Liu","given":"Zijun"},{"family":"Li","given":"Zilin"},{"family":"Xie","given":"Ziwei"},{"family":"Song","given":"Ziyang"},{"family":"Pan","given":"Zizheng"},{"family":"Huang","given":"Zhen"},{"family":"Xu","given":"Zhipeng"},{"family":"Zhang","given":"Zhongyu"},{"family":"Zhang","given":"Zhen"}],"citation-key":"deepseek-aiDeepSeekR1IncentivizingReasoning2025","DOI":"10.48550/arXiv.2501.12948","issued":{"date-parts":[["2025",1,22]]},"language":"en","number":"arXiv:2501.12948","publisher":"arXiv","source":"arXiv.org","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning","title-short":"DeepSeek-R1","type":"article","URL":"http://arxiv.org/abs/2501.12948"},{"id":"deepseek-aiDeepSeekV2StrongEconomical2024","abstract":"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V2.","accessed":{"date-parts":[["2025",1,29]]},"author":[{"family":"DeepSeek-AI","given":""},{"family":"Liu","given":"Aixin"},{"family":"Feng","given":"Bei"},{"family":"Wang","given":"Bin"},{"family":"Wang","given":"Bingxuan"},{"family":"Liu","given":"Bo"},{"family":"Zhao","given":"Chenggang"},{"family":"Dengr","given":"Chengqi"},{"family":"Ruan","given":"Chong"},{"family":"Dai","given":"Damai"},{"family":"Guo","given":"Daya"},{"family":"Yang","given":"Dejian"},{"family":"Chen","given":"Deli"},{"family":"Ji","given":"Dongjie"},{"family":"Li","given":"Erhang"},{"family":"Lin","given":"Fangyun"},{"family":"Luo","given":"Fuli"},{"family":"Hao","given":"Guangbo"},{"family":"Chen","given":"Guanting"},{"family":"Li","given":"Guowei"},{"family":"Zhang","given":"H."},{"family":"Xu","given":"Hanwei"},{"family":"Yang","given":"Hao"},{"family":"Zhang","given":"Haowei"},{"family":"Ding","given":"Honghui"},{"family":"Xin","given":"Huajian"},{"family":"Gao","given":"Huazuo"},{"family":"Li","given":"Hui"},{"family":"Qu","given":"Hui"},{"family":"Cai","given":"J. L."},{"family":"Liang","given":"Jian"},{"family":"Guo","given":"Jianzhong"},{"family":"Ni","given":"Jiaqi"},{"family":"Li","given":"Jiashi"},{"family":"Chen","given":"Jin"},{"family":"Yuan","given":"Jingyang"},{"family":"Qiu","given":"Junjie"},{"family":"Song","given":"Junxiao"},{"family":"Dong","given":"Kai"},{"family":"Gao","given":"Kaige"},{"family":"Guan","given":"Kang"},{"family":"Wang","given":"Lean"},{"family":"Zhang","given":"Lecong"},{"family":"Xu","given":"Lei"},{"family":"Xia","given":"Leyi"},{"family":"Zhao","given":"Liang"},{"family":"Zhang","given":"Liyue"},{"family":"Li","given":"Meng"},{"family":"Wang","given":"Miaojun"},{"family":"Zhang","given":"Mingchuan"},{"family":"Zhang","given":"Minghua"},{"family":"Tang","given":"Minghui"},{"family":"Li","given":"Mingming"},{"family":"Tian","given":"Ning"},{"family":"Huang","given":"Panpan"},{"family":"Wang","given":"Peiyi"},{"family":"Zhang","given":"Peng"},{"family":"Zhu","given":"Qihao"},{"family":"Chen","given":"Qinyu"},{"family":"Du","given":"Qiushi"},{"family":"Chen","given":"R. J."},{"family":"Jin","given":"R. L."},{"family":"Ge","given":"Ruiqi"},{"family":"Pan","given":"Ruizhe"},{"family":"Xu","given":"Runxin"},{"family":"Chen","given":"Ruyi"},{"family":"Li","given":"S. S."},{"family":"Lu","given":"Shanghao"},{"family":"Zhou","given":"Shangyan"},{"family":"Chen","given":"Shanhuang"},{"family":"Wu","given":"Shaoqing"},{"family":"Ye","given":"Shengfeng"},{"family":"Ma","given":"Shirong"},{"family":"Wang","given":"Shiyu"},{"family":"Zhou","given":"Shuang"},{"family":"Yu","given":"Shuiping"},{"family":"Zhou","given":"Shunfeng"},{"family":"Zheng","given":"Size"},{"family":"Wang","given":"T."},{"family":"Pei","given":"Tian"},{"family":"Yuan","given":"Tian"},{"family":"Sun","given":"Tianyu"},{"family":"Xiao","given":"W. L."},{"family":"Zeng","given":"Wangding"},{"family":"An","given":"Wei"},{"family":"Liu","given":"Wen"},{"family":"Liang","given":"Wenfeng"},{"family":"Gao","given":"Wenjun"},{"family":"Zhang","given":"Wentao"},{"family":"Li","given":"X. Q."},{"family":"Jin","given":"Xiangyue"},{"family":"Wang","given":"Xianzu"},{"family":"Bi","given":"Xiao"},{"family":"Liu","given":"Xiaodong"},{"family":"Wang","given":"Xiaohan"},{"family":"Shen","given":"Xiaojin"},{"family":"Chen","given":"Xiaokang"},{"family":"Chen","given":"Xiaosha"},{"family":"Nie","given":"Xiaotao"},{"family":"Sun","given":"Xiaowen"},{"family":"Wang","given":"Xiaoxiang"},{"family":"Liu","given":"Xin"},{"family":"Xie","given":"Xin"},{"family":"Yu","given":"Xingkai"},{"family":"Song","given":"Xinnan"},{"family":"Zhou","given":"Xinyi"},{"family":"Yang","given":"Xinyu"},{"family":"Lu","given":"Xuan"},{"family":"Su","given":"Xuecheng"},{"family":"Wu","given":"Y."},{"family":"Li","given":"Y. K."},{"family":"Wei","given":"Y. X."},{"family":"Zhu","given":"Y. X."},{"family":"Xu","given":"Yanhong"},{"family":"Huang","given":"Yanping"},{"family":"Li","given":"Yao"},{"family":"Zhao","given":"Yao"},{"family":"Sun","given":"Yaofeng"},{"family":"Li","given":"Yaohui"},{"family":"Wang","given":"Yaohui"},{"family":"Zheng","given":"Yi"},{"family":"Zhang","given":"Yichao"},{"family":"Xiong","given":"Yiliang"},{"family":"Zhao","given":"Yilong"},{"family":"He","given":"Ying"},{"family":"Tang","given":"Ying"},{"family":"Piao","given":"Yishi"},{"family":"Dong","given":"Yixin"},{"family":"Tan","given":"Yixuan"},{"family":"Liu","given":"Yiyuan"},{"family":"Wang","given":"Yongji"},{"family":"Guo","given":"Yongqiang"},{"family":"Zhu","given":"Yuchen"},{"family":"Wang","given":"Yuduan"},{"family":"Zou","given":"Yuheng"},{"family":"Zha","given":"Yukun"},{"family":"Ma","given":"Yunxian"},{"family":"Yan","given":"Yuting"},{"family":"You","given":"Yuxiang"},{"family":"Liu","given":"Yuxuan"},{"family":"Ren","given":"Z. Z."},{"family":"Ren","given":"Zehui"},{"family":"Sha","given":"Zhangli"},{"family":"Fu","given":"Zhe"},{"family":"Huang","given":"Zhen"},{"family":"Zhang","given":"Zhen"},{"family":"Xie","given":"Zhenda"},{"family":"Hao","given":"Zhewen"},{"family":"Shao","given":"Zhihong"},{"family":"Wen","given":"Zhiniu"},{"family":"Xu","given":"Zhipeng"},{"family":"Zhang","given":"Zhongyu"},{"family":"Li","given":"Zhuoshu"},{"family":"Wang","given":"Zihan"},{"family":"Gu","given":"Zihui"},{"family":"Li","given":"Zilin"},{"family":"Xie","given":"Ziwei"}],"citation-key":"deepseek-aiDeepSeekV2StrongEconomical2024","DOI":"10.48550/arXiv.2405.04434","issued":{"date-parts":[["2024",6,19]]},"language":"en","number":"arXiv:2405.04434","publisher":"arXiv","source":"arXiv.org","title":"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model","title-short":"DeepSeek-V2","type":"article","URL":"http://arxiv.org/abs/2405.04434"},{"id":"deepseek-aiDeepSeekV3TechnicalReport2024","abstract":"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.","accessed":{"date-parts":[["2025",1,29]]},"author":[{"family":"DeepSeek-AI","given":""},{"family":"Liu","given":"Aixin"},{"family":"Feng","given":"Bei"},{"family":"Xue","given":"Bing"},{"family":"Wang","given":"Bingxuan"},{"family":"Wu","given":"Bochao"},{"family":"Lu","given":"Chengda"},{"family":"Zhao","given":"Chenggang"},{"family":"Deng","given":"Chengqi"},{"family":"Zhang","given":"Chenyu"},{"family":"Ruan","given":"Chong"},{"family":"Dai","given":"Damai"},{"family":"Guo","given":"Daya"},{"family":"Yang","given":"Dejian"},{"family":"Chen","given":"Deli"},{"family":"Ji","given":"Dongjie"},{"family":"Li","given":"Erhang"},{"family":"Lin","given":"Fangyun"},{"family":"Dai","given":"Fucong"},{"family":"Luo","given":"Fuli"},{"family":"Hao","given":"Guangbo"},{"family":"Chen","given":"Guanting"},{"family":"Li","given":"Guowei"},{"family":"Zhang","given":"H."},{"family":"Bao","given":"Han"},{"family":"Xu","given":"Hanwei"},{"family":"Wang","given":"Haocheng"},{"family":"Zhang","given":"Haowei"},{"family":"Ding","given":"Honghui"},{"family":"Xin","given":"Huajian"},{"family":"Gao","given":"Huazuo"},{"family":"Li","given":"Hui"},{"family":"Qu","given":"Hui"},{"family":"Cai","given":"J. L."},{"family":"Liang","given":"Jian"},{"family":"Guo","given":"Jianzhong"},{"family":"Ni","given":"Jiaqi"},{"family":"Li","given":"Jiashi"},{"family":"Wang","given":"Jiawei"},{"family":"Chen","given":"Jin"},{"family":"Chen","given":"Jingchang"},{"family":"Yuan","given":"Jingyang"},{"family":"Qiu","given":"Junjie"},{"family":"Li","given":"Junlong"},{"family":"Song","given":"Junxiao"},{"family":"Dong","given":"Kai"},{"family":"Hu","given":"Kai"},{"family":"Gao","given":"Kaige"},{"family":"Guan","given":"Kang"},{"family":"Huang","given":"Kexin"},{"family":"Yu","given":"Kuai"},{"family":"Wang","given":"Lean"},{"family":"Zhang","given":"Lecong"},{"family":"Xu","given":"Lei"},{"family":"Xia","given":"Leyi"},{"family":"Zhao","given":"Liang"},{"family":"Wang","given":"Litong"},{"family":"Zhang","given":"Liyue"},{"family":"Li","given":"Meng"},{"family":"Wang","given":"Miaojun"},{"family":"Zhang","given":"Mingchuan"},{"family":"Zhang","given":"Minghua"},{"family":"Tang","given":"Minghui"},{"family":"Li","given":"Mingming"},{"family":"Tian","given":"Ning"},{"family":"Huang","given":"Panpan"},{"family":"Wang","given":"Peiyi"},{"family":"Zhang","given":"Peng"},{"family":"Wang","given":"Qiancheng"},{"family":"Zhu","given":"Qihao"},{"family":"Chen","given":"Qinyu"},{"family":"Du","given":"Qiushi"},{"family":"Chen","given":"R. J."},{"family":"Jin","given":"R. L."},{"family":"Ge","given":"Ruiqi"},{"family":"Zhang","given":"Ruisong"},{"family":"Pan","given":"Ruizhe"},{"family":"Wang","given":"Runji"},{"family":"Xu","given":"Runxin"},{"family":"Zhang","given":"Ruoyu"},{"family":"Chen","given":"Ruyi"},{"family":"Li","given":"S. S."},{"family":"Lu","given":"Shanghao"},{"family":"Zhou","given":"Shangyan"},{"family":"Chen","given":"Shanhuang"},{"family":"Wu","given":"Shaoqing"},{"family":"Ye","given":"Shengfeng"},{"family":"Ye","given":"Shengfeng"},{"family":"Ma","given":"Shirong"},{"family":"Wang","given":"Shiyu"},{"family":"Zhou","given":"Shuang"},{"family":"Yu","given":"Shuiping"},{"family":"Zhou","given":"Shunfeng"},{"family":"Pan","given":"Shuting"},{"family":"Wang","given":"T."},{"family":"Yun","given":"Tao"},{"family":"Pei","given":"Tian"},{"family":"Sun","given":"Tianyu"},{"family":"Xiao","given":"W. L."},{"family":"Zeng","given":"Wangding"},{"family":"Zhao","given":"Wanjia"},{"family":"An","given":"Wei"},{"family":"Liu","given":"Wen"},{"family":"Liang","given":"Wenfeng"},{"family":"Gao","given":"Wenjun"},{"family":"Yu","given":"Wenqin"},{"family":"Zhang","given":"Wentao"},{"family":"Li","given":"X. Q."},{"family":"Jin","given":"Xiangyue"},{"family":"Wang","given":"Xianzu"},{"family":"Bi","given":"Xiao"},{"family":"Liu","given":"Xiaodong"},{"family":"Wang","given":"Xiaohan"},{"family":"Shen","given":"Xiaojin"},{"family":"Chen","given":"Xiaokang"},{"family":"Zhang","given":"Xiaokang"},{"family":"Chen","given":"Xiaosha"},{"family":"Nie","given":"Xiaotao"},{"family":"Sun","given":"Xiaowen"},{"family":"Wang","given":"Xiaoxiang"},{"family":"Cheng","given":"Xin"},{"family":"Liu","given":"Xin"},{"family":"Xie","given":"Xin"},{"family":"Liu","given":"Xingchao"},{"family":"Yu","given":"Xingkai"},{"family":"Song","given":"Xinnan"},{"family":"Shan","given":"Xinxia"},{"family":"Zhou","given":"Xinyi"},{"family":"Yang","given":"Xinyu"},{"family":"Li","given":"Xinyuan"},{"family":"Su","given":"Xuecheng"},{"family":"Lin","given":"Xuheng"},{"family":"Li","given":"Y. K."},{"family":"Wang","given":"Y. Q."},{"family":"Wei","given":"Y. X."},{"family":"Zhu","given":"Y. X."},{"family":"Zhang","given":"Yang"},{"family":"Xu","given":"Yanhong"},{"family":"Xu","given":"Yanhong"},{"family":"Huang","given":"Yanping"},{"family":"Li","given":"Yao"},{"family":"Zhao","given":"Yao"},{"family":"Sun","given":"Yaofeng"},{"family":"Li","given":"Yaohui"},{"family":"Wang","given":"Yaohui"},{"family":"Yu","given":"Yi"},{"family":"Zheng","given":"Yi"},{"family":"Zhang","given":"Yichao"},{"family":"Shi","given":"Yifan"},{"family":"Xiong","given":"Yiliang"},{"family":"He","given":"Ying"},{"family":"Tang","given":"Ying"},{"family":"Piao","given":"Yishi"},{"family":"Wang","given":"Yisong"},{"family":"Tan","given":"Yixuan"},{"family":"Ma","given":"Yiyang"},{"family":"Liu","given":"Yiyuan"},{"family":"Guo","given":"Yongqiang"},{"family":"Wu","given":"Yu"},{"family":"Ou","given":"Yuan"},{"family":"Zhu","given":"Yuchen"},{"family":"Wang","given":"Yuduan"},{"family":"Gong","given":"Yue"},{"family":"Zou","given":"Yuheng"},{"family":"He","given":"Yujia"},{"family":"Zha","given":"Yukun"},{"family":"Xiong","given":"Yunfan"},{"family":"Ma","given":"Yunxian"},{"family":"Yan","given":"Yuting"},{"family":"Luo","given":"Yuxiang"},{"family":"You","given":"Yuxiang"},{"family":"Liu","given":"Yuxuan"},{"family":"Zhou","given":"Yuyang"},{"family":"Wu","given":"Z. F."},{"family":"Ren","given":"Z. Z."},{"family":"Ren","given":"Zehui"},{"family":"Sha","given":"Zhangli"},{"family":"Fu","given":"Zhe"},{"family":"Xu","given":"Zhean"},{"family":"Huang","given":"Zhen"},{"family":"Zhang","given":"Zhen"},{"family":"Xie","given":"Zhenda"},{"family":"Zhang","given":"Zhengyan"},{"family":"Hao","given":"Zhewen"},{"family":"Gou","given":"Zhibin"},{"family":"Ma","given":"Zhicheng"},{"family":"Yan","given":"Zhigang"},{"family":"Shao","given":"Zhihong"},{"family":"Xu","given":"Zhipeng"},{"family":"Wu","given":"Zhiyu"},{"family":"Zhang","given":"Zhongyu"},{"family":"Li","given":"Zhuoshu"},{"family":"Gu","given":"Zihui"},{"family":"Zhu","given":"Zijia"},{"family":"Liu","given":"Zijun"},{"family":"Li","given":"Zilin"},{"family":"Xie","given":"Ziwei"},{"family":"Song","given":"Ziyang"},{"family":"Gao","given":"Ziyi"},{"family":"Pan","given":"Zizheng"}],"citation-key":"deepseek-aiDeepSeekV3TechnicalReport2024","DOI":"10.48550/arXiv.2412.19437","issued":{"date-parts":[["2024",12,27]]},"language":"en","number":"arXiv:2412.19437","publisher":"arXiv","source":"arXiv.org","title":"DeepSeek-V3 Technical Report","type":"article","URL":"http://arxiv.org/abs/2412.19437"},{"id":"dhuliawalaChainofVerificationReducesHallucination2023","abstract":"Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (COVE) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show COVE decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Dhuliawala","given":"Shehzaad"},{"family":"Komeili","given":"Mojtaba"},{"family":"Xu","given":"Jing"},{"family":"Raileanu","given":"Roberta"},{"family":"Li","given":"Xian"},{"family":"Celikyilmaz","given":"Asli"},{"family":"Weston","given":"Jason"}],"citation-key":"dhuliawalaChainofVerificationReducesHallucination2023","DOI":"10.48550/arXiv.2309.11495","issued":{"date-parts":[["2023",9,25]]},"language":"en","number":"arXiv:2309.11495","publisher":"arXiv","source":"arXiv.org","title":"Chain-of-Verification Reduces Hallucination in Large Language Models","type":"article","URL":"http://arxiv.org/abs/2309.11495"},{"id":"echterhoffCognitiveBiasDecisionMaking2024","abstract":"Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BIASBUSTER, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method utilising LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across different commercial and open-source models. We demonstrate that our self-help debiasing effectively mitigate cognitive bias without having to manually craft examples for each bias type.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Echterhoff","given":"Jessica"},{"family":"Liu","given":"Yao"},{"family":"Alessa","given":"Abeer"},{"family":"McAuley","given":"Julian"},{"family":"He","given":"Zexue"}],"citation-key":"echterhoffCognitiveBiasDecisionMaking2024","DOI":"10.48550/arXiv.2403.00811","issued":{"date-parts":[["2024",10,3]]},"language":"en","number":"arXiv:2403.00811","publisher":"arXiv","source":"arXiv.org","title":"Cognitive Bias in Decision-Making with LLMs","type":"article","URL":"http://arxiv.org/abs/2403.00811"},{"id":"grattafioriLlama3Herd2024","abstract":"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.","accessed":{"date-parts":[["2025",1,29]]},"author":[{"family":"Grattafiori","given":"Aaron"},{"family":"Dubey","given":"Abhimanyu"},{"family":"Jauhri","given":"Abhinav"},{"family":"Pandey","given":"Abhinav"},{"family":"Kadian","given":"Abhishek"},{"family":"Al-Dahle","given":"Ahmad"},{"family":"Letman","given":"Aiesha"},{"family":"Mathur","given":"Akhil"},{"family":"Schelten","given":"Alan"},{"family":"Vaughan","given":"Alex"},{"family":"Yang","given":"Amy"},{"family":"Fan","given":"Angela"},{"family":"Goyal","given":"Anirudh"},{"family":"Hartshorn","given":"Anthony"},{"family":"Yang","given":"Aobo"},{"family":"Mitra","given":"Archi"},{"family":"Sravankumar","given":"Archie"},{"family":"Korenev","given":"Artem"},{"family":"Hinsvark","given":"Arthur"},{"family":"Rao","given":"Arun"},{"family":"Zhang","given":"Aston"},{"family":"Rodriguez","given":"Aurelien"},{"family":"Gregerson","given":"Austen"},{"family":"Spataru","given":"Ava"},{"family":"Roziere","given":"Baptiste"},{"family":"Biron","given":"Bethany"},{"family":"Tang","given":"Binh"},{"family":"Chern","given":"Bobbie"},{"family":"Caucheteux","given":"Charlotte"},{"family":"Nayak","given":"Chaya"},{"family":"Bi","given":"Chloe"},{"family":"Marra","given":"Chris"},{"family":"McConnell","given":"Chris"},{"family":"Keller","given":"Christian"},{"family":"Touret","given":"Christophe"},{"family":"Wu","given":"Chunyang"},{"family":"Wong","given":"Corinne"},{"family":"Ferrer","given":"Cristian Canton"},{"family":"Nikolaidis","given":"Cyrus"},{"family":"Allonsius","given":"Damien"},{"family":"Song","given":"Daniel"},{"family":"Pintz","given":"Danielle"},{"family":"Livshits","given":"Danny"},{"family":"Wyatt","given":"Danny"},{"family":"Esiobu","given":"David"},{"family":"Choudhary","given":"Dhruv"},{"family":"Mahajan","given":"Dhruv"},{"family":"Garcia-Olano","given":"Diego"},{"family":"Perino","given":"Diego"},{"family":"Hupkes","given":"Dieuwke"},{"family":"Lakomkin","given":"Egor"},{"family":"AlBadawy","given":"Ehab"},{"family":"Lobanova","given":"Elina"},{"family":"Dinan","given":"Emily"},{"family":"Smith","given":"Eric Michael"},{"family":"Radenovic","given":"Filip"},{"family":"Guzmán","given":"Francisco"},{"family":"Zhang","given":"Frank"},{"family":"Synnaeve","given":"Gabriel"},{"family":"Lee","given":"Gabrielle"},{"family":"Anderson","given":"Georgia Lewis"},{"family":"Thattai","given":"Govind"},{"family":"Nail","given":"Graeme"},{"family":"Mialon","given":"Gregoire"},{"family":"Pang","given":"Guan"},{"family":"Cucurell","given":"Guillem"},{"family":"Nguyen","given":"Hailey"},{"family":"Korevaar","given":"Hannah"},{"family":"Xu","given":"Hu"},{"family":"Touvron","given":"Hugo"},{"family":"Zarov","given":"Iliyan"},{"family":"Ibarra","given":"Imanol Arrieta"},{"family":"Kloumann","given":"Isabel"},{"family":"Misra","given":"Ishan"},{"family":"Evtimov","given":"Ivan"},{"family":"Zhang","given":"Jack"},{"family":"Copet","given":"Jade"},{"family":"Lee","given":"Jaewon"},{"family":"Geffert","given":"Jan"},{"family":"Vranes","given":"Jana"},{"family":"Park","given":"Jason"},{"family":"Mahadeokar","given":"Jay"},{"family":"Shah","given":"Jeet"},{"family":"Linde","given":"Jelmer","dropping-particle":"van der"},{"family":"Billock","given":"Jennifer"},{"family":"Hong","given":"Jenny"},{"family":"Lee","given":"Jenya"},{"family":"Fu","given":"Jeremy"},{"family":"Chi","given":"Jianfeng"},{"family":"Huang","given":"Jianyu"},{"family":"Liu","given":"Jiawen"},{"family":"Wang","given":"Jie"},{"family":"Yu","given":"Jiecao"},{"family":"Bitton","given":"Joanna"},{"family":"Spisak","given":"Joe"},{"family":"Park","given":"Jongsoo"},{"family":"Rocca","given":"Joseph"},{"family":"Johnstun","given":"Joshua"},{"family":"Saxe","given":"Joshua"},{"family":"Jia","given":"Junteng"},{"family":"Alwala","given":"Kalyan Vasuden"},{"family":"Prasad","given":"Karthik"},{"family":"Upasani","given":"Kartikeya"},{"family":"Plawiak","given":"Kate"},{"family":"Li","given":"Ke"},{"family":"Heafield","given":"Kenneth"},{"family":"Stone","given":"Kevin"},{"family":"El-Arini","given":"Khalid"},{"family":"Iyer","given":"Krithika"},{"family":"Malik","given":"Kshitiz"},{"family":"Chiu","given":"Kuenley"},{"family":"Bhalla","given":"Kunal"},{"family":"Lakhotia","given":"Kushal"},{"family":"Rantala-Yeary","given":"Lauren"},{"family":"Maaten","given":"Laurens","dropping-particle":"van der"},{"family":"Chen","given":"Lawrence"},{"family":"Tan","given":"Liang"},{"family":"Jenkins","given":"Liz"},{"family":"Martin","given":"Louis"},{"family":"Madaan","given":"Lovish"},{"family":"Malo","given":"Lubo"},{"family":"Blecher","given":"Lukas"},{"family":"Landzaat","given":"Lukas"},{"family":"Oliveira","given":"Luke","dropping-particle":"de"},{"family":"Muzzi","given":"Madeline"},{"family":"Pasupuleti","given":"Mahesh"},{"family":"Singh","given":"Mannat"},{"family":"Paluri","given":"Manohar"},{"family":"Kardas","given":"Marcin"},{"family":"Tsimpoukelli","given":"Maria"},{"family":"Oldham","given":"Mathew"},{"family":"Rita","given":"Mathieu"},{"family":"Pavlova","given":"Maya"},{"family":"Kambadur","given":"Melanie"},{"family":"Lewis","given":"Mike"},{"family":"Si","given":"Min"},{"family":"Singh","given":"Mitesh Kumar"},{"family":"Hassan","given":"Mona"},{"family":"Goyal","given":"Naman"},{"family":"Torabi","given":"Narjes"},{"family":"Bashlykov","given":"Nikolay"},{"family":"Bogoychev","given":"Nikolay"},{"family":"Chatterji","given":"Niladri"},{"family":"Zhang","given":"Ning"},{"family":"Duchenne","given":"Olivier"},{"family":"Çelebi","given":"Onur"},{"family":"Alrassy","given":"Patrick"},{"family":"Zhang","given":"Pengchuan"},{"family":"Li","given":"Pengwei"},{"family":"Vasic","given":"Petar"},{"family":"Weng","given":"Peter"},{"family":"Bhargava","given":"Prajjwal"},{"family":"Dubal","given":"Pratik"},{"family":"Krishnan","given":"Praveen"},{"family":"Koura","given":"Punit Singh"},{"family":"Xu","given":"Puxin"},{"family":"He","given":"Qing"},{"family":"Dong","given":"Qingxiao"},{"family":"Srinivasan","given":"Ragavan"},{"family":"Ganapathy","given":"Raj"},{"family":"Calderer","given":"Ramon"},{"family":"Cabral","given":"Ricardo Silveira"},{"family":"Stojnic","given":"Robert"},{"family":"Raileanu","given":"Roberta"},{"family":"Maheswari","given":"Rohan"},{"family":"Girdhar","given":"Rohit"},{"family":"Patel","given":"Rohit"},{"family":"Sauvestre","given":"Romain"},{"family":"Polidoro","given":"Ronnie"},{"family":"Sumbaly","given":"Roshan"},{"family":"Taylor","given":"Ross"},{"family":"Silva","given":"Ruan"},{"family":"Hou","given":"Rui"},{"family":"Wang","given":"Rui"},{"family":"Hosseini","given":"Saghar"},{"family":"Chennabasappa","given":"Sahana"},{"family":"Singh","given":"Sanjay"},{"family":"Bell","given":"Sean"},{"family":"Kim","given":"Seohyun Sonia"},{"family":"Edunov","given":"Sergey"},{"family":"Nie","given":"Shaoliang"},{"family":"Narang","given":"Sharan"},{"family":"Raparthy","given":"Sharath"},{"family":"Shen","given":"Sheng"},{"family":"Wan","given":"Shengye"},{"family":"Bhosale","given":"Shruti"},{"family":"Zhang","given":"Shun"},{"family":"Vandenhende","given":"Simon"},{"family":"Batra","given":"Soumya"},{"family":"Whitman","given":"Spencer"},{"family":"Sootla","given":"Sten"},{"family":"Collot","given":"Stephane"},{"family":"Gururangan","given":"Suchin"},{"family":"Borodinsky","given":"Sydney"},{"family":"Herman","given":"Tamar"},{"family":"Fowler","given":"Tara"},{"family":"Sheasha","given":"Tarek"},{"family":"Georgiou","given":"Thomas"},{"family":"Scialom","given":"Thomas"},{"family":"Speckbacher","given":"Tobias"},{"family":"Mihaylov","given":"Todor"},{"family":"Xiao","given":"Tong"},{"family":"Karn","given":"Ujjwal"},{"family":"Goswami","given":"Vedanuj"},{"family":"Gupta","given":"Vibhor"},{"family":"Ramanathan","given":"Vignesh"},{"family":"Kerkez","given":"Viktor"},{"family":"Gonguet","given":"Vincent"},{"family":"Do","given":"Virginie"},{"family":"Vogeti","given":"Vish"},{"family":"Albiero","given":"Vítor"},{"family":"Petrovic","given":"Vladan"},{"family":"Chu","given":"Weiwei"},{"family":"Xiong","given":"Wenhan"},{"family":"Fu","given":"Wenyin"},{"family":"Meers","given":"Whitney"},{"family":"Martinet","given":"Xavier"},{"family":"Wang","given":"Xiaodong"},{"family":"Wang","given":"Xiaofang"},{"family":"Tan","given":"Xiaoqing Ellen"},{"family":"Xia","given":"Xide"},{"family":"Xie","given":"Xinfeng"},{"family":"Jia","given":"Xuchao"},{"family":"Wang","given":"Xuewei"},{"family":"Goldschlag","given":"Yaelle"},{"family":"Gaur","given":"Yashesh"},{"family":"Babaei","given":"Yasmine"},{"family":"Wen","given":"Yi"},{"family":"Song","given":"Yiwen"},{"family":"Zhang","given":"Yuchen"},{"family":"Li","given":"Yue"},{"family":"Mao","given":"Yuning"},{"family":"Coudert","given":"Zacharie Delpierre"},{"family":"Yan","given":"Zheng"},{"family":"Chen","given":"Zhengxing"},{"family":"Papakipos","given":"Zoe"},{"family":"Singh","given":"Aaditya"},{"family":"Srivastava","given":"Aayushi"},{"family":"Jain","given":"Abha"},{"family":"Kelsey","given":"Adam"},{"family":"Shajnfeld","given":"Adam"},{"family":"Gangidi","given":"Adithya"},{"family":"Victoria","given":"Adolfo"},{"family":"Goldstand","given":"Ahuva"},{"family":"Menon","given":"Ajay"},{"family":"Sharma","given":"Ajay"},{"family":"Boesenberg","given":"Alex"},{"family":"Baevski","given":"Alexei"},{"family":"Feinstein","given":"Allie"},{"family":"Kallet","given":"Amanda"},{"family":"Sangani","given":"Amit"},{"family":"Teo","given":"Amos"},{"family":"Yunus","given":"Anam"},{"family":"Lupu","given":"Andrei"},{"family":"Alvarado","given":"Andres"},{"family":"Caples","given":"Andrew"},{"family":"Gu","given":"Andrew"},{"family":"Ho","given":"Andrew"},{"family":"Poulton","given":"Andrew"},{"family":"Ryan","given":"Andrew"},{"family":"Ramchandani","given":"Ankit"},{"family":"Dong","given":"Annie"},{"family":"Franco","given":"Annie"},{"family":"Goyal","given":"Anuj"},{"family":"Saraf","given":"Aparajita"},{"family":"Chowdhury","given":"Arkabandhu"},{"family":"Gabriel","given":"Ashley"},{"family":"Bharambe","given":"Ashwin"},{"family":"Eisenman","given":"Assaf"},{"family":"Yazdan","given":"Azadeh"},{"family":"James","given":"Beau"},{"family":"Maurer","given":"Ben"},{"family":"Leonhardi","given":"Benjamin"},{"family":"Huang","given":"Bernie"},{"family":"Loyd","given":"Beth"},{"family":"Paola","given":"Beto De"},{"family":"Paranjape","given":"Bhargavi"},{"family":"Liu","given":"Bing"},{"family":"Wu","given":"Bo"},{"family":"Ni","given":"Boyu"},{"family":"Hancock","given":"Braden"},{"family":"Wasti","given":"Bram"},{"family":"Spence","given":"Brandon"},{"family":"Stojkovic","given":"Brani"},{"family":"Gamido","given":"Brian"},{"family":"Montalvo","given":"Britt"},{"family":"Parker","given":"Carl"},{"family":"Burton","given":"Carly"},{"family":"Mejia","given":"Catalina"},{"family":"Liu","given":"Ce"},{"family":"Wang","given":"Changhan"},{"family":"Kim","given":"Changkyu"},{"family":"Zhou","given":"Chao"},{"family":"Hu","given":"Chester"},{"family":"Chu","given":"Ching-Hsiang"},{"family":"Cai","given":"Chris"},{"family":"Tindal","given":"Chris"},{"family":"Feichtenhofer","given":"Christoph"},{"family":"Gao","given":"Cynthia"},{"family":"Civin","given":"Damon"},{"family":"Beaty","given":"Dana"},{"family":"Kreymer","given":"Daniel"},{"family":"Li","given":"Daniel"},{"family":"Adkins","given":"David"},{"family":"Xu","given":"David"},{"family":"Testuggine","given":"Davide"},{"family":"David","given":"Delia"},{"family":"Parikh","given":"Devi"},{"family":"Liskovich","given":"Diana"},{"family":"Foss","given":"Didem"},{"family":"Wang","given":"Dingkang"},{"family":"Le","given":"Duc"},{"family":"Holland","given":"Dustin"},{"family":"Dowling","given":"Edward"},{"family":"Jamil","given":"Eissa"},{"family":"Montgomery","given":"Elaine"},{"family":"Presani","given":"Eleonora"},{"family":"Hahn","given":"Emily"},{"family":"Wood","given":"Emily"},{"family":"Le","given":"Eric-Tuan"},{"family":"Brinkman","given":"Erik"},{"family":"Arcaute","given":"Esteban"},{"family":"Dunbar","given":"Evan"},{"family":"Smothers","given":"Evan"},{"family":"Sun","given":"Fei"},{"family":"Kreuk","given":"Felix"},{"family":"Tian","given":"Feng"},{"family":"Kokkinos","given":"Filippos"},{"family":"Ozgenel","given":"Firat"},{"family":"Caggioni","given":"Francesco"},{"family":"Kanayet","given":"Frank"},{"family":"Seide","given":"Frank"},{"family":"Florez","given":"Gabriela Medina"},{"family":"Schwarz","given":"Gabriella"},{"family":"Badeer","given":"Gada"},{"family":"Swee","given":"Georgia"},{"family":"Halpern","given":"Gil"},{"family":"Herman","given":"Grant"},{"family":"Sizov","given":"Grigory"},{"family":"Guangyi","given":""},{"family":"Zhang","given":""},{"family":"Lakshminarayanan","given":"Guna"},{"family":"Inan","given":"Hakan"},{"family":"Shojanazeri","given":"Hamid"},{"family":"Zou","given":"Han"},{"family":"Wang","given":"Hannah"},{"family":"Zha","given":"Hanwen"},{"family":"Habeeb","given":"Haroun"},{"family":"Rudolph","given":"Harrison"},{"family":"Suk","given":"Helen"},{"family":"Aspegren","given":"Henry"},{"family":"Goldman","given":"Hunter"},{"family":"Zhan","given":"Hongyuan"},{"family":"Damlaj","given":"Ibrahim"},{"family":"Molybog","given":"Igor"},{"family":"Tufanov","given":"Igor"},{"family":"Leontiadis","given":"Ilias"},{"family":"Veliche","given":"Irina-Elena"},{"family":"Gat","given":"Itai"},{"family":"Weissman","given":"Jake"},{"family":"Geboski","given":"James"},{"family":"Kohli","given":"James"},{"family":"Lam","given":"Janice"},{"family":"Asher","given":"Japhet"},{"family":"Gaya","given":"Jean-Baptiste"},{"family":"Marcus","given":"Jeff"},{"family":"Tang","given":"Jeff"},{"family":"Chan","given":"Jennifer"},{"family":"Zhen","given":"Jenny"},{"family":"Reizenstein","given":"Jeremy"},{"family":"Teboul","given":"Jeremy"},{"family":"Zhong","given":"Jessica"},{"family":"Jin","given":"Jian"},{"family":"Yang","given":"Jingyi"},{"family":"Cummings","given":"Joe"},{"family":"Carvill","given":"Jon"},{"family":"Shepard","given":"Jon"},{"family":"McPhie","given":"Jonathan"},{"family":"Torres","given":"Jonathan"},{"family":"Ginsburg","given":"Josh"},{"family":"Wang","given":"Junjie"},{"family":"Wu","given":"Kai"},{"family":"U","given":"Kam Hou"},{"family":"Saxena","given":"Karan"},{"family":"Khandelwal","given":"Kartikay"},{"family":"Zand","given":"Katayoun"},{"family":"Matosich","given":"Kathy"},{"family":"Veeraraghavan","given":"Kaushik"},{"family":"Michelena","given":"Kelly"},{"family":"Li","given":"Keqian"},{"family":"Jagadeesh","given":"Kiran"},{"family":"Huang","given":"Kun"},{"family":"Chawla","given":"Kunal"},{"family":"Huang","given":"Kyle"},{"family":"Chen","given":"Lailin"},{"family":"Garg","given":"Lakshya"},{"family":"A","given":"Lavender"},{"family":"Silva","given":"Leandro"},{"family":"Bell","given":"Lee"},{"family":"Zhang","given":"Lei"},{"family":"Guo","given":"Liangpeng"},{"family":"Yu","given":"Licheng"},{"family":"Moshkovich","given":"Liron"},{"family":"Wehrstedt","given":"Luca"},{"family":"Khabsa","given":"Madian"},{"family":"Avalani","given":"Manav"},{"family":"Bhatt","given":"Manish"},{"family":"Mankus","given":"Martynas"},{"family":"Hasson","given":"Matan"},{"family":"Lennie","given":"Matthew"},{"family":"Reso","given":"Matthias"},{"family":"Groshev","given":"Maxim"},{"family":"Naumov","given":"Maxim"},{"family":"Lathi","given":"Maya"},{"family":"Keneally","given":"Meghan"},{"family":"Liu","given":"Miao"},{"family":"Seltzer","given":"Michael L."},{"family":"Valko","given":"Michal"},{"family":"Restrepo","given":"Michelle"},{"family":"Patel","given":"Mihir"},{"family":"Vyatskov","given":"Mik"},{"family":"Samvelyan","given":"Mikayel"},{"family":"Clark","given":"Mike"},{"family":"Macey","given":"Mike"},{"family":"Wang","given":"Mike"},{"family":"Hermoso","given":"Miquel Jubert"},{"family":"Metanat","given":"Mo"},{"family":"Rastegari","given":"Mohammad"},{"family":"Bansal","given":"Munish"},{"family":"Santhanam","given":"Nandhini"},{"family":"Parks","given":"Natascha"},{"family":"White","given":"Natasha"},{"family":"Bawa","given":"Navyata"},{"family":"Singhal","given":"Nayan"},{"family":"Egebo","given":"Nick"},{"family":"Usunier","given":"Nicolas"},{"family":"Mehta","given":"Nikhil"},{"family":"Laptev","given":"Nikolay Pavlovich"},{"family":"Dong","given":"Ning"},{"family":"Cheng","given":"Norman"},{"family":"Chernoguz","given":"Oleg"},{"family":"Hart","given":"Olivia"},{"family":"Salpekar","given":"Omkar"},{"family":"Kalinli","given":"Ozlem"},{"family":"Kent","given":"Parkin"},{"family":"Parekh","given":"Parth"},{"family":"Saab","given":"Paul"},{"family":"Balaji","given":"Pavan"},{"family":"Rittner","given":"Pedro"},{"family":"Bontrager","given":"Philip"},{"family":"Roux","given":"Pierre"},{"family":"Dollar","given":"Piotr"},{"family":"Zvyagina","given":"Polina"},{"family":"Ratanchandani","given":"Prashant"},{"family":"Yuvraj","given":"Pritish"},{"family":"Liang","given":"Qian"},{"family":"Alao","given":"Rachad"},{"family":"Rodriguez","given":"Rachel"},{"family":"Ayub","given":"Rafi"},{"family":"Murthy","given":"Raghotham"},{"family":"Nayani","given":"Raghu"},{"family":"Mitra","given":"Rahul"},{"family":"Parthasarathy","given":"Rangaprabhu"},{"family":"Li","given":"Raymond"},{"family":"Hogan","given":"Rebekkah"},{"family":"Battey","given":"Robin"},{"family":"Wang","given":"Rocky"},{"family":"Howes","given":"Russ"},{"family":"Rinott","given":"Ruty"},{"family":"Mehta","given":"Sachin"},{"family":"Siby","given":"Sachin"},{"family":"Bondu","given":"Sai Jayesh"},{"family":"Datta","given":"Samyak"},{"family":"Chugh","given":"Sara"},{"family":"Hunt","given":"Sara"},{"family":"Dhillon","given":"Sargun"},{"family":"Sidorov","given":"Sasha"},{"family":"Pan","given":"Satadru"},{"family":"Mahajan","given":"Saurabh"},{"family":"Verma","given":"Saurabh"},{"family":"Yamamoto","given":"Seiji"},{"family":"Ramaswamy","given":"Sharadh"},{"family":"Lindsay","given":"Shaun"},{"family":"Lindsay","given":"Shaun"},{"family":"Feng","given":"Sheng"},{"family":"Lin","given":"Shenghao"},{"family":"Zha","given":"Shengxin Cindy"},{"family":"Patil","given":"Shishir"},{"family":"Shankar","given":"Shiva"},{"family":"Zhang","given":"Shuqiang"},{"family":"Zhang","given":"Shuqiang"},{"family":"Wang","given":"Sinong"},{"family":"Agarwal","given":"Sneha"},{"family":"Sajuyigbe","given":"Soji"},{"family":"Chintala","given":"Soumith"},{"family":"Max","given":"Stephanie"},{"family":"Chen","given":"Stephen"},{"family":"Kehoe","given":"Steve"},{"family":"Satterfield","given":"Steve"},{"family":"Govindaprasad","given":"Sudarshan"},{"family":"Gupta","given":"Sumit"},{"family":"Deng","given":"Summer"},{"family":"Cho","given":"Sungmin"},{"family":"Virk","given":"Sunny"},{"family":"Subramanian","given":"Suraj"},{"family":"Choudhury","given":"Sy"},{"family":"Goldman","given":"Sydney"},{"family":"Remez","given":"Tal"},{"family":"Glaser","given":"Tamar"},{"family":"Best","given":"Tamara"},{"family":"Koehler","given":"Thilo"},{"family":"Robinson","given":"Thomas"},{"family":"Li","given":"Tianhe"},{"family":"Zhang","given":"Tianjun"},{"family":"Matthews","given":"Tim"},{"family":"Chou","given":"Timothy"},{"family":"Shaked","given":"Tzook"},{"family":"Vontimitta","given":"Varun"},{"family":"Ajayi","given":"Victoria"},{"family":"Montanez","given":"Victoria"},{"family":"Mohan","given":"Vijai"},{"family":"Kumar","given":"Vinay Satish"},{"family":"Mangla","given":"Vishal"},{"family":"Ionescu","given":"Vlad"},{"family":"Poenaru","given":"Vlad"},{"family":"Mihailescu","given":"Vlad Tiberiu"},{"family":"Ivanov","given":"Vladimir"},{"family":"Li","given":"Wei"},{"family":"Wang","given":"Wenchen"},{"family":"Jiang","given":"Wenwen"},{"family":"Bouaziz","given":"Wes"},{"family":"Constable","given":"Will"},{"family":"Tang","given":"Xiaocheng"},{"family":"Wu","given":"Xiaojian"},{"family":"Wang","given":"Xiaolan"},{"family":"Wu","given":"Xilun"},{"family":"Gao","given":"Xinbo"},{"family":"Kleinman","given":"Yaniv"},{"family":"Chen","given":"Yanjun"},{"family":"Hu","given":"Ye"},{"family":"Jia","given":"Ye"},{"family":"Qi","given":"Ye"},{"family":"Li","given":"Yenda"},{"family":"Zhang","given":"Yilin"},{"family":"Zhang","given":"Ying"},{"family":"Adi","given":"Yossi"},{"family":"Nam","given":"Youngjin"},{"family":"Yu","given":""},{"family":"Wang","given":""},{"family":"Zhao","given":"Yu"},{"family":"Hao","given":"Yuchen"},{"family":"Qian","given":"Yundi"},{"family":"Li","given":"Yunlu"},{"family":"He","given":"Yuzi"},{"family":"Rait","given":"Zach"},{"family":"DeVito","given":"Zachary"},{"family":"Rosnbrick","given":"Zef"},{"family":"Wen","given":"Zhaoduo"},{"family":"Yang","given":"Zhenyu"},{"family":"Zhao","given":"Zhiwei"},{"family":"Ma","given":"Zhiyu"}],"citation-key":"grattafioriLlama3Herd2024","DOI":"10.48550/arXiv.2407.21783","issued":{"date-parts":[["2024",11,23]]},"language":"en","number":"arXiv:2407.21783","publisher":"arXiv","source":"arXiv.org","title":"The Llama 3 Herd of Models","type":"article","URL":"http://arxiv.org/abs/2407.21783"},{"id":"hongHallucinationsLeaderboardOpen2024","abstract":"Large Language Models (LLMs) have transformed the Natural Language Processing (NLP) landscape with their remarkable ability to understand and generate human-like text. However, these models are prone to “hallucinations” — outputs that do not align with factual reality or the input context. This paper introduces the Hallucinations Leaderboard, an open initiative to quantitatively measure and compare the tendency of each model to produce hallucinations. The leaderboard uses a comprehensive set of benchmarks focusing on different aspects of hallucinations, such as factuality and faithfulness, across various tasks, including questionanswering, summarisation, and reading comprehension. Our analysis provides insights into the performance of different models, guiding researchers and practitioners in choosing the most reliable models for their applications.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Hong","given":"Giwon"},{"family":"Gema","given":"Aryo Pradipta"},{"family":"Saxena","given":"Rohit"},{"family":"Du","given":"Xiaotang"},{"family":"Nie","given":"Ping"},{"family":"Zhao","given":"Yu"},{"family":"Perez-Beltrachini","given":"Laura"},{"family":"Ryabinin","given":"Max"},{"family":"He","given":"Xuanli"},{"family":"Fourrier","given":"Clémentine"},{"family":"Minervini","given":"Pasquale"}],"citation-key":"hongHallucinationsLeaderboardOpen2024","DOI":"10.48550/arXiv.2404.05904","issued":{"date-parts":[["2024",4,17]]},"language":"en","number":"arXiv:2404.05904","publisher":"arXiv","source":"arXiv.org","title":"The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models","type":"article","URL":"http://arxiv.org/abs/2404.05904"},{"id":"huangSurveyHallucinationLarge2024","abstract":"The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations. CCS Concepts: • Computing methodologies → Natural language generation; • General and reference → Surveys and overviews.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Huang","given":"Lei"},{"family":"Yu","given":"Weijiang"},{"family":"Ma","given":"Weitao"},{"family":"Zhong","given":"Weihong"},{"family":"Feng","given":"Zhangyin"},{"family":"Wang","given":"Haotian"},{"family":"Chen","given":"Qianglong"},{"family":"Peng","given":"Weihua"},{"family":"Feng","given":"Xiaocheng"},{"family":"Qin","given":"Bing"},{"family":"Liu","given":"Ting"}],"citation-key":"huangSurveyHallucinationLarge2024","container-title":"ACM Transactions on Information Systems","container-title-short":"ACM Trans. Inf. Syst.","DOI":"10.1145/3703155","ISSN":"1046-8188, 1558-2868","issued":{"date-parts":[["2024",11,20]]},"language":"en","page":"3703155","source":"arXiv.org","title":"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions","title-short":"A Survey on Hallucination in Large Language Models","type":"article-journal","URL":"http://arxiv.org/abs/2311.05232"},{"id":"huLoRALowRankAdaptation2021","abstract":"An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.","accessed":{"date-parts":[["2025",2,5]]},"author":[{"family":"Hu","given":"Edward J."},{"family":"Shen","given":"Yelong"},{"family":"Wallis","given":"Phillip"},{"family":"Allen-Zhu","given":"Zeyuan"},{"family":"Li","given":"Yuanzhi"},{"family":"Wang","given":"Shean"},{"family":"Wang","given":"Lu"},{"family":"Chen","given":"Weizhu"}],"citation-key":"huLoRALowRankAdaptation2021","DOI":"10.48550/arXiv.2106.09685","issued":{"date-parts":[["2021",10,16]]},"language":"en","number":"arXiv:2106.09685","publisher":"arXiv","source":"arXiv.org","title":"LoRA: Low-Rank Adaptation of Large Language Models","title-short":"LoRA","type":"article","URL":"http://arxiv.org/abs/2106.09685"},{"id":"huLoRALowRankAdaptation2021a","abstract":"An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.","accessed":{"date-parts":[["2025",2,5]]},"author":[{"family":"Hu","given":"Edward J."},{"family":"Shen","given":"Yelong"},{"family":"Wallis","given":"Phillip"},{"family":"Allen-Zhu","given":"Zeyuan"},{"family":"Li","given":"Yuanzhi"},{"family":"Wang","given":"Shean"},{"family":"Wang","given":"Lu"},{"family":"Chen","given":"Weizhu"}],"citation-key":"huLoRALowRankAdaptation2021a","DOI":"10.48550/arXiv.2106.09685","issued":{"date-parts":[["2021",10,16]]},"number":"arXiv:2106.09685","publisher":"arXiv","source":"arXiv.org","title":"LoRA: Low-Rank Adaptation of Large Language Models","title-short":"LoRA","type":"article","URL":"http://arxiv.org/abs/2106.09685"},{"id":"joshiTriviaQALargeScale2017","abstract":"We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/","accessed":{"date-parts":[["2025",2,10]]},"author":[{"family":"Joshi","given":"Mandar"},{"family":"Choi","given":"Eunsol"},{"family":"Weld","given":"Daniel S."},{"family":"Zettlemoyer","given":"Luke"}],"citation-key":"joshiTriviaQALargeScale2017","DOI":"10.48550/arXiv.1705.03551","issued":{"date-parts":[["2017",5,13]]},"number":"arXiv:1705.03551","publisher":"arXiv","source":"arXiv.org","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension","title-short":"TriviaQA","type":"article","URL":"http://arxiv.org/abs/1705.03551"},{"id":"kwiatkowskiNaturalQuestionsBenchmark2019","abstract":"We present the Natural Questions corpus, a question answering data set. Questions\nconsist of real anonymized, aggregated queries issued to the Google search\nengine. An annotator is presented with a question along with a Wikipedia page\nfrom the top 5 search results, and annotates a long answer (typically a\nparagraph) and a short answer (one or more entities) if present on the page, or\nmarks null if no long/short answer is present. The public release consists of\n307,373 training examples with single annotations; 7,830 examples with 5-way\nannotations for development data; and a further 7,842 examples with 5-way\nannotated sequestered as test data. We present experiments validating quality of\nthe data. We also describe analysis of 25-way annotations on 302 examples,\ngiving insights into human variability on the annotation task. We introduce\nrobust metrics for the purposes of evaluating question answering systems;\ndemonstrate high human upper bounds on these metrics; and establish baseline\nresults using competitive methods drawn from related literature.","accessed":{"date-parts":[["2025",2,10]]},"author":[{"family":"Kwiatkowski","given":"Tom"},{"family":"Palomaki","given":"Jennimaria"},{"family":"Redfield","given":"Olivia"},{"family":"Collins","given":"Michael"},{"family":"Parikh","given":"Ankur"},{"family":"Alberti","given":"Chris"},{"family":"Epstein","given":"Danielle"},{"family":"Polosukhin","given":"Illia"},{"family":"Devlin","given":"Jacob"},{"family":"Lee","given":"Kenton"},{"family":"Toutanova","given":"Kristina"},{"family":"Jones","given":"Llion"},{"family":"Kelcey","given":"Matthew"},{"family":"Chang","given":"Ming-Wei"},{"family":"Dai","given":"Andrew M."},{"family":"Uszkoreit","given":"Jakob"},{"family":"Le","given":"Quoc"},{"family":"Petrov","given":"Slav"}],"citation-key":"kwiatkowskiNaturalQuestionsBenchmark2019","container-title":"Transactions of the Association for Computational Linguistics","container-title-short":"Transactions of the Association for Computational Linguistics","DOI":"10.1162/tacl_a_00276","ISSN":"2307-387X","issued":{"date-parts":[["2019",8,1]]},"page":"453-466","source":"Silverchair","title":"Natural Questions: A Benchmark for Question Answering Research","title-short":"Natural Questions","type":"article-journal","URL":"https://doi.org/10.1162/tacl_a_00276","volume":"7"},{"id":"linTruthfulQAMeasuringHow2022","abstract":"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","accessed":{"date-parts":[["2025",2,10]]},"author":[{"family":"Lin","given":"Stephanie"},{"family":"Hilton","given":"Jacob"},{"family":"Evans","given":"Owain"}],"citation-key":"linTruthfulQAMeasuringHow2022","DOI":"10.48550/arXiv.2109.07958","issued":{"date-parts":[["2022",5,8]]},"number":"arXiv:2109.07958","publisher":"arXiv","source":"arXiv.org","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods","title-short":"TruthfulQA","type":"article","URL":"http://arxiv.org/abs/2109.07958"},{"id":"mallenWhenNotTrust2023","abstract":"Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only when necessary. Experimental results show that this significantly improves models' performance while reducing the inference costs.","accessed":{"date-parts":[["2025",2,10]]},"author":[{"family":"Mallen","given":"Alex"},{"family":"Asai","given":"Akari"},{"family":"Zhong","given":"Victor"},{"family":"Das","given":"Rajarshi"},{"family":"Khashabi","given":"Daniel"},{"family":"Hajishirzi","given":"Hannaneh"}],"citation-key":"mallenWhenNotTrust2023","DOI":"10.48550/arXiv.2212.10511","issued":{"date-parts":[["2023",7,2]]},"number":"arXiv:2212.10511","publisher":"arXiv","source":"arXiv.org","title":"When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories","title-short":"When Not to Trust Language Models","type":"article","URL":"http://arxiv.org/abs/2212.10511"},{"id":"manakulSelfCheckGPTZeroResourceBlackBox2023","abstract":"Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.","accessed":{"date-parts":[["2025",1,27]]},"author":[{"family":"Manakul","given":"Potsawee"},{"family":"Liusie","given":"Adian"},{"family":"Gales","given":"Mark J. F."}],"citation-key":"manakulSelfCheckGPTZeroResourceBlackBox2023","DOI":"10.48550/arXiv.2303.08896","issued":{"date-parts":[["2023",10,11]]},"language":"en","number":"arXiv:2303.08896","publisher":"arXiv","source":"arXiv.org","title":"SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models","title-short":"SelfCheckGPT","type":"article","URL":"http://arxiv.org/abs/2303.08896"},{"id":"minFActScoreFinegrainedAtomic2023","abstract":"Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Min","given":"Sewon"},{"family":"Krishna","given":"Kalpesh"},{"family":"Lyu","given":"Xinxi"},{"family":"Lewis","given":"Mike"},{"family":"Yih","given":"Wen-tau"},{"family":"Koh","given":"Pang Wei"},{"family":"Iyyer","given":"Mohit"},{"family":"Zettlemoyer","given":"Luke"},{"family":"Hajishirzi","given":"Hannaneh"}],"citation-key":"minFActScoreFinegrainedAtomic2023","DOI":"10.48550/arXiv.2305.14251","issued":{"date-parts":[["2023",10,11]]},"language":"en","number":"arXiv:2305.14251","publisher":"arXiv","source":"arXiv.org","title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation","title-short":"FActScore","type":"article","URL":"http://arxiv.org/abs/2305.14251"},{"id":"shiTrustingYourEvidence2024","abstract":"Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLANT5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model’s prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential. Our code is publicly released at https://github.com/ xhan77/context-aware-decoding.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Shi","given":"Weijia"},{"family":"Han","given":"Xiaochuang"},{"family":"Lewis","given":"Mike"},{"family":"Tsvetkov","given":"Yulia"},{"family":"Zettlemoyer","given":"Luke"},{"family":"Yih","given":"Wen-tau"}],"citation-key":"shiTrustingYourEvidence2024","container-title":"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)","DOI":"10.18653/v1/2024.naacl-short.69","event-place":"Mexico City, Mexico","event-title":"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)","issued":{"date-parts":[["2024"]]},"language":"en","page":"783-791","publisher":"Association for Computational Linguistics","publisher-place":"Mexico City, Mexico","source":"DOI.org (Crossref)","title":"Trusting Your Evidence: Hallucinate Less with Context-aware Decoding","title-short":"Trusting Your Evidence","type":"paper-conference","URL":"https://aclanthology.org/2024.naacl-short.69"},{"id":"stahlbergNMTSearchErrors2019","abstract":"We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-ﬁrst search. We use our exact search to ﬁnd the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to ﬁnd these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Stahlberg","given":"Felix"},{"family":"Byrne","given":"Bill"}],"citation-key":"stahlbergNMTSearchErrors2019","container-title":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","DOI":"10.18653/v1/D19-1331","event-place":"Hong Kong, China","event-title":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","issued":{"date-parts":[["2019"]]},"language":"en","page":"3354-3360","publisher":"Association for Computational Linguistics","publisher-place":"Hong Kong, China","source":"DOI.org (Crossref)","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?","title-short":"On NMT Search Errors and Model Errors","type":"paper-conference","URL":"https://www.aclweb.org/anthology/D19-1331"},{"id":"sunCHAIRClassifierHallucination2025","abstract":"This paper presents a supervised method for detecting hallucinations in large language models. By analyzing token scores (logitis) across layers of the LLaMA model (Touvron et al.), we derive a small set, aiming to reduce overfitting, of features-including maximum, minimum, mean, standard deviation, and slope. We use logistic regression for classification and validate the model on the TruthfulQA and MMLU datasets. The results demonstrate significant performance gains, especially in zero-shot scenarios, highlighting the effectiveness and potential for generalization.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Sun","given":"Ao"}],"citation-key":"sunCHAIRClassifierHallucination2025","DOI":"10.48550/arXiv.2501.02518","issued":{"date-parts":[["2025",1,22]]},"language":"en","number":"arXiv:2501.02518","publisher":"arXiv","source":"arXiv.org","title":"CHAIR -- Classifier of Hallucination as Improver","type":"article","URL":"http://arxiv.org/abs/2501.02518"},{"id":"thorneFEVERLargescaleDataset2018","abstract":"In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss $\\kappa$. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.","accessed":{"date-parts":[["2025",2,10]]},"author":[{"family":"Thorne","given":"James"},{"family":"Vlachos","given":"Andreas"},{"family":"Christodoulopoulos","given":"Christos"},{"family":"Mittal","given":"Arpit"}],"citation-key":"thorneFEVERLargescaleDataset2018","DOI":"10.48550/arXiv.1803.05355","issued":{"date-parts":[["2018",12,18]]},"number":"arXiv:1803.05355","publisher":"arXiv","source":"arXiv.org","title":"FEVER: a large-scale dataset for Fact Extraction and VERification","title-short":"FEVER","type":"article","URL":"http://arxiv.org/abs/1803.05355"},{"id":"varshneyStitchTimeSaves2023","abstract":"Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to ‘hallucinate’ which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model’s logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with GPT-3.5 (text-davinci-003) on the ‘article generation task’, we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of ∼ 88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. Then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another LLM from a different model family (Vicuna). In summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Varshney","given":"Neeraj"},{"family":"Yao","given":"Wenlin"},{"family":"Zhang","given":"Hongming"},{"family":"Chen","given":"Jianshu"},{"family":"Yu","given":"Dong"}],"citation-key":"varshneyStitchTimeSaves2023","DOI":"10.48550/arXiv.2307.03987","issued":{"date-parts":[["2023",8,12]]},"language":"en","number":"arXiv:2307.03987","publisher":"arXiv","source":"arXiv.org","title":"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation","title-short":"A Stitch in Time Saves Nine","type":"article","URL":"http://arxiv.org/abs/2307.03987"},{"id":"vaswaniAttentionAllYou2023","abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","accessed":{"date-parts":[["2025",2,6]]},"author":[{"family":"Vaswani","given":"Ashish"},{"family":"Shazeer","given":"Noam"},{"family":"Parmar","given":"Niki"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Jones","given":"Llion"},{"family":"Gomez","given":"Aidan N."},{"family":"Kaiser","given":"Lukasz"},{"family":"Polosukhin","given":"Illia"}],"citation-key":"vaswaniAttentionAllYou2023","DOI":"10.48550/arXiv.1706.03762","issued":{"date-parts":[["2023",8,2]]},"number":"arXiv:1706.03762","publisher":"arXiv","source":"arXiv.org","title":"Attention Is All You Need","type":"article","URL":"http://arxiv.org/abs/1706.03762"},{"id":"xuBenchmarkingBenchmarkLeakage2024","abstract":"Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models (LLMs). This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field’s healthy development. To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy—two simple and scalable metrics that gauge a model’s prediction precision on benchmark—to identify potential data leakages. By analyzing 31 LLMs under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons. These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations. Notably, we propose the “Benchmark Transparency Card” (Tab. 19) to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of LLMs. we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research.","accessed":{"date-parts":[["2025",1,24]]},"author":[{"family":"Xu","given":"Ruijie"},{"family":"Wang","given":"Zengzhi"},{"family":"Fan","given":"Run-Ze"},{"family":"Liu","given":"Pengfei"}],"citation-key":"xuBenchmarkingBenchmarkLeakage2024","DOI":"10.48550/arXiv.2404.18824","issued":{"date-parts":[["2024",4,29]]},"language":"en","number":"arXiv:2404.18824","publisher":"arXiv","source":"arXiv.org","title":"Benchmarking Benchmark Leakage in Large Language Models","type":"article","URL":"http://arxiv.org/abs/2404.18824"},{"id":"narayanDontGiveMe2018","type":"article","abstract":"We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question \"What is the article about?\". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.","DOI":"10.48550/arXiv.1808.08745","note":"arXiv:1808.08745 [cs]","number":"arXiv:1808.08745","publisher":"arXiv","source":"arXiv.org","title":"Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization","URL":"http://arxiv.org/abs/1808.08745","author":[{"family":"Narayan","given":"Shashi"},{"family":"Cohen","given":"Shay B."},{"family":"Lapata","given":"Mirella"}],"accessed":{"date-parts":[["2025",2,11]]},"issued":{"date-parts":[["2018",8,27]]},"citation-key":"narayanDontGiveMe2018","library":"My Library","citekey":"narayanDontGiveMe2018"},{"id":"gaoEmpowerYourModel2023","type":"article","abstract":"Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on \"why models are unable to compensate or strengthen their capabilities on their own\". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted on the challenging XSum dataset using LLaMa-7b model with context token length ranging from 800 to 1900. Results demonstrate that we achieve substantial improvements compared with the original generation results evaluated by GPT4.","DOI":"10.48550/arXiv.2307.13365","note":"arXiv:2307.13365 [cs]","number":"arXiv:2307.13365","publisher":"arXiv","source":"arXiv.org","title":"Empower Your Model with Longer and Better Context Comprehension","URL":"http://arxiv.org/abs/2307.13365","author":[{"family":"Gao","given":"Yifei"},{"family":"Wang","given":"Lei"},{"family":"Fang","given":"Jun"},{"family":"Hu","given":"Longhua"},{"family":"Cheng","given":"Jun"}],"accessed":{"date-parts":[["2025",2,11]]},"issued":{"date-parts":[["2023",7,27]]},"citation-key":"gaoEmpowerYourModel2023","library":"My Library","citekey":"gaoEmpowerYourModel2023"},{"id":"chenThoroughExaminationCNN2016","type":"article","abstract":"Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these two datasets, exceeding current state-of-the-art results by 7-10% and approaching what we believe is the ceiling for performance on this task.","DOI":"10.48550/arXiv.1606.02858","note":"arXiv:1606.02858 [cs]","number":"arXiv:1606.02858","publisher":"arXiv","source":"arXiv.org","title":"A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task","URL":"http://arxiv.org/abs/1606.02858","author":[{"family":"Chen","given":"Danqi"},{"family":"Bolton","given":"Jason"},{"family":"Manning","given":"Christopher D."}],"accessed":{"date-parts":[["2025",2,11]]},"issued":{"date-parts":[["2016",8,8]]},"citation-key":"chenThoroughExaminationCNN2016","library":"My Library","citekey":"chenThoroughExaminationCNN2016"},{"id":"hermannTeachingMachinesRead2015","type":"paper-conference","abstract":"Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.","container-title":"Advances in Neural Information Processing Systems","publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Teaching Machines to Read and Comprehend","URL":"https://papers.nips.cc/paper_files/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html","volume":"28","author":[{"family":"Hermann","given":"Karl Moritz"},{"family":"Kocisky","given":"Tomas"},{"family":"Grefenstette","given":"Edward"},{"family":"Espeholt","given":"Lasse"},{"family":"Kay","given":"Will"},{"family":"Suleyman","given":"Mustafa"},{"family":"Blunsom","given":"Phil"}],"accessed":{"date-parts":[["2025",2,11]]},"issued":{"date-parts":[["2015"]]},"citation-key":"hermannTeachingMachinesRead2015","library":"My Library","citekey":"hermannTeachingMachinesRead2015"}]