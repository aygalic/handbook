{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Thesis handbook","text":"<p>This handbook is design in two part that will be updated all along my thesis. </p> <p>The first part will be dedicated to grass root knowledge, covering fundamental concepts for subsequent research.</p> <p>The second part will cover everything related to my research.</p> <p>Part 1:</p> <ul> <li>Linear Algebra (upcoming)</li> <li>Machine Learning</li> <li>Statistics</li> <li>Coding practices</li> </ul> <p>Part 2: - Research</p>"},{"location":"grassroots/coding_practices/","title":"Coding practices basics","text":"<p>In this section, we cover coding practices and MLOps tool.</p> <ul> <li>1 - Python</li> <li>2 - MLOps</li> </ul>"},{"location":"grassroots/coding_practices/MLOps/","title":"MLOps and Machine Learning Engineering","text":""},{"location":"grassroots/coding_practices/MLOps/#machine-learning-pipeline-components","title":"Machine Learning Pipeline Components","text":""},{"location":"grassroots/coding_practices/MLOps/#version-control-for-ml","title":"Version Control for ML","text":"<ul> <li> <p>Code Version Control</p> <ul> <li>Git best practices for ML projects</li> <li>Branching strategies (feature branches, model versions)</li> <li><code>.gitignore</code> for ML projects (data, models, logs)</li> </ul> </li> <li> <p>Data Version Control</p> <ul> <li>DVC (Data Version Control) <pre><code># DVC example\ndvc init\ndvc add data/training_data.csv\ndvc remote add -d storage s3://mybucket/dvcstore\ndvc push\n</code></pre></li> <li>Benefits:<ul> <li>Track large files</li> <li>Reproduce experiments</li> <li>Share data between team members</li> </ul> </li> </ul> </li> <li> <p>Model Version Control</p> <ul> <li>MLflow model registry</li> <li>Model versioning strategies</li> <li>Metadata tracking</li> </ul> </li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#experiment-tracking","title":"Experiment Tracking","text":"<ul> <li> <p>Components to track:</p> <ul> <li>Model parameters</li> <li>Training metrics</li> <li>Dataset versions</li> <li>Environment details</li> </ul> </li> <li> <p>MLflow Example <pre><code>import mlflow\n\nwith mlflow.start_run():\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_param(\"epochs\", 100)\n\n    # Training loop\n    for epoch in range(100):\n        loss = train_epoch()\n        mlflow.log_metric(\"loss\", loss, step=epoch)\n\n    mlflow.sklearn.log_model(model, \"model\")\n</code></pre></p> </li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#model-serving","title":"Model Serving","text":""},{"location":"grassroots/coding_practices/MLOps/#rest-api-with-fastapi","title":"REST API with FastAPI","text":"<pre><code>from fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load(\"model.joblib\")\n\nclass PredictionInput(BaseModel):\n    features: List[float]\n\n@app.post(\"/predict\")\ndef predict(input_data: PredictionInput):\n    prediction = model.predict([input_data.features])\n    return {\"prediction\": prediction.tolist()}\n</code></pre>"},{"location":"grassroots/coding_practices/MLOps/#model-serving-patterns","title":"Model Serving Patterns","text":"<ul> <li> <p>Online Serving</p> <ul> <li>Real-time predictions</li> <li>Low latency requirements</li> <li>API endpoints</li> </ul> </li> <li> <p>Batch Serving</p> <ul> <li>Large-scale predictions</li> <li>Scheduled jobs</li> <li>Data pipeline integration</li> </ul> </li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#containerization-and-orchestration","title":"Containerization and Orchestration","text":""},{"location":"grassroots/coding_practices/MLOps/#docker-for-ml-applications","title":"Docker for ML Applications","text":"<ul> <li>Dockerfile Example <pre><code>FROM python:3.9-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY model/ model/\nCOPY src/ src/\n\nEXPOSE 8000\nCMD [\"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#kubernetes-for-ml-workloads","title":"Kubernetes for ML Workloads","text":"<ul> <li> <p>Key Concepts</p> <ul> <li>Pods vs Deployments</li> <li>Services</li> <li>ConfigMaps and Secrets</li> <li>Resource management</li> </ul> </li> <li> <p>Example Deployment <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: model-service\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: model-service\n  template:\n    metadata:\n      labels:\n        app: model-service\n    spec:\n      containers:\n      - name: model-service\n        image: model-service:v1\n        resources:\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 8000\n</code></pre></p> </li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"grassroots/coding_practices/MLOps/#model-monitoring","title":"Model Monitoring","text":"<ul> <li> <p>Metrics to Track</p> <ul> <li>Model performance (accuracy, F1, etc.)</li> <li>Prediction latency</li> <li>Feature drift</li> <li>Data quality</li> </ul> </li> <li> <p>Monitoring Setup Example <pre><code>from prometheus_client import Counter, Histogram\nimport time\n\nPREDICTION_LATENCY = Histogram(\n    'model_prediction_latency_seconds',\n    'Time spent processing prediction'\n)\nPREDICTION_COUNTER = Counter(\n    'model_predictions_total',\n    'Total number of predictions'\n)\n\n@PREDICTION_LATENCY.time()\ndef predict(features):\n    PREDICTION_COUNTER.inc()\n    return model.predict(features)\n</code></pre></p> </li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#data-quality-monitoring","title":"Data Quality Monitoring","text":"<ul> <li>Schema validation</li> <li>Feature statistics</li> <li>Data drift detection</li> <li>Example with Great Expectations: <pre><code>import great_expectations as ge\n\ndef validate_dataset(df):\n    expectation_suite = ge.core.ExpectationSuite(\n        expectation_suite_name=\"my_suite\"\n    )\n    df.expect_column_values_to_not_be_null(\"important_feature\")\n    df.expect_column_values_to_be_between(\n        \"numeric_feature\", min_value=0, max_value=100\n    )\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#cicd-for-machine-learning","title":"CI/CD for Machine Learning","text":""},{"location":"grassroots/coding_practices/MLOps/#continuous-integration","title":"Continuous Integration","text":"<ul> <li>Unit tests for ML code</li> <li>Integration tests</li> <li>Model validation tests</li> <li>Example test: <pre><code>def test_model_prediction_shape():\n    model = load_model()\n    X_test = load_test_data()\n    predictions = model.predict(X_test)\n    assert predictions.shape[0] == X_test.shape[0]\n    assert predictions.shape[1] == n_classes\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#continuous-deployment","title":"Continuous Deployment","text":"<ul> <li> <p>Deployment Strategies</p> <ul> <li>Blue-Green deployment</li> <li>Canary releases</li> <li>A/B testing</li> </ul> </li> <li> <p>Example Feature Flag Configuration <pre><code>def get_model_version(user_id):\n    if feature_flag.is_enabled('new_model', user_id):\n        return load_model('v2')\n    return load_model('v1')\n</code></pre></p> </li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":""},{"location":"grassroots/coding_practices/MLOps/#terraform-example","title":"Terraform Example","text":"<pre><code>resource \"aws_sagemaker_model\" \"example\" {\n  name               = \"my-model\"\n  execution_role_arn = aws_iam_role.example.arn\n\n  primary_container {\n    image          = \"${aws_ecr_repository.example.repository_url}:latest\"\n    model_data_url = \"s3://${aws_s3_bucket.example.bucket}/model.tar.gz\"\n  }\n}\n</code></pre>"},{"location":"grassroots/coding_practices/MLOps/#cloud-specific-services","title":"Cloud-Specific Services","text":""},{"location":"grassroots/coding_practices/MLOps/#aws-ml-stack","title":"AWS ML Stack","text":"<ul> <li>SageMaker</li> <li>Lambda</li> <li>ECS/EKS</li> <li>S3</li> <li>CloudWatch</li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#gcp-ml-stack","title":"GCP ML Stack","text":"<ul> <li>Vertex AI</li> <li>Cloud Run</li> <li>GKE</li> <li>Cloud Storage</li> <li>Cloud Monitoring</li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#best-practices","title":"Best Practices","text":""},{"location":"grassroots/coding_practices/MLOps/#production-ml-checklist","title":"Production ML Checklist","text":"<ol> <li>Model versioning and reproducibility</li> <li>Automated testing pipeline</li> <li>Monitoring and alerting</li> <li>Documentation</li> <li>Resource optimization</li> <li>Security considerations</li> <li>Compliance requirements</li> </ol>"},{"location":"grassroots/coding_practices/MLOps/#security-considerations","title":"Security Considerations","text":"<ul> <li>Model access control</li> <li>Data encryption</li> <li>API authentication</li> <li>Example secure endpoint: <pre><code>from fastapi import FastAPI, Security\nfrom fastapi.security import APIKeyHeader\n\napp = FastAPI()\napi_key_header = APIKeyHeader(name=\"X-API-Key\")\n\n@app.post(\"/predict\")\nasync def predict(\n    input_data: PredictionInput,\n    api_key: str = Security(api_key_header)\n):\n    if not is_valid_api_key(api_key):\n        raise HTTPException(status_code=403)\n    return {\"prediction\": model.predict([input_data.features])}\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Model quantization</li> <li>Batch prediction optimization</li> <li>Caching strategies</li> <li>Example caching: <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef get_prediction(feature_tuple):\n    return model.predict([feature_tuple])\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/MLOps/#interview-tips-for-mlops","title":"Interview Tips for MLOps","text":"<ol> <li>Be prepared to discuss end-to-end ML pipelines</li> <li>Understand scalability challenges</li> <li>Know common failure points and solutions</li> <li>Be familiar with cloud services</li> <li>Understanding of monitoring and observability</li> <li>Knowledge of deployment strategies</li> <li>Security best practices</li> </ol> <p>Remember to focus on: - Real-world problem-solving - Trade-offs between different approaches - Scalability considerations - Cost optimization - Team collaboration aspects</p>"},{"location":"grassroots/coding_practices/python/","title":"Python and Computer Science Fundamentals","text":""},{"location":"grassroots/coding_practices/python/#data-structures-in-python","title":"Data Structures in Python","text":""},{"location":"grassroots/coding_practices/python/#lists-vs-tuples","title":"Lists vs. Tuples","text":"<p>Lists and tuples are both sequence types in Python, but they have crucial differences:</p> <ul> <li> <p>Mutability</p> <ul> <li>Lists are mutable: Elements can be modified, added, or removed after creation</li> <li>Tuples are immutable: Once created, elements cannot be modified</li> </ul> </li> <li> <p>Syntax</p> <ul> <li>Lists use square brackets: <code>my_list = [1, 2, 3]</code></li> <li>Tuples use parentheses: <code>my_tuple = (1, 2, 3)</code></li> </ul> </li> <li> <p>Memory and Performance</p> <ul> <li>Tuples generally consume less memory than lists</li> <li>Tuples are slightly faster to access and iterate over</li> <li>Lists require extra memory for potential growth</li> </ul> </li> <li> <p>Use Cases</p> <ul> <li>Lists: When you need a collection that will change (e.g., growing dataset)</li> <li>Tuples: When data shouldn't change (e.g., coordinates, database records)</li> </ul> </li> </ul>"},{"location":"grassroots/coding_practices/python/#sets-and-dictionaries","title":"Sets and Dictionaries","text":""},{"location":"grassroots/coding_practices/python/#sets","title":"Sets","text":"<ul> <li>Unordered collections of unique elements</li> <li>Useful for:<ul> <li>Removing duplicates</li> <li>Set operations (union, intersection, difference)</li> <li>Membership testing (faster than lists)</li> </ul> </li> <li>Example: <pre><code>my_set = {1, 2, 3}\nother_set = {3, 4, 5}\nunion_set = my_set | other_set  # {1, 2, 3, 4, 5}\nintersection = my_set &amp; other_set  # {3}\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#dictionaries","title":"Dictionaries","text":"<ul> <li>Key-value pairs with O(1) lookup time</li> <li>Keys must be immutable (strings, numbers, tuples)</li> <li>Common operations: <pre><code># Creation and access\nmy_dict = {'a': 1, 'b': 2}\nvalue = my_dict['a']  # Direct access\nvalue = my_dict.get('c', 0)  # Safe access with default\n\n# Methods\nkeys = my_dict.keys()\nvalues = my_dict.values()\nitems = my_dict.items()\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#iteration-and-generators","title":"Iteration and Generators","text":""},{"location":"grassroots/coding_practices/python/#iterators","title":"Iterators","text":"<ul> <li>An iterator is an object that implements:<ul> <li><code>__iter__()</code>: Returns the iterator object itself</li> <li><code>__next__()</code>: Returns the next value or raises StopIteration</li> </ul> </li> <li>Used in for loops and comprehensions</li> <li>Example: <pre><code>class CountUpTo:\n    def __init__(self, max_value):\n        self.max_value = max_value\n        self.current = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.current &gt;= self.max_value:\n            raise StopIteration\n        self.current += 1\n        return self.current\n\n# Usage\nfor num in CountUpTo(3):\n    print(num)  # Prints 1, 2, 3\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#generators","title":"Generators","text":"<ul> <li>Special type of iterator created using functions with <code>yield</code></li> <li>Memory efficient: Values generated on-the-fly</li> <li>State is preserved between calls</li> <li>Example: <pre><code>def fibonacci(n):\n    a, b = 0, 1\n    for _ in range(n):\n        yield a\n        a, b = b, a + b\n\n# Usage\nfor num in fibonacci(5):\n    print(num)  # Prints 0, 1, 1, 2, 3\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#list-comprehensions-vs-generator-expressions","title":"List Comprehensions vs. Generator Expressions","text":"<pre><code># List comprehension (creates full list in memory)\nsquares_list = [x**2 for x in range(1000)]\n\n# Generator expression (generates values on demand)\nsquares_gen = (x**2 for x in range(1000))\n</code></pre>"},{"location":"grassroots/coding_practices/python/#memory-management","title":"Memory Management","text":""},{"location":"grassroots/coding_practices/python/#reference-counting-and-garbage-collection","title":"Reference Counting and Garbage Collection","text":"<ul> <li>Python uses reference counting for memory management</li> <li>Objects are deallocated when reference count reaches zero</li> <li>Circular references handled by garbage collector</li> <li>Example: <pre><code># Reference counting\nx = [1, 2, 3]  # ref count: 1\ny = x  # ref count: 2\ndel x  # ref count: 1\ndel y  # ref count: 0, list is deallocated\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#memory-model","title":"Memory Model","text":"<ul> <li>Everything is an object</li> <li>Variables are references to objects</li> <li>Assignment creates new references</li> <li>Example: <pre><code>a = [1, 2, 3]\nb = a  # Both variables reference the same list\nb.append(4)  # Modifies the list through either reference\nprint(a)  # [1, 2, 3, 4]\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#object-oriented-programming","title":"Object-Oriented Programming","text":""},{"location":"grassroots/coding_practices/python/#classes-and-objects","title":"Classes and Objects","text":"<ul> <li>Classes define blueprints for objects</li> <li>Objects are instances of classes</li> <li>Key concepts:<ul> <li>Encapsulation</li> <li>Inheritance</li> <li>Polymorphism</li> </ul> </li> <li>Example: <pre><code>class DataProcessor:\n    def __init__(self, data):\n        self.data = data\n\n    @property\n    def size(self):\n        return len(self.data)\n\n    def process(self):\n        raise NotImplementedError(\"Subclasses must implement process()\")\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#special-methods-magic-methods","title":"Special Methods (Magic Methods)","text":"<ul> <li>Define behavior for built-in operations</li> <li>Common methods:<ul> <li><code>__init__</code>: Constructor</li> <li><code>__str__</code>: String representation</li> <li><code>__len__</code>: Length</li> <li><code>__call__</code>: Make object callable</li> </ul> </li> <li>Example: <pre><code>class Dataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __str__(self):\n        return f\"Dataset with {len(self)} records\"\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#common-interview-topics","title":"Common Interview Topics","text":""},{"location":"grassroots/coding_practices/python/#time-and-space-complexity","title":"Time and Space Complexity","text":"<ul> <li>Big O notation basics</li> <li>Common complexities:<ul> <li>O(1): Constant time</li> <li>O(log n): Logarithmic</li> <li>O(n): Linear</li> <li>O(n log n): Log-linear</li> <li>O(n\u00b2): Quadratic</li> </ul> </li> </ul>"},{"location":"grassroots/coding_practices/python/#common-algorithms","title":"Common Algorithms","text":"<ul> <li>Sorting algorithms<ul> <li>Quick sort: Average O(n log n)</li> <li>Merge sort: Guaranteed O(n log n)</li> <li>Bubble sort: O(n\u00b2)</li> </ul> </li> <li>Search algorithms<ul> <li>Binary search: O(log n)</li> <li>Linear search: O(n)</li> </ul> </li> </ul>"},{"location":"grassroots/coding_practices/python/#threading-vs-multiprocessing","title":"Threading vs. Multiprocessing","text":"<ul> <li>Threading:<ul> <li>Shares memory space</li> <li>Good for I/O-bound tasks</li> <li>Limited by GIL for CPU-bound tasks</li> </ul> </li> <li>Multiprocessing:<ul> <li>Separate memory spaces</li> <li>Good for CPU-bound tasks</li> <li>Higher memory overhead</li> </ul> </li> </ul>"},{"location":"grassroots/coding_practices/python/#context-managers","title":"Context Managers","text":"<ul> <li>Used with <code>with</code> statement</li> <li>Ensures proper resource management</li> <li>Example: <pre><code>class DataConnection:\n    def __enter__(self):\n        print(\"Opening connection\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        print(\"Closing connection\")\n\n# Usage\nwith DataConnection() as conn:\n    # Work with connection\n    pass  # Connection automatically closed\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#decorators","title":"Decorators","text":"<ul> <li>Modify or enhance functions</li> <li>Common uses:<ul> <li>Logging</li> <li>Timing</li> <li>Access control</li> </ul> </li> <li>Example: <pre><code>def timer(func):\n    def wrapper(*args, **kwargs):\n        import time\n        start = time.time()\n        result = func(*args, **kwargs)\n        print(f\"Function took {time.time() - start:.2f} seconds\")\n        return result\n    return wrapper\n\n@timer\ndef slow_function():\n    import time\n    time.sleep(1)\n</code></pre></li> </ul>"},{"location":"grassroots/coding_practices/python/#best-practices-for-interviews","title":"Best Practices for Interviews","text":"<ol> <li>Always discuss trade-offs between different approaches</li> <li>Explain your thought process clearly</li> <li>Consider edge cases</li> <li>Be prepared to explain time and space complexity</li> <li>Have examples ready for each concept</li> <li>Be familiar with Python-specific implementations</li> <li>Understand how concepts relate to data science tasks</li> </ol> <p>Remember to practice implementing these concepts and be ready to explain them in the context of real-world data science problems.</p>"},{"location":"grassroots/machine_learning/","title":"Index","text":"<p>The following sections cover the basic ideas and tools for machine learning.</p> <ul> <li>1 - Theoretical Fundamentals</li> <li>2 - Python Fundamentals</li> <li>3 - Deep Learning</li> <li>4 - Transformers</li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/","title":"Machine Learning Theory: Classical Models and Methods","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#linear-models","title":"Linear Models","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#linear-regression","title":"Linear Regression","text":"<ul> <li>Concept: Models relationship between dependent variable y and independent variables X using a linear equation</li> <li>Mathematical Form: y = X\u03b2 + \u03b5</li> <li> <p>Loss Function: Mean Squared Error (MSE)</p> <ul> <li>L(\u03b2) = 1/n \u03a3(yi - xi\u1d40\u03b2)\u00b2</li> </ul> </li> <li> <p>Key Properties:</p> <ul> <li>Easy to interpret</li> <li>Fast to train</li> <li>Assumes linear relationship</li> <li>Sensitive to outliers</li> <li>Assumes independence of features</li> </ul> </li> <li> <p>Regularization Variants:</p> <ol> <li> <p>Ridge Regression (L2)</p> <ul> <li>Adds penalty term: \u03bb||\u03b2||\u2082\u00b2</li> <li>Shrinks coefficients toward zero</li> <li>Good for handling multicollinearity</li> </ul> </li> <li> <p>Lasso Regression (L1)</p> <ul> <li>Adds penalty term: \u03bb||\u03b2||\u2081</li> <li>Can zero out coefficients</li> <li>Performs feature selection</li> </ul> </li> <li> <p>Elastic Net</p> <ul> <li>Combines L1 and L2 penalties</li> <li>Penalty term: \u03bb\u2081||\u03b2||\u2081 + \u03bb\u2082||\u03b2||\u2082\u00b2</li> </ul> </li> </ol> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#logistic-regression","title":"Logistic Regression","text":"<ul> <li>Concept: Models probability of binary outcome using logistic function</li> <li>Mathematical Form: P(y=1|X) = 1/(1 + e^(-X\u03b2))</li> <li> <p>Loss Function: Binary Cross-Entropy</p> <ul> <li>L(\u03b2) = -1/n \u03a3(yi log(pi) + (1-yi)log(1-pi))</li> </ul> </li> <li> <p>Key Properties:</p> <ul> <li>Outputs probabilities between 0 and 1</li> <li>Can be extended to multiclass (one-vs-rest or multinomial)</li> <li>Assumes linear decision boundary</li> <li>Less sensitive to outliers than linear regression</li> </ul> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#support-vector-machines-svm","title":"Support Vector Machines (SVM)","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#linear-svm","title":"Linear SVM","text":"<ul> <li>Concept: Finds optimal hyperplane maximizing margin between classes</li> <li> <p>Mathematical Form: </p> <ul> <li>Primal: min 1/2||w||\u00b2 subject to yi(w\u1d40xi + b) \u2265 1</li> <li>Dual: max \u03a3\u03b1i - 1/2\u03a3(\u03b1i\u03b1jyiyj) <li> <p>Key Properties:</p> <ul> <li>Maximizes margin between classes</li> <li>Less prone to overfitting</li> <li>Sensitive to feature scaling</li> <li>Memory efficient (only stores support vectors)</li> </ul> </li>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#kernel-svm","title":"Kernel SVM","text":"<ul> <li>Concept: Projects data to higher dimensional space using kernel trick</li> <li> <p>Common Kernels:</p> <ol> <li> <p>RBF (Gaussian):</p> <ul> <li>K(x,y) = exp(-\u03b3||x-y||\u00b2)</li> <li>\u03b3 controls flexibility</li> </ul> </li> <li> <p>Polynomial:</p> <ul> <li>K(x,y) = (\u03b3 + r)^d <li>d is polynomial degree</li> <li> <p>Sigmoid:</p> <ul> <li>K(x,y) = tanh(\u03b3 + r) <li> <p>Key Properties:</p> <ul> <li>Can learn non-linear decision boundaries</li> <li>Computationally intensive for large datasets</li> <li>Requires careful kernel selection</li> </ul> </li>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#decision-trees","title":"Decision Trees","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#basic-decision-tree","title":"Basic Decision Tree","text":"<ul> <li>Concept: Recursively splits data based on feature values</li> <li> <p>Splitting Criteria:</p> <ol> <li> <p>Classification:</p> <ul> <li>Gini Impurity: 1 - \u03a3pi\u00b2</li> <li>Entropy: -\u03a3pi log(pi)</li> </ul> </li> <li> <p>Regression:</p> <ul> <li>MSE: 1/n \u03a3(yi - \u0233)\u00b2</li> <li>MAE: 1/n \u03a3|yi - \u0233|</li> </ul> </li> </ol> </li> <li> <p>Key Properties:</p> <ul> <li>Easy to interpret</li> <li>Can handle non-linear relationships</li> <li>No feature scaling needed</li> <li>Prone to overfitting</li> <li>Can capture feature interactions</li> </ul> </li> <li> <p>Hyperparameters:</p> <ul> <li>Maximum depth</li> <li>Minimum samples per leaf</li> <li>Maximum features</li> <li>Minimum impurity decrease</li> </ul> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#ensemble-methods","title":"Ensemble Methods","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#bagging-bootstrap-aggregating","title":"Bagging (Bootstrap Aggregating)","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#random-forest","title":"Random Forest","text":"<ul> <li>Concept: Averages predictions from multiple decorrelated trees</li> <li> <p>Key Components:</p> <ol> <li>Bootstrap sampling of data</li> <li>Random feature subset at each split</li> <li>Averaging (regression) or voting (classification)</li> </ol> </li> <li> <p>Key Properties:</p> <ul> <li>Reduces overfitting</li> <li>Lower variance than single trees</li> <li>Can estimate feature importance</li> <li>Parallelizable</li> </ul> </li> <li> <p>Mathematical Basis:</p> <ul> <li>Variance reduction through averaging</li> <li>Error rate bound related to tree correlation</li> </ul> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#boosting","title":"Boosting","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#adaboost-adaptive-boosting","title":"AdaBoost (Adaptive Boosting)","text":"<ul> <li>Concept: Sequentially builds weak learners focusing on misclassified samples</li> <li>Algorithm:<ol> <li>Initialize sample weights: wi = 1/n</li> <li>For each iteration t:<ul> <li>Train weak learner ht</li> <li>Calculate error: \u03b5t = \u03a3wi[yi \u2260 ht(xi)]</li> <li>Calculate weight: \u03b1t = 1/2 ln((1-\u03b5t)/\u03b5t)</li> <li>Update sample weights</li> </ul> </li> <li>Final prediction: H(x) = sign(\u03a3\u03b1tht(x))</li> </ol> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#gradient-boosting","title":"Gradient Boosting","text":"<ul> <li>Concept: Sequentially builds models to minimize loss function gradient</li> <li> <p>Algorithm:</p> <ol> <li>Initialize prediction: F\u2080(x) = argmin(\u03b3) \u03a3 L(yi,\u03b3)</li> <li>For each iteration m:<ul> <li>Calculate negative gradients: rim</li> <li>Fit base learner hm to rim</li> <li>Find optimal step size: \u03b3m</li> <li>Update model: Fm(x) = Fm-\u2081(x) + \u03b3m hm(x)</li> </ul> </li> </ol> </li> <li> <p>Variants:</p> <ol> <li> <p>XGBoost:</p> <ul> <li>Adds regularization term</li> <li>Second-order approximation</li> <li>Efficient implementation</li> </ul> </li> <li> <p>LightGBM:</p> <ul> <li>Gradient-based one-side sampling</li> <li>Exclusive feature bundling</li> <li>Leaf-wise tree growth</li> </ul> </li> <li> <p>CatBoost:</p> <ul> <li>Ordered boosting</li> <li>Efficient categorical feature handling</li> <li>Less prone to overfitting</li> </ul> </li> </ol> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#model-selection-and-complexity","title":"Model Selection and Complexity","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<ul> <li>Bias: Model's ability to capture underlying patterns</li> <li>Variance: Model's sensitivity to training data variation</li> <li>Total Error: Error = Bias\u00b2 + Variance + Irreducible Error</li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#model-complexity-control","title":"Model Complexity Control","text":"<ul> <li> <p>Regularization Methods:</p> <ol> <li>Parameter penalties (L1, L2)</li> <li>Early stopping</li> <li>Pruning (for trees)</li> <li>Dropout (for neural networks)</li> </ol> </li> <li> <p>Validation Strategies:</p> <ol> <li>Hold-out validation</li> <li>K-fold cross-validation</li> <li>Leave-one-out cross-validation</li> <li>Stratified cross-validation</li> </ol> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#feature-importance-and-model-interpretation","title":"Feature Importance and Model Interpretation","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#global-interpretation","title":"Global Interpretation","text":"<ul> <li>Feature Importance Methods:<ol> <li>Coefficient magnitude (linear models)</li> <li>Gini importance (trees)</li> <li>Permutation importance</li> <li>SHAP values</li> </ol> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#local-interpretation","title":"Local Interpretation","text":"<ul> <li>Methods:<ol> <li>LIME (Local Interpretable Model-agnostic Explanations)</li> <li>Individual SHAP values</li> <li>Local feature importance</li> </ol> </li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#assumptions-and-limitations","title":"Assumptions and Limitations","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#linear-models_1","title":"Linear Models","text":"<ul> <li>Linearity</li> <li>Independence</li> <li>Homoscedasticity</li> <li>Normal distribution (for inference)</li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#svm","title":"SVM","text":"<ul> <li>Feature scaling importance</li> <li>Kernel selection</li> <li>Computational complexity</li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#trees-and-ensembles","title":"Trees and Ensembles","text":"<ul> <li>Prone to overfitting (single trees)</li> <li>Memory requirements</li> <li>Training time (especially boosting)</li> <li>Black box nature (ensembles)</li> </ul>"},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#practical-considerations","title":"Practical Considerations","text":""},{"location":"grassroots/machine_learning/1_theoretical_fundamentals/#model-selection-guidelines","title":"Model Selection Guidelines","text":"<ol> <li> <p>Dataset Size:</p> <ul> <li>Small: Linear models, SVM</li> <li>Medium: Random Forest, Boosting</li> <li>Large: Gradient Boosting, Deep Learning</li> </ul> </li> <li> <p>Feature Types:</p> <ul> <li>Numerical: Any model</li> <li>Categorical: Trees, CatBoost</li> <li>Mixed: Trees, Ensemble methods</li> </ul> </li> <li> <p>Interpretability Requirements:</p> <ul> <li>High: Linear models, Single trees</li> <li>Medium: Random Forest</li> <li>Low: Complex ensembles</li> </ul> </li> <li> <p>Training Time Constraints:</p> <ul> <li>Fast: Linear models, Single trees</li> <li>Medium: Random Forest</li> <li>Slow: Boosting, Kernel SVM</li> </ul> </li> </ol> <p>Remember: 1. No free lunch theorem - no universally best model 2. Start simple, increase complexity as needed 3. Domain knowledge is crucial 4. Consider computational resources 5. Balance accuracy vs. interpretability</p>"},{"location":"grassroots/machine_learning/2_python_fundamentals/","title":"Machine Learning with Python","text":""},{"location":"grassroots/machine_learning/2_python_fundamentals/#data-processing-and-analysis","title":"Data Processing and Analysis","text":""},{"location":"grassroots/machine_learning/2_python_fundamentals/#loading-and-initial-data-exploration","title":"Loading and Initial Data Exploration","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Load data\ndf = pd.read_csv('data.csv')\n\n# Basic exploration\nprint(df.info())  # Data types and missing values\nprint(df.describe())  # Statistical summary\n\n# Missing values analysis\nmissing_values = df.isnull().sum() / len(df) * 100\nprint(missing_values[missing_values &gt; 0])\n\n# Correlation analysis\ncorrelation_matrix = df.corr()\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#data-cleaning-and-preprocessing","title":"Data Cleaning and Preprocessing","text":""},{"location":"grassroots/machine_learning/2_python_fundamentals/#handling-missing-values","title":"Handling Missing Values","text":"<pre><code># Strategy 1: Remove rows with missing values\ndf_cleaned = df.dropna()\n\n# Strategy 2: Imputation\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\n# Mean imputation\nimputer = SimpleImputer(strategy='mean')\ndf['column'] = imputer.fit_transform(df[['column']])\n\n# KNN imputation for more complex relationships\nimputer = KNNImputer(n_neighbors=5)\ndf_imputed = pd.DataFrame(\n    imputer.fit_transform(df),\n    columns=df.columns\n)\n\n# Strategy 3: Advanced imputation\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n# Uses the relationships between features to impute values\nimputer = IterativeImputer(random_state=42)\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#feature-scaling","title":"Feature Scaling","text":"<pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# Standardization (z-score)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Min-Max scaling\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Robust scaling (handles outliers better)\nscaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#handling-categorical-variables","title":"Handling Categorical Variables","text":"<pre><code># One-hot encoding\nX_encoded = pd.get_dummies(df, columns=['categorical_column'])\n\n# Label encoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['encoded_column'] = le.fit_transform(df['categorical_column'])\n\n# Ordinal encoding\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#feature-engineering","title":"Feature Engineering","text":""},{"location":"grassroots/machine_learning/2_python_fundamentals/#creating-new-features","title":"Creating New Features","text":"<pre><code># Polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\n# Interaction terms\ndf['interaction'] = df['feature1'] * df['feature2']\n\n# Date features\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day_of_week'] = df['date'].dt.dayofweek\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#feature-selection","title":"Feature Selection","text":"<pre><code>from sklearn.feature_selection import (\n    SelectKBest, f_classif, RFE, SelectFromModel\n)\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Statistical selection\nselector = SelectKBest(score_func=f_classif, k=10)\nX_selected = selector.fit_transform(X, y)\n\n# Recursive feature elimination\nrfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)\nX_selected = rfe.fit_transform(X, y)\n\n# L1-based selection\nfrom sklearn.linear_model import Lasso\nselector = SelectFromModel(Lasso())\nX_selected = selector.fit_transform(X, y)\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#model-training-and-evaluation","title":"Model Training and Evaluation","text":""},{"location":"grassroots/machine_learning/2_python_fundamentals/#train-test-split","title":"Train-Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\n# Simple split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Stratified split for classification\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#cross-validation","title":"Cross-Validation","text":"<pre><code>from sklearn.model_selection import (\n    cross_val_score, KFold, StratifiedKFold,\n    TimeSeriesSplit\n)\n\n# K-Fold CV\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=cv)\n\n# Stratified K-Fold for classification\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=cv)\n\n# Time series CV\ncv = TimeSeriesSplit(n_splits=5)\nscores = cross_val_score(model, X, y, cv=cv)\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#model-training-and-hyperparameter-tuning","title":"Model Training and Hyperparameter Tuning","text":"<pre><code>from sklearn.model_selection import (\n    GridSearchCV, RandomizedSearchCV\n)\n\n# Grid search\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\ngrid_search = GridSearchCV(\n    estimator=model,\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\ngrid_search.fit(X_train, y_train)\n\n# Random search\nfrom scipy.stats import randint\nparam_dist = {\n    'n_estimators': randint(100, 500),\n    'max_depth': randint(10, 50),\n    'min_samples_split': randint(2, 20)\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=model,\n    param_distributions=param_dist,\n    n_iter=100,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\nrandom_search.fit(X_train, y_train)\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#model-evaluation","title":"Model Evaluation","text":""},{"location":"grassroots/machine_learning/2_python_fundamentals/#classification-metrics","title":"Classification Metrics","text":"<pre><code>from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, confusion_matrix,\n    classification_report\n)\n\n# Basic metrics\ny_pred = model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.3f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.3f}\")\nprint(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.3f}\")\n\n# ROC-AUC for binary classification\ny_prob = model.predict_proba(X_test)[:, 1]\nprint(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.3f}\")\n\n# Detailed classification report\nprint(classification_report(y_test, y_pred))\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#regression-metrics","title":"Regression Metrics","text":"<pre><code>from sklearn.metrics import (\n    mean_squared_error, mean_absolute_error,\n    r2_score, mean_absolute_percentage_error\n)\n\n# Calculate metrics\ny_pred = model.predict(X_test)\nprint(f\"MSE: {mean_squared_error(y_test, y_pred):.3f}\")\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}\")\nprint(f\"MAE: {mean_absolute_error(y_test, y_pred):.3f}\")\nprint(f\"R2 Score: {r2_score(y_test, y_pred):.3f}\")\nprint(f\"MAPE: {mean_absolute_percentage_error(y_test, y_pred):.3f}\")\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#model-interpretability","title":"Model Interpretability","text":"<pre><code>import shap\nfrom sklearn.inspection import permutation_importance\n\n# SHAP values\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n# Feature importance\nfeature_imp = pd.DataFrame(\n    {'feature': X.columns, 'importance': model.feature_importances_}\n).sort_values('importance', ascending=False)\n\n# Permutation importance\nperm_importance = permutation_importance(model, X_test, y_test)\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#best-practices","title":"Best Practices","text":""},{"location":"grassroots/machine_learning/2_python_fundamentals/#pipeline-creation","title":"Pipeline Creation","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Define preprocessing for numerical and categorical columns\nnumeric_features = ['feature1', 'feature2']\ncategorical_features = ['feature3', 'feature4']\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Create full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier())\n])\n\n# Use pipeline\npipeline.fit(X_train, y_train)\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#model-persistence","title":"Model Persistence","text":"<pre><code>import joblib\n\n# Save model\njoblib.dump(model, 'model.joblib')\n\n# Load model\nloaded_model = joblib.load('model.joblib')\n</code></pre>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#cross-validation-best-practices","title":"Cross-Validation Best Practices","text":"<ol> <li> <p>Data Leakage Prevention <pre><code># Wrong: Scaling before splitting\nX_scaled = scaler.fit_transform(X)\nX_train, X_test = train_test_split(X_scaled, y)\n\n# Correct: Scaling after splitting\nX_train, X_test = train_test_split(X, y)\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre></p> </li> <li> <p>Nested Cross-Validation <pre><code>from sklearn.model_selection import cross_validate, KFold\n\ndef nested_cv(X, y, inner_cv, outer_cv, model, param_grid):\n    outer_scores = []\n\n    for train_idx, test_idx in outer_cv.split(X, y):\n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        # Inner CV for parameter tuning\n        grid_search = GridSearchCV(\n            estimator=model,\n            param_grid=param_grid,\n            cv=inner_cv\n        )\n        grid_search.fit(X_train, y_train)\n\n        # Evaluate on test set\n        score = grid_search.score(X_test, y_test)\n        outer_scores.append(score)\n\n    return outer_scores\n</code></pre></p> </li> </ol>"},{"location":"grassroots/machine_learning/2_python_fundamentals/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":"<ol> <li> <p>Class Imbalance <pre><code>from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# Handling imbalanced data\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# Or use pipeline with SMOTE\npipeline = ImbPipeline([\n    ('preprocessor', preprocessor),\n    ('smote', SMOTE()),\n    ('classifier', RandomForestClassifier())\n])\n</code></pre></p> </li> <li> <p>Feature Scaling for Tree-Based Models <pre><code># Tree-based models don't require scaling\ntree_pipeline = Pipeline([\n    ('preprocessor', categorical_transformer),  # Only handle categorical\n    ('classifier', RandomForestClassifier())\n])\n\n# Non-tree models need scaling\nlinear_pipeline = Pipeline([\n    ('preprocessor', preprocessor),  # Handle both numeric and categorical\n    ('classifier', LogisticRegression())\n])\n</code></pre></p> </li> </ol> <p>Remember: 1. Always split data before preprocessing 2. Use pipelines to prevent data leakage 3. Choose appropriate cross-validation strategy 4. Consider domain-specific requirements 5. Monitor for overfitting/underfitting 6. Document preprocessing steps and parameters</p>"},{"location":"grassroots/machine_learning/3_deep_learning/","title":"Deep Learning: From Theory to Practice","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#neural-network-foundations","title":"Neural Network Foundations","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#basic-architecture","title":"Basic Architecture","text":"<ul> <li> <p>Neurons (Units)</p> <ul> <li>Sum of weighted inputs: z = \u03a3(wixi) + b</li> <li>Activation function: a = f(z)</li> </ul> </li> <li> <p>Common Activation Functions</p> <ol> <li> <p>ReLU: f(x) = max(0, x)</p> <ul> <li>Fast to compute</li> <li>Helps with vanishing gradient</li> <li>Can cause \"dying ReLU\" problem</li> </ul> </li> <li> <p>Sigmoid: f(x) = 1/(1 + e^(-x))</p> <ul> <li>Outputs between [0,1]</li> <li>Can cause vanishing gradient</li> <li>Used in binary classification output</li> </ul> </li> <li> <p>Tanh: f(x) = (e^x - e^(-x))/(e^x + e^(-x))</p> <ul> <li>Outputs between [-1,1]</li> <li>Zero-centered</li> <li>Still has vanishing gradient issues</li> </ul> </li> <li> <p>LeakyReLU: f(x) = max(\u03b1x, x)</p> <ul> <li>Prevents dying ReLU</li> <li>\u03b1 typically small (e.g., 0.01)</li> </ul> </li> </ol> </li> </ul>"},{"location":"grassroots/machine_learning/3_deep_learning/#forward-propagation","title":"Forward Propagation","text":"<pre><code>class Layer:\n    def __init__(self, n_inputs, n_neurons):\n        self.weights = np.random.randn(n_inputs, n_neurons) * 0.01\n        self.biases = np.zeros((1, n_neurons))\n\n    def forward(self, inputs):\n        self.output = np.dot(inputs, self.weights) + self.biases\n        return self.output\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#backpropagation","title":"Backpropagation","text":"<ul> <li> <p>Chain Rule Application</p> <ul> <li>\u2202L/\u2202w = \u2202L/\u2202a * \u2202a/\u2202z * \u2202z/\u2202w</li> <li>\u2202L/\u2202b = \u2202L/\u2202a * \u2202a/\u2202z * \u2202z/\u2202b</li> </ul> </li> <li> <p>Gradient Descent</p> <ul> <li>w = w - \u03b1 * \u2202L/\u2202w</li> <li>b = b - \u03b1 * \u2202L/\u2202b</li> </ul> </li> </ul>"},{"location":"grassroots/machine_learning/3_deep_learning/#loss-functions-and-optimization","title":"Loss Functions and Optimization","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#common-loss-functions","title":"Common Loss Functions","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#classification","title":"Classification","text":"<pre><code>def binary_cross_entropy(y_true, y_pred):\n    return -np.mean(y_true * np.log(y_pred) + \n                   (1 - y_true) * np.log(1 - y_pred))\n\ndef categorical_cross_entropy(y_true, y_pred):\n    return -np.sum(y_true * np.log(y_pred), axis=1).mean()\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#regression","title":"Regression","text":"<pre><code>def mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef huber_loss(y_true, y_pred, delta=1.0):\n    error = y_true - y_pred\n    is_small_error = np.abs(error) &lt;= delta\n    squared_loss = 0.5 * error ** 2\n    linear_loss = delta * np.abs(error) - 0.5 * delta ** 2\n    return np.mean(np.where(is_small_error, squared_loss, linear_loss))\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#optimizers","title":"Optimizers","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#sgd-with-momentum","title":"SGD with Momentum","text":"<pre><code>class SGDMomentum:\n    def __init__(self, learning_rate=0.01, momentum=0.9):\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.velocity = None\n\n    def update(self, params, grads):\n        if self.velocity is None:\n            self.velocity = {k: np.zeros_like(v) \n                           for k, v in params.items()}\n\n        for key in params:\n            self.velocity[key] = (self.momentum * self.velocity[key] - \n                                self.learning_rate * grads[key])\n            params[key] += self.velocity[key]\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#adam","title":"Adam","text":"<pre><code>class Adam:\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = None\n        self.v = None\n        self.t = 0\n\n    def update(self, params, grads):\n        if self.m is None:\n            self.m = {k: np.zeros_like(v) for k, v in params.items()}\n            self.v = {k: np.zeros_like(v) for k, v in params.items()}\n\n        self.t += 1\n\n        for key in params:\n            # Momentum\n            self.m[key] = (self.beta1 * self.m[key] + \n                          (1 - self.beta1) * grads[key])\n            # RMSprop\n            self.v[key] = (self.beta2 * self.v[key] + \n                          (1 - self.beta2) * grads[key]**2)\n\n            # Bias correction\n            m_hat = self.m[key] / (1 - self.beta1**self.t)\n            v_hat = self.v[key] / (1 - self.beta2**self.t)\n\n            # Update parameters\n            params[key] -= (self.learning_rate * m_hat / \n                          (np.sqrt(v_hat) + self.epsilon))\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#deep-learning-architectures","title":"Deep Learning Architectures","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#convolutional-neural-networks-cnn","title":"Convolutional Neural Networks (CNN)","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#basic-components","title":"Basic Components","text":"<ol> <li> <p>Convolutional Layer <pre><code>class Conv2D:\n    def __init__(self, filters, kernel_size, stride=1, padding='valid'):\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, input):\n        # Implement convolution operation\n        pass\n</code></pre></p> </li> <li> <p>Pooling Layer <pre><code>class MaxPool2D:\n    def __init__(self, pool_size=2, stride=None):\n        self.pool_size = pool_size\n        self.stride = stride or pool_size\n\n    def forward(self, input):\n        # Implement max pooling\n        pass\n</code></pre></p> </li> </ol>"},{"location":"grassroots/machine_learning/3_deep_learning/#common-architectures","title":"Common Architectures","text":"<ol> <li> <p>VGG</p> <ul> <li>Stack of 3x3 convolutions</li> <li>Max pooling reduces spatial dimensions</li> <li>Dense layers for classification</li> </ul> </li> <li> <p>ResNet</p> <ul> <li>Skip connections</li> <li>Batch normalization</li> <li>Deep architecture (50+ layers)</li> </ul> </li> </ol>"},{"location":"grassroots/machine_learning/3_deep_learning/#recurrent-neural-networks-rnn","title":"Recurrent Neural Networks (RNN)","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#basic-rnn-cell","title":"Basic RNN Cell","text":"<pre><code>class RNNCell:\n    def __init__(self, input_size, hidden_size):\n        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.Wxh = np.random.randn(input_size, hidden_size) * 0.01\n        self.bh = np.zeros((1, hidden_size))\n\n    def forward(self, x, h_prev):\n        return np.tanh(np.dot(x, self.Wxh) + \n                      np.dot(h_prev, self.Whh) + self.bh)\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#lstm-cell","title":"LSTM Cell","text":"<pre><code>class LSTMCell:\n    def __init__(self, input_size, hidden_size):\n        # Initialize gates\n        self.forget_gate = Layer(input_size + hidden_size, hidden_size)\n        self.input_gate = Layer(input_size + hidden_size, hidden_size)\n        self.output_gate = Layer(input_size + hidden_size, hidden_size)\n        self.cell_gate = Layer(input_size + hidden_size, hidden_size)\n\n    def forward(self, x, h_prev, c_prev):\n        # Concatenate input and previous hidden state\n        combined = np.concatenate((x, h_prev), axis=1)\n\n        # Gate computations\n        f = sigmoid(self.forget_gate.forward(combined))\n        i = sigmoid(self.input_gate.forward(combined))\n        o = sigmoid(self.output_gate.forward(combined))\n        c_tilde = np.tanh(self.cell_gate.forward(combined))\n\n        # Update cell and hidden state\n        c = f * c_prev + i * c_tilde\n        h = o * np.tanh(c)\n\n        return h, c\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#transformers","title":"Transformers","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#self-attention-mechanism","title":"Self-Attention Mechanism","text":"<pre><code>def scaled_dot_product_attention(Q, K, V, mask=None):\n    d_k = K.shape[-1]\n    attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)\n\n    if mask is not None:\n        attention_scores += (mask * -1e9)\n\n    attention_weights = softmax(attention_scores)\n    output = np.dot(attention_weights, V)\n\n    return output, attention_weights\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#multi-head-attention","title":"Multi-Head Attention","text":"<pre><code>class MultiHeadAttention:\n    def __init__(self, d_model, num_heads):\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert d_model % num_heads == 0\n\n        self.depth = d_model // num_heads\n\n        self.wq = Layer(d_model, d_model)\n        self.wk = Layer(d_model, d_model)\n        self.wv = Layer(d_model, d_model)\n\n        self.dense = Layer(d_model, d_model)\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#advanced-topics","title":"Advanced Topics","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#regularization-techniques","title":"Regularization Techniques","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#dropout","title":"Dropout","text":"<pre><code>def dropout(x, keep_prob):\n    mask = np.random.binomial(1, keep_prob, size=x.shape)\n    return (x * mask) / keep_prob\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#batch-normalization","title":"Batch Normalization","text":"<pre><code>class BatchNorm:\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.running_mean = np.zeros(num_features)\n        self.running_var = np.ones(num_features)\n\n    def forward(self, x, training=True):\n        if training:\n            mean = np.mean(x, axis=0)\n            var = np.var(x, axis=0)\n\n            # Update running statistics\n            self.running_mean = ((1 - self.momentum) * self.running_mean + \n                               self.momentum * mean)\n            self.running_var = ((1 - self.momentum) * self.running_var + \n                              self.momentum * var)\n        else:\n            mean = self.running_mean\n            var = self.running_var\n\n        # Normalize\n        x_norm = (x - mean) / np.sqrt(var + self.eps)\n        return x_norm\n</code></pre>"},{"location":"grassroots/machine_learning/3_deep_learning/#transfer-learning","title":"Transfer Learning","text":"<ol> <li> <p>Feature Extraction</p> <ul> <li>Freeze pre-trained layers</li> <li>Add new classification head</li> <li>Train only new layers</li> </ul> </li> <li> <p>Fine-Tuning</p> <ul> <li>Start with pre-trained model</li> <li>Unfreeze some/all layers</li> <li>Train with small learning rate</li> </ul> </li> </ol>"},{"location":"grassroots/machine_learning/3_deep_learning/#model-deployment","title":"Model Deployment","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#model-optimization","title":"Model Optimization","text":"<ol> <li> <p>Quantization</p> <ul> <li>Reduce precision of weights</li> <li>Integer-only inference</li> <li>Reduced memory footprint</li> </ul> </li> <li> <p>Pruning</p> <ul> <li>Remove unnecessary connections</li> <li>Maintain accuracy</li> <li>Reduce model size</li> </ul> </li> <li> <p>Knowledge Distillation</p> <ul> <li>Teacher-student architecture</li> <li>Transfer knowledge to smaller model</li> <li>Maintain performance</li> </ul> </li> </ol>"},{"location":"grassroots/machine_learning/3_deep_learning/#best-practices","title":"Best Practices","text":""},{"location":"grassroots/machine_learning/3_deep_learning/#training-tips","title":"Training Tips","text":"<ol> <li> <p>Learning Rate Selection</p> <ul> <li>Use learning rate finder</li> <li>Implement learning rate scheduling</li> <li>Monitor training dynamics</li> </ul> </li> <li> <p>Batch Size Selection</p> <ul> <li>Memory constraints</li> <li>Training stability</li> <li>Generalization impact</li> </ul> </li> <li> <p>Initialization</p> <ul> <li>Xavier/Glorot initialization</li> <li>He initialization for ReLU</li> <li>Careful with deep networks</li> </ul> </li> </ol>"},{"location":"grassroots/machine_learning/3_deep_learning/#debugging-strategies","title":"Debugging Strategies","text":"<ol> <li> <p>Loss Not Decreasing</p> <ul> <li>Check gradients</li> <li>Verify data preprocessing</li> <li>Inspect learning rate</li> </ul> </li> <li> <p>Overfitting</p> <ul> <li>Add regularization</li> <li>Increase training data</li> <li>Reduce model capacity</li> </ul> </li> <li> <p>Underfitting</p> <ul> <li>Increase model capacity</li> <li>Reduce regularization</li> <li>Check for data issues</li> </ul> </li> </ol>"},{"location":"grassroots/machine_learning/3_deep_learning/#performance-optimization","title":"Performance Optimization","text":"<ol> <li> <p>Memory Management</p> <ul> <li>Gradient accumulation</li> <li>Mixed precision training</li> <li>Efficient data loading</li> </ul> </li> <li> <p>Training Speed</p> <ul> <li>Parallel processing</li> <li>GPU utilization</li> <li>Efficient data pipeline</li> </ul> </li> </ol> <p>Remember: 1. Start simple and gradually increase complexity 2. Monitor training metrics carefully 3. Use appropriate regularization techniques 4. Consider computational resources 5. Test thoroughly before deployment</p>"},{"location":"grassroots/machine_learning/4_transformers/","title":"Understanding Transformers and the Attention Mechanism","text":""},{"location":"grassroots/machine_learning/4_transformers/#introduction","title":"Introduction","text":"<p>Transformers have revolutionized natural language processing and many other domains since their introduction in the \"Attention Is All You Need\" paper (Vaswhatever et al., 2017). This chapter breaks down their core mechanisms and explains how they process information.</p>"},{"location":"grassroots/machine_learning/4_transformers/#core-architecture","title":"Core Architecture","text":"<p>The Transformer architecture consists of two main components: 1. An encoder that processes the input sequence 2. A decoder that generates the output sequence</p> <p>Both components are built from stacks of identical layers, each containing: - Multi-head self-attention mechanisms - Feed-forward neural networks - Layer normalization and residual connections</p>"},{"location":"grassroots/machine_learning/4_transformers/#the-attention-mechanism-step-by-step","title":"The Attention Mechanism: Step by Step","text":""},{"location":"grassroots/machine_learning/4_transformers/#1-input-embedding-and-positional-encoding","title":"1. Input Embedding and Positional Encoding","text":"<ul> <li>First, input tokens are converted to embeddings (dense vectors)</li> <li>Positional encodings are added to maintain sequence order information</li> <li>These use sine and cosine functions of different frequencies:   pos_encoding(pos, 2i) = sin(pos/10000^(2i/d_model))   pos_encoding(pos, 2i+1) = cos(pos/10000^(2i/d_model))</li> </ul>"},{"location":"grassroots/machine_learning/4_transformers/#2-query-key-and-value-computation","title":"2. Query, Key, and Value Computation","text":"<p>The attention mechanism uses three main components: - Query (Q): What we're looking for - Key (K): What we match against - Value (V): What we want to retrieve</p> <p>These are computed by multiplying the input embeddings by learned weight matrices: <pre><code>Q = input \u00d7 W_Q\nK = input \u00d7 W_K\nV = input \u00d7 W_V\n</code></pre></p>"},{"location":"grassroots/machine_learning/4_transformers/#3-attention-score-calculation","title":"3. Attention Score Calculation","text":"<p>The attention scores are computed in several steps:</p> <p>a. Compatibility Function <pre><code>scores = (Q \u00d7 K^T) / \u221ad_k\n</code></pre> where d_k is the dimension of the key vectors, and the division by \u221ad_k prevents values from becoming too large for the softmax.</p> <p>b. Masking (if necessary) In decoder self-attention or for padding tokens: - Set unwanted attention scores to -infinity - This ensures these positions have zero weight after softmax</p> <p>c. Softmax Application <pre><code>attention_weights = softmax(scores)\n</code></pre> This normalizes the scores into probabilities that sum to 1.</p>"},{"location":"grassroots/machine_learning/4_transformers/#4-computing-the-output","title":"4. Computing the Output","text":"<p>The final attention output is computed by: <pre><code>attention_output = attention_weights \u00d7 V\n</code></pre></p>"},{"location":"grassroots/machine_learning/4_transformers/#5-multi-head-attention","title":"5. Multi-Head Attention","text":"<p>Instead of performing attention once, Transformers use multiple attention heads:</p> <ol> <li>Each head has its own Q, K, V projections</li> <li>Compute attention independently for each head</li> <li>Concatenate the results</li> <li>Project back to the model dimension: <pre><code>MultiHead(Q, K, V) = Concat(head_1, ..., head_h) \u00d7 W_O\nwhere head_i = Attention(Q \u00d7 W_Q_i, K \u00d7 W_K_i, V \u00d7 W_V_i)\n</code></pre></li> </ol>"},{"location":"grassroots/machine_learning/4_transformers/#feed-forward-networks-and-residual-connections","title":"Feed-Forward Networks and Residual Connections","text":"<p>After attention, each sub-layer contains:</p> <ol> <li> <p>Layer Normalization <pre><code>norm(x) = \u03b3 \u00d7 (x - \u03bc)/\u03c3 + \u03b2\n</code></pre> where \u03bc and \u03c3 are mean and standard deviation, \u03b3 and \u03b2 are learned parameters</p> </li> <li> <p>Feed-Forward Network <pre><code>FFN(x) = GELU(xW_1 + b_1)W_2 + b_2\n</code></pre></p> </li> </ol> <p>where:</p> <p>GELU is the Gaussian Error Linear Unit activation function W_1 transforms from model dimension to intermediate dimension (typically 4x larger) W_2 transforms back to model dimension b_1 and b_2 are bias terms</p> <p>Note: While earlier transformer implementations used ReLU (max(0, x)), modern transformers typically use GELU activation functions which provide smoother gradients.</p> <ol> <li>Residual Connection <pre><code>output = LayerNorm(x + Sublayer(x))\n</code></pre></li> </ol>"},{"location":"grassroots/machine_learning/4_transformers/#practical-example-computing-self-attention","title":"Practical Example: Computing Self-Attention","text":"<p>Let's walk through a simplified example with 4-dimensional vectors:</p> <pre><code># Input embeddings (batch_size=1, seq_len=3, d_model=4)\nX = [\n    [1, 0, 1, 0],  # First token\n    [0, 1, 0, 1],  # Second token\n    [1, 1, 0, 0]   # Third token\n]\n\n# Project to Q, K, V (simplified weights)\nW_Q = W_K = W_V = I  # Identity matrix for simplicity\nQ = K = V = X        # In practice, these would be different\n\n# Compute attention scores\nscores = Q \u00d7 K^T / \u221a4\n# Results in:\n# [[1.0, 0.5, 0.5],\n#  [0.5, 1.0, 0.5],\n#  [0.5, 0.5, 1.0]]\n\n# Apply softmax\nweights = softmax(scores)\n# Results in attention weights focusing most on matching positions\n\n# Final output\noutput = weights \u00d7 V\n# Each position now contains a weighted mix of values\n</code></pre>"},{"location":"grassroots/machine_learning/4_transformers/#training-and-optimization","title":"Training and Optimization","text":"<p>Transformers are typically trained using: - Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03b5 = 10^-9 - Learning rate scheduling with warmup - Dropout for regularization - Label smoothing</p> <p>The loss function varies by task: - Cross-entropy for classification/language modeling - Custom losses for specific applications</p>"},{"location":"grassroots/machine_learning/4_transformers/#common-variations-and-improvements","title":"Common Variations and Improvements","text":"<p>Modern transformers often include: - Relative positional encoding - Sparse attention patterns - Parameter sharing across layers - Efficient attention approximations</p> <p>These modifications help with: - Longer sequence handling - Computational efficiency - Task-specific requirements</p>"},{"location":"grassroots/machine_learning/4_transformers/#references","title":"References","text":"<p>Note: While this technical explanation is accurate, you should verify specific implementation details against primary sources and documentation for your particular use case.</p>"},{"location":"grassroots/statistics/","title":"Core Statistical Concepts for Data Science Interviews","text":"<p>In this section, we introduce the main tool for statistical analysis.</p> <ul> <li>1 - Descriptive statistiques</li> <li>2 - Probability distributions</li> <li>3 - Computational rules</li> <li>4 - Statistical inference</li> <li>5 - Regression analysis</li> <li>6 - Experimental design</li> <li>7 - Bayesian statistics</li> <li>8 - Advanced topics</li> </ul>"},{"location":"grassroots/statistics/#common-interview-questions","title":"Common Interview Questions","text":"When would you use a t-test vs z-test? <p>Let me break down the key differences between t-tests and z-tests and explain when to use each one:</p> How do you handle missing data? <p>Content for handling missing data...</p> What's the difference between correlation and causation? <p>Content for correlation vs causation...</p> Explain the central limit theorem <p>Content for central limit theorem...</p> How do you detect and handle outliers? <p>Content for detecting and handling outliers...</p> What's the difference between parametric and non-parametric tests? <p>Content for parametric vs non-parametric tests...</p> How do you choose between different types of regression? <p>Content for choosing between different types of regression...</p> Explain cross-validation and its importance <p>Content for cross-validation and its importance...</p>"},{"location":"grassroots/statistics/#key-distinctions","title":"Key Distinctions","text":"<ol> <li> <p>Population Standard Deviation</p> <ul> <li>Z-test: Used when we KNOW the population standard deviation (\u03c3)</li> <li>T-test: Used when we DON'T know the population standard deviation and must estimate it using sample standard deviation (s)</li> </ul> </li> <li> <p>Sample Size</p> <ul> <li>Z-test: Generally used for large samples (n &gt; 30)</li> <li>T-test: Better for small samples (n &lt; 30) because it accounts for extra uncertainty in estimating the standard deviation</li> </ul> </li> <li> <p>Distribution</p> <ul> <li>Z-test: Assumes data follows a normal distribution</li> <li>T-test: Uses Student's t-distribution, which has heavier tails than normal distribution to account for additional uncertainty</li> </ul> </li> </ol>"},{"location":"grassroots/statistics/#practical-examples","title":"Practical Examples","text":"<p>Scenario 1: Quality Control in Large Manufacturing Plant - Testing widget weights - Years of historical data available - Known population standard deviation - Large daily samples \u2192 Use Z-test because you know \u03c3 and have large samples</p> <p>Scenario 2: Medical Research Study - Testing new drug effectiveness - Small patient group (n=20) - No known population standard deviation - Need to estimate variance from sample \u2192 Use T-test because of small sample size and unknown \u03c3</p>"},{"location":"grassroots/statistics/#mathematical-details","title":"Mathematical Details","text":""},{"location":"grassroots/statistics/#heavier-tails-of-t-distribution","title":"Heavier Tails of T-Distribution","text":"<p>The t-distribution is defined as:</p> \\[ t = \\frac{Z}{\\sqrt{V/n}} \\] <p>where: - Z follows N(0,1) - V follows \u03c7\u00b2(n) (chi-square with n degrees of freedom) - Z and V are independent</p> <p>The probability density function (PDF) of t-distribution with \u03bd degrees of freedom is:</p> \\[ f(t) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}(1 + \\frac{t^2}{\\nu})^{-\\frac{\\nu + 1}{2}} \\] <p>Compare this to the normal distribution PDF:</p> \\[ f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}} \\]"},{"location":"grassroots/statistics/#convergence-to-normal-distribution","title":"Convergence to Normal Distribution","text":"<p>As n \u2192 \u221e, we can prove convergence using:</p> <ol> <li> <p>The Central Limit Theorem for V/n:     $$     \\frac{V/n - 1}{\\sqrt{2/n}} \\xrightarrow{d} N(0,1)     $$</p> </li> <li> <p>Therefore, as n \u2192 \u221e:     $$     \\sqrt{V/n} \\xrightarrow{p} 1     $$</p> </li> <li> <p>Thus:     $$     t = \\frac{Z}{\\sqrt{V/n}} \\xrightarrow{d} Z \\sim N(0,1)     $$</p> </li> </ol>"},{"location":"grassroots/statistics/#power-analysis","title":"Power Analysis","text":"<p>The power function for a z-test: $$ \\pi_Z(\\mu) = 1 - \\Phi(z_{\u03b1/2} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}) + \\Phi(-z_{\u03b1/2} - \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt{n}}) $$</p> <p>The power function for a t-test: $$ \\pi_T(\\mu) = 1 - F_t(t_{\u03b1/2,n-1} - \\frac{\\mu - \\mu_0}{s/\\sqrt{n}}) + F_t(-t_{\u03b1/2,n-1} - \\frac{\\mu - \\mu_0}{s/\\sqrt{n}}) $$</p> <p>where: - \u03a6 is the standard normal CDF - F_t is the t-distribution CDF - z_{\u03b1/2} is the normal critical value - t_{\u03b1/2,n-1} is the t critical value</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/","title":"Descriptive Statistics","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#measures-of-central-tendency","title":"Measures of Central Tendency","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-arithmetic-mean-am","title":"1. Arithmetic Mean (AM)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#definition","title":"Definition","text":"<p>For a set of numbers {x\u2081, x\u2082, ..., x\u2099}: \\(AM = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#properties","title":"Properties","text":"<ol> <li> <p>Minimizes squared deviations:    AM minimizes \\(\\sum_{i=1}^{n} (x_i - \\mu)^2\\)</p> </li> <li> <p>Linear property:    \\(\\bar{ax + b} = a\\bar{x} + b\\)</p> </li> <li> <p>Effect of outliers:    Highly sensitive to extreme values</p> </li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/#use-cases","title":"Use Cases","text":"<ul> <li>Default measure of central tendency</li> <li>When data points contribute equally</li> <li>Financial calculations (e.g., average daily returns)</li> <li>Physical measurements with random errors</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#relationship-with-standard-deviation","title":"Relationship with Standard Deviation","text":"<p>For a dataset {x\u2081, ..., x\u2099}: \\(\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\frac{1}{n}\\sum_{i=1}^{n} x_i^2 - (\\bar{x})^2\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#other-properties","title":"Other properties","text":"<ul> <li>Median: Robustness to outliers</li> <li>Mode: Use in categorical data</li> <li>Relationship between mean, median, mode in skewed distributions</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#measures-of-dispersion","title":"Measures of Dispersion","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-variance-and-standard-deviation","title":"1. Variance and Standard Deviation","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#population-variance-2","title":"Population Variance (\u03c3\u00b2)","text":"<p>\\(\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#sample-variance-s2","title":"Sample Variance (s\u00b2)","text":"<p>\\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#properties_1","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#computational-formula","title":"Computational Formula:","text":"<p>\\(s^2 = \\frac{1}{n-1}(\\sum_{i=1}^{n} x_i^2 - \\frac{1}{n}(\\sum_{i=1}^{n} x_i)^2)\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#algebraic-properties","title":"Algebraic Properties:","text":"<p>$$ Var(aX + b) = a^2Var(X) $$    $$ Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y) $$    For independent X,Y: $$ Var(X + Y) = Var(X) + Var(Y) $$</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#standard-deviation","title":"Standard Deviation:","text":"<p>\\(\\sigma = \\sqrt{\\sigma^2}\\) or \\(s = \\sqrt{s^2}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#use-cases_1","title":"Use Cases","text":"<ul> <li>Most common measure of variability</li> <li>Optimal for normal distributions</li> <li>Input for many statistical procedures</li> <li>Basis for least squares estimation</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#2-range-and-interquartile-range-iqr","title":"2. Range and Interquartile Range (IQR)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#range","title":"Range","text":"<p>\\(R = x_{max} - x_{min}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#interquartile-range","title":"Interquartile Range","text":"<p>\\(IQR = Q_3 - Q_1\\)</p> <p>where: - Q\u2081 is the 25th percentile - Q\u2083 is the 75th percentile</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#properties_2","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-outlier-detection","title":"1. Outlier Detection:","text":"<ul> <li>Lower fence: \\(Q_1 - 1.5 \u00d7 IQR\\)</li> <li>Upper fence: \\(Q_3 + 1.5 \u00d7 IQR\\)</li> <li>Points beyond fences are potential outliers</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#2-relationship-to-standard-deviation","title":"2. Relationship to Standard Deviation:","text":"<p>For normal distribution:    - \\(IQR \u2248 1.349\\sigma\\)    - Range \u2248 4\u03c3 (for moderate sample sizes)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#use-cases_2","title":"Use Cases","text":"<ul> <li>Non-parametric analyses</li> <li>Robust statistics</li> <li>Box plots</li> <li>Quick dispersion estimates</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#3-coefficient-of-variation-cv","title":"3. Coefficient of Variation (CV)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#definition_1","title":"Definition","text":"<p>\\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#properties_3","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-scale-independence","title":"1. Scale Independence:","text":"<ul> <li>Unitless measure</li> <li>Allows comparison across different scales</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#2-limitations","title":"2. Limitations:","text":"<ul> <li>Only meaningful for ratio scales</li> <li>Not suitable when mean \u2248 0</li> <li>Not defined for negative values</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#use-cases_3","title":"Use Cases","text":"<ul> <li>Comparing variability across different units</li> <li>Quality control</li> <li>Investment risk assessment</li> <li>Biological variation studies</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#4-mean-absolute-deviation-mad","title":"4. Mean Absolute Deviation (MAD)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#population-mad","title":"Population MAD","text":"<p>\\(MAD = \\frac{1}{N}\\sum_{i=1}^{N} |x_i - \\mu|\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#sample-mad","title":"Sample MAD","text":"<p>\\(MAD = \\frac{1}{n}\\sum_{i=1}^{n} |x_i - \\bar{x}|\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#properties_4","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-relationship-to-standard-deviation","title":"1. Relationship to Standard Deviation:","text":"<p>For normal distribution:    \\(MAD \u2248 0.8\\sigma\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#2-robustness","title":"2. Robustness:","text":"<ul> <li>Less sensitive to outliers than variance</li> <li>L1 norm vs L2 norm for variance</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#use-cases_4","title":"Use Cases","text":"<ul> <li>Robust statistics</li> <li>Financial risk measures</li> <li>Time series analysis</li> <li>Bio statistics</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#5-covariance-matrix","title":"5. Covariance Matrix (\u03a3)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#definition_2","title":"Definition","text":"<p>For random vectors X = [X\u2081, ..., X\u209a]: \\(\\Sigma_{ij} = Cov(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#sample-covariance-matrix-s","title":"Sample Covariance Matrix (S)","text":"<p>\\(S_{ij} = \\frac{1}{n-1}\\sum_{k=1}^{n} (x_{ki} - \\bar{x}_i)(x_{kj} - \\bar{x}_j)\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#properties_5","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-matrix-properties","title":"1. Matrix Properties:","text":"<ul> <li>Symmetric: \\(\\Sigma_{ij} = \\Sigma_{ji}\\)</li> <li>Positive semi-definite</li> <li>Diagonal elements are variances</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#2-eigendecomposition","title":"2. Eigendecomposition:","text":"<p>\\(\\Sigma = PDP^T\\)    where:    - D: diagonal matrix of eigenvalues    - P: matrix of eigenvectors</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#use-cases_5","title":"Use Cases","text":"<ul> <li>Principal Component Analysis</li> <li>Multivariate analysis</li> <li>Portfolio optimization</li> <li>Machine learning algorithms</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#6-relationships-and-comparisons","title":"6. Relationships and Comparisons","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-robustness-hierarchy-most-to-least-robust","title":"1. Robustness Hierarchy (most to least robust):","text":"<ol> <li>IQR</li> <li>MAD</li> <li>Standard Deviation</li> <li>Range</li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/#2-efficiency-under-normality-most-to-least-efficient","title":"2. Efficiency under Normality (most to least efficient):","text":"<ol> <li>Standard Deviation</li> <li>MAD</li> <li>IQR</li> <li>Range</li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/#3-computational-complexity","title":"3. Computational Complexity:","text":"<ul> <li>Range: O(n)</li> <li>IQR: O(n log n)</li> <li>MAD: O(n)</li> <li>Variance: O(n)</li> <li>Covariance Matrix: O(n\u00b2)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#7-selection-guidelines","title":"7. Selection Guidelines","text":"<p>Use Standard Deviation when:    - Data is approximately normal    - Need mathematical tractability    - Working with inferential statistics</p> <p>Use IQR when:    - Data has outliers    - Distribution is skewed    - Need robust measure</p> <p>Use CV when:    - Comparing different scales    - Working with strictly positive data    - Need relative variation</p> <p>Use MAD when:    - Need robust measure    - Working with time series    - Dealing with non-normal data</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#expected-value-variance-relationship","title":"Expected Value - Variance Relationship","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-basic-definitions","title":"1. Basic Definitions","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#discrete-random-variables","title":"Discrete Random Variables","text":"<p>For a discrete random variable X: \\(E[X] = \\sum_{x} x \\cdot P(X = x)\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#continuous-random-variables","title":"Continuous Random Variables","text":"<p>For a continuous random variable X: \\(E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) dx\\)</p> <p>where f(x) is the probability density function.</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#alternative-notation","title":"Alternative Notation","text":"<ul> <li>\\(E[X]\\)</li> <li>\\(\\mu\\)</li> <li>\\(\\mu_X\\)</li> <li>\\(\\langle X \\rangle\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#2-fundamental-properties","title":"2. Fundamental Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#linearity-of-expectation","title":"Linearity of Expectation","text":"<ol> <li>\\(E[aX + b] = aE[X] + b\\)</li> <li>\\(E[X + Y] = E[X] + E[Y]\\)</li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/#proof-for-discrete-case","title":"Proof for Discrete Case:","text":"<pre><code>E[aX + b] = \u2211(ax + b)P(X = x)\n          = a\u2211xP(X = x) + b\u2211P(X = x)\n          = aE[X] + b    (since \u2211P(X = x) = 1)\n</code></pre>"},{"location":"grassroots/statistics/1_descriptive_statistics/#law-of-total-expectation","title":"Law of Total Expectation","text":"<p>\\(E[X] = E[E[X|Y]]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#proof","title":"Proof:","text":"<p>For discrete case: <pre><code>E[E[X|Y]] = \u2211_y E[X|Y=y]P(Y=y)\n          = \u2211_y \u2211_x xP(X=x|Y=y)P(Y=y)\n          = \u2211_x x\u2211_y P(X=x|Y=y)P(Y=y)\n          = \u2211_x xP(X=x)\n          = E[X]\n</code></pre></p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#3-special-cases-and-important-results","title":"3. Special Cases and Important Results","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#functions-of-random-variables","title":"Functions of Random Variables","text":"<p>For any function g(X): \\(E[g(X)] = \\begin{cases}  \\sum_x g(x)P(X=x) &amp; \\text{discrete case} \\\\ \\int_{-\\infty}^{\\infty} g(x)f(x)dx &amp; \\text{continuous case} \\end{cases}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#expected-value-of-product","title":"Expected Value of Product","text":"<p>For any two random variables X and Y: \\(E[XY] = E[X]E[Y] + Cov(X,Y)\\)</p> <p>For independent variables: \\(E[XY] = E[X]E[Y]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#4-sample-mean-properties","title":"4. Sample Mean Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#sample-mean","title":"Sample Mean","text":"<p>For observations {X\u2081, ..., X\u2099}: \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#properties-of-sample-mean","title":"Properties of Sample Mean","text":"<ol> <li>\\(E[\\bar{X}] = \\mu\\) (unbiased)</li> <li>\\(Var(\\bar{X}) = \\frac{\\sigma^2}{n}\\)</li> <li>\\(SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\)</li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/#5-important-inequalities","title":"5. Important Inequalities","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#jensens-inequality","title":"Jensen's Inequality","text":"<p>For convex function g: \\(g(E[X]) \\leq E[g(X)]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#markovs-inequality","title":"Markov's Inequality","text":"<p>For non-negative X and a &gt; 0: \\(P(X \\geq a) \\leq \\frac{E[X]}{a}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#chebyshevs-inequality","title":"Chebyshev's Inequality","text":"<p>For any k &gt; 0: \\(P(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#6-relationship-with-characteristic-function","title":"6. Relationship with Characteristic Function","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#definition_3","title":"Definition","text":"<p>\\(\\phi_X(t) = E[e^{itX}]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#moments-from-characteristic-function","title":"Moments from Characteristic Function","text":"<p>\\(E[X^n] = i^{-n}\\frac{d^n}{dt^n}\\phi_X(t)|_{t=0}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#7-computational-considerations","title":"7. Computational Considerations","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#monte-carlo-estimation","title":"Monte Carlo Estimation","text":"<p>\\(E[X] \\approx \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#error-in-estimation","title":"Error in Estimation","text":"<p>Standard Error = \\(\\frac{\\sigma}{\\sqrt{n}}\\)</p> <p>95% Confidence Interval \u2248 \\(\\bar{X} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#distribution-characteristics","title":"Distribution Characteristics","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#1-descriptive-measures-of-distributions","title":"1. Descriptive Measures of Distributions","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#skewness","title":"Skewness","text":"<p>Skewness measures the asymmetry of a probability distribution: - Left-skewed (Negative): Tail extends to the left, mean &lt; median - Right-skewed (Positive): Tail extends to the right, mean &gt; median - Symmetric: Mean = median, skewness = 0</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#kurtosis","title":"Kurtosis","text":"<p>Kurtosis measures the \"tailedness\" of a probability distribution: - Heavy-tailed (Leptokurtic): Higher kurtosis, more extreme values - Light-tailed (Platykurtic): Lower kurtosis, fewer extreme values - Normal (Mesokurtic): Reference kurtosis = 3 for normal distribution</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#moments-of-a-distribution","title":"Moments of a Distribution","text":"<p>Moments are quantitative measures that describe the shape of a distribution: - First Moment: Mean (Expected Value) - Second Moment: Variance and Standard Deviation - Third Moment: Related to Skewness - Fourth Moment: Related to Kurtosis</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#2-common-distribution-expected-values-and-variances","title":"2. Common Distribution Expected Values and Variances","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#normal-distribution-n-2","title":"Normal Distribution N(\u03bc, \u03c3\u00b2)","text":"<ul> <li>Expected Value: \\(E[X] = \\mu\\)</li> <li>Variance: \\(Var(X) = \\sigma^2\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#exponential-distribution-rate","title":"Exponential Distribution (rate \u03bb)","text":"<ul> <li>Expected Value: \\(E[X] = \\frac{1}{\\lambda}\\)</li> <li>Variance: \\(Var(X) = \\frac{1}{\\lambda^2}\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#poisson-distribution-rate","title":"Poisson Distribution (rate \u03bb)","text":"<ul> <li>Expected Value: \\(E[X] = \\lambda\\)</li> <li>Variance: \\(Var(X) = \\lambda\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#binomial-distribution-n-trials-probability-p","title":"Binomial Distribution (n trials, probability p)","text":"<ul> <li>Expected Value: \\(E[X] = np\\)</li> <li>Variance: \\(Var(X) = np(1-p)\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#geometric-distribution-probability-p","title":"Geometric Distribution (probability p)","text":"<ul> <li>Expected Value: \\(E[X] = \\frac{1}{p}\\)</li> <li>Variance: \\(Var(X) = \\frac{1-p}{p^2}\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/#3-higher-moments","title":"3. Higher Moments","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/#kth-moment","title":"kth Moment","text":"<p>The kth moment about zero (raw moment) is defined as:</p> <p>\\(E[X^k] = \\begin{cases} \\sum_x x^k P(X=x) &amp; \\text{discrete case} \\\\ \\int_{-\\infty}^{\\infty} x^k f(x)dx &amp; \\text{continuous case} \\end{cases}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#central-moments","title":"Central Moments","text":"<p>The kth central moment (moment about the mean) is defined as:</p> <p>\\(E[(X-\\mu)^k] = \\begin{cases} \\sum_x (x-\\mu)^k P(X=x) &amp; \\text{discrete case} \\\\ \\int_{-\\infty}^{\\infty} (x-\\mu)^k f(x)dx &amp; \\text{continuous case} \\end{cases}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/#important-relations","title":"Important Relations","text":"<ul> <li>First Central Moment: \\(E[X-\\mu] = 0\\)</li> <li>Second Central Moment: \\(E[(X-\\mu)^2] = Var(X)\\)</li> <li>Third Standardized Moment: \\(E[(X-\\mu)^3]/\\sigma^3\\) (Skewness)</li> <li>Fourth Standardized Moment: \\(E[(X-\\mu)^4]/\\sigma^4\\) (Kurtosis)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/central_tendency/","title":"Central tendency","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/central_tendency/#measures-of-central-tendency","title":"Measures of Central Tendency","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/central_tendency/#1-arithmetic-mean-am","title":"1. Arithmetic Mean (AM)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/central_tendency/#definition","title":"Definition","text":"<p>For a set of numbers {x\u2081, x\u2082, ..., x\u2099}: \\(AM = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/central_tendency/#properties","title":"Properties","text":"<ol> <li> <p>Minimizes squared deviations:    AM minimizes \\(\\sum_{i=1}^{n} (x_i - \\mu)^2\\)</p> </li> <li> <p>Linear property:    \\(\\bar{ax + b} = a\\bar{x} + b\\)</p> </li> <li> <p>Effect of outliers:    Highly sensitive to extreme values</p> </li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/central_tendency/#use-cases","title":"Use Cases","text":"<ul> <li>Default measure of central tendency</li> <li>When data points contribute equally</li> <li>Financial calculations (e.g., average daily returns)</li> <li>Physical measurements with random errors</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/central_tendency/#relationship-with-standard-deviation","title":"Relationship with Standard Deviation","text":"<p>For a dataset {x\u2081, ..., x\u2099}: \\(\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\frac{1}{n}\\sum_{i=1}^{n} x_i^2 - (\\bar{x})^2\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/central_tendency/#other-properties","title":"Other properties","text":"<ul> <li>Median: Robustness to outliers</li> <li>Mode: Use in categorical data</li> <li>Relationship between mean, median, mode in skewed distributions</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/","title":"Dispersion","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#measures-of-dispersion","title":"Measures of Dispersion","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#1-variance-and-standard-deviation","title":"1. Variance and Standard Deviation","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#population-variance-2","title":"Population Variance (\u03c3\u00b2)","text":"<p>\\(\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#sample-variance-s2","title":"Sample Variance (s\u00b2)","text":"<p>\\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#properties","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#computational-formula","title":"Computational Formula:","text":"<p>\\(s^2 = \\frac{1}{n-1}(\\sum_{i=1}^{n} x_i^2 - \\frac{1}{n}(\\sum_{i=1}^{n} x_i)^2)\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#algebraic-properties","title":"Algebraic Properties:","text":"<p>$$ Var(aX + b) = a^2Var(X) $$    $$ Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y) $$    For independent X,Y: $$ Var(X + Y) = Var(X) + Var(Y) $$</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#standard-deviation","title":"Standard Deviation:","text":"<p>\\(\\sigma = \\sqrt{\\sigma^2}\\) or \\(s = \\sqrt{s^2}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#use-cases","title":"Use Cases","text":"<ul> <li>Most common measure of variability</li> <li>Optimal for normal distributions</li> <li>Input for many statistical procedures</li> <li>Basis for least squares estimation</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#2-range-and-interquartile-range-iqr","title":"2. Range and Interquartile Range (IQR)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#range","title":"Range","text":"<p>\\(R = x_{max} - x_{min}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#interquartile-range","title":"Interquartile Range","text":"<p>\\(IQR = Q_3 - Q_1\\)</p> <p>where: - Q\u2081 is the 25th percentile - Q\u2083 is the 75th percentile</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#properties_1","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#1-outlier-detection","title":"1. Outlier Detection:","text":"<ul> <li>Lower fence: \\(Q_1 - 1.5 \u00d7 IQR\\)</li> <li>Upper fence: \\(Q_3 + 1.5 \u00d7 IQR\\)</li> <li>Points beyond fences are potential outliers</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#2-relationship-to-standard-deviation","title":"2. Relationship to Standard Deviation:","text":"<p>For normal distribution:    - \\(IQR \u2248 1.349\\sigma\\)    - Range \u2248 4\u03c3 (for moderate sample sizes)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#use-cases_1","title":"Use Cases","text":"<ul> <li>Non-parametric analyses</li> <li>Robust statistics</li> <li>Box plots</li> <li>Quick dispersion estimates</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#3-coefficient-of-variation-cv","title":"3. Coefficient of Variation (CV)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#definition","title":"Definition","text":"<p>\\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#properties_2","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#1-scale-independence","title":"1. Scale Independence:","text":"<ul> <li>Unitless measure</li> <li>Allows comparison across different scales</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#2-limitations","title":"2. Limitations:","text":"<ul> <li>Only meaningful for ratio scales</li> <li>Not suitable when mean \u2248 0</li> <li>Not defined for negative values</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#use-cases_2","title":"Use Cases","text":"<ul> <li>Comparing variability across different units</li> <li>Quality control</li> <li>Investment risk assessment</li> <li>Biological variation studies</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#4-mean-absolute-deviation-mad","title":"4. Mean Absolute Deviation (MAD)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#population-mad","title":"Population MAD","text":"<p>\\(MAD = \\frac{1}{N}\\sum_{i=1}^{N} |x_i - \\mu|\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#sample-mad","title":"Sample MAD","text":"<p>\\(MAD = \\frac{1}{n}\\sum_{i=1}^{n} |x_i - \\bar{x}|\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#properties_3","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#1-relationship-to-standard-deviation","title":"1. Relationship to Standard Deviation:","text":"<p>For normal distribution:    \\(MAD \u2248 0.8\\sigma\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#2-robustness","title":"2. Robustness:","text":"<ul> <li>Less sensitive to outliers than variance</li> <li>L1 norm vs L2 norm for variance</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#use-cases_3","title":"Use Cases","text":"<ul> <li>Robust statistics</li> <li>Financial risk measures</li> <li>Time series analysis</li> <li>Bio statistics</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#5-covariance-matrix","title":"5. Covariance Matrix (\u03a3)","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#definition_1","title":"Definition","text":"<p>For random vectors X = [X\u2081, ..., X\u209a]: \\(\\Sigma_{ij} = Cov(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#sample-covariance-matrix-s","title":"Sample Covariance Matrix (S)","text":"<p>\\(S_{ij} = \\frac{1}{n-1}\\sum_{k=1}^{n} (x_{ki} - \\bar{x}_i)(x_{kj} - \\bar{x}_j)\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#properties_4","title":"Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#1-matrix-properties","title":"1. Matrix Properties:","text":"<ul> <li>Symmetric: \\(\\Sigma_{ij} = \\Sigma_{ji}\\)</li> <li>Positive semi-definite</li> <li>Diagonal elements are variances</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#2-eigendecomposition","title":"2. Eigendecomposition:","text":"<p>\\(\\Sigma = PDP^T\\)    where:    - D: diagonal matrix of eigenvalues    - P: matrix of eigenvectors</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#use-cases_4","title":"Use Cases","text":"<ul> <li>Principal Component Analysis</li> <li>Multivariate analysis</li> <li>Portfolio optimization</li> <li>Machine learning algorithms</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#6-relationships-and-comparisons","title":"6. Relationships and Comparisons","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#1-robustness-hierarchy-most-to-least-robust","title":"1. Robustness Hierarchy (most to least robust):","text":"<ol> <li>IQR</li> <li>MAD</li> <li>Standard Deviation</li> <li>Range</li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#2-efficiency-under-normality-most-to-least-efficient","title":"2. Efficiency under Normality (most to least efficient):","text":"<ol> <li>Standard Deviation</li> <li>MAD</li> <li>IQR</li> <li>Range</li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#3-computational-complexity","title":"3. Computational Complexity:","text":"<ul> <li>Range: O(n)</li> <li>IQR: O(n log n)</li> <li>MAD: O(n)</li> <li>Variance: O(n)</li> <li>Covariance Matrix: O(n\u00b2)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/dispersion/#7-selection-guidelines","title":"7. Selection Guidelines","text":"<p>Use Standard Deviation when:    - Data is approximately normal    - Need mathematical tractability    - Working with inferential statistics</p> <p>Use IQR when:    - Data has outliers    - Distribution is skewed    - Need robust measure</p> <p>Use CV when:    - Comparing different scales    - Working with strictly positive data    - Need relative variation</p> <p>Use MAD when:    - Need robust measure    - Working with time series    - Dealing with non-normal data</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/","title":"Distribution Characteristics","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#1-descriptive-measures-of-distributions","title":"1. Descriptive Measures of Distributions","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#skewness","title":"Skewness","text":"<p>Skewness measures the asymmetry of a probability distribution: - Left-skewed (Negative): Tail extends to the left, mean &lt; median - Right-skewed (Positive): Tail extends to the right, mean &gt; median - Symmetric: Mean = median, skewness = 0</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#kurtosis","title":"Kurtosis","text":"<p>Kurtosis measures the \"tailedness\" of a probability distribution: - Heavy-tailed (Leptokurtic): Higher kurtosis, more extreme values - Light-tailed (Platykurtic): Lower kurtosis, fewer extreme values - Normal (Mesokurtic): Reference kurtosis = 3 for normal distribution</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#moments-of-a-distribution","title":"Moments of a Distribution","text":"<p>Moments are quantitative measures that describe the shape of a distribution: - First Moment: Mean (Expected Value) - Second Moment: Variance and Standard Deviation - Third Moment: Related to Skewness - Fourth Moment: Related to Kurtosis</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#2-common-distribution-expected-values-and-variances","title":"2. Common Distribution Expected Values and Variances","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#normal-distribution-n-2","title":"Normal Distribution N(\u03bc, \u03c3\u00b2)","text":"<ul> <li>Expected Value: \\(E[X] = \\mu\\)</li> <li>Variance: \\(Var(X) = \\sigma^2\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#exponential-distribution-rate","title":"Exponential Distribution (rate \u03bb)","text":"<ul> <li>Expected Value: \\(E[X] = \\frac{1}{\\lambda}\\)</li> <li>Variance: \\(Var(X) = \\frac{1}{\\lambda^2}\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#poisson-distribution-rate","title":"Poisson Distribution (rate \u03bb)","text":"<ul> <li>Expected Value: \\(E[X] = \\lambda\\)</li> <li>Variance: \\(Var(X) = \\lambda\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#binomial-distribution-n-trials-probability-p","title":"Binomial Distribution (n trials, probability p)","text":"<ul> <li>Expected Value: \\(E[X] = np\\)</li> <li>Variance: \\(Var(X) = np(1-p)\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#geometric-distribution-probability-p","title":"Geometric Distribution (probability p)","text":"<ul> <li>Expected Value: \\(E[X] = \\frac{1}{p}\\)</li> <li>Variance: \\(Var(X) = \\frac{1-p}{p^2}\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#3-higher-moments","title":"3. Higher Moments","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#kth-moment","title":"kth Moment","text":"<p>The kth moment about zero (raw moment) is defined as:</p> <p>\\(E[X^k] = \\begin{cases} \\sum_x x^k P(X=x) &amp; \\text{discrete case} \\\\ \\int_{-\\infty}^{\\infty} x^k f(x)dx &amp; \\text{continuous case} \\end{cases}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#central-moments","title":"Central Moments","text":"<p>The kth central moment (moment about the mean) is defined as:</p> <p>\\(E[(X-\\mu)^k] = \\begin{cases} \\sum_x (x-\\mu)^k P(X=x) &amp; \\text{discrete case} \\\\ \\int_{-\\infty}^{\\infty} (x-\\mu)^k f(x)dx &amp; \\text{continuous case} \\end{cases}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/distribution_characteristics/#important-relations","title":"Important Relations","text":"<ul> <li>First Central Moment: \\(E[X-\\mu] = 0\\)</li> <li>Second Central Moment: \\(E[(X-\\mu)^2] = Var(X)\\)</li> <li>Third Standardized Moment: \\(E[(X-\\mu)^3]/\\sigma^3\\) (Skewness)</li> <li>Fourth Standardized Moment: \\(E[(X-\\mu)^4]/\\sigma^4\\) (Kurtosis)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/","title":"Expected Value - Variance Relationship","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#1-basic-definitions","title":"1. Basic Definitions","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#discrete-random-variables","title":"Discrete Random Variables","text":"<p>For a discrete random variable X: \\(E[X] = \\sum_{x} x \\cdot P(X = x)\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#continuous-random-variables","title":"Continuous Random Variables","text":"<p>For a continuous random variable X: \\(E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) dx\\)</p> <p>where f(x) is the probability density function.</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#alternative-notation","title":"Alternative Notation","text":"<ul> <li>\\(E[X]\\)</li> <li>\\(\\mu\\)</li> <li>\\(\\mu_X\\)</li> <li>\\(\\langle X \\rangle\\)</li> </ul>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#2-fundamental-properties","title":"2. Fundamental Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#linearity-of-expectation","title":"Linearity of Expectation","text":"<ol> <li>\\(E[aX + b] = aE[X] + b\\)</li> <li>\\(E[X + Y] = E[X] + E[Y]\\)</li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#proof-for-discrete-case","title":"Proof for Discrete Case:","text":"<pre><code>E[aX + b] = \u2211(ax + b)P(X = x)\n          = a\u2211xP(X = x) + b\u2211P(X = x)\n          = aE[X] + b    (since \u2211P(X = x) = 1)\n</code></pre>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#law-of-total-expectation","title":"Law of Total Expectation","text":"<p>\\(E[X] = E[E[X|Y]]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#proof","title":"Proof:","text":"<p>For discrete case: <pre><code>E[E[X|Y]] = \u2211_y E[X|Y=y]P(Y=y)\n          = \u2211_y \u2211_x xP(X=x|Y=y)P(Y=y)\n          = \u2211_x x\u2211_y P(X=x|Y=y)P(Y=y)\n          = \u2211_x xP(X=x)\n          = E[X]\n</code></pre></p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#3-special-cases-and-important-results","title":"3. Special Cases and Important Results","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#functions-of-random-variables","title":"Functions of Random Variables","text":"<p>For any function g(X): \\(E[g(X)] = \\begin{cases}  \\sum_x g(x)P(X=x) &amp; \\text{discrete case} \\\\ \\int_{-\\infty}^{\\infty} g(x)f(x)dx &amp; \\text{continuous case} \\end{cases}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#expected-value-of-product","title":"Expected Value of Product","text":"<p>For any two random variables X and Y: \\(E[XY] = E[X]E[Y] + Cov(X,Y)\\)</p> <p>For independent variables: \\(E[XY] = E[X]E[Y]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#4-sample-mean-properties","title":"4. Sample Mean Properties","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#sample-mean","title":"Sample Mean","text":"<p>For observations {X\u2081, ..., X\u2099}: \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#properties-of-sample-mean","title":"Properties of Sample Mean","text":"<ol> <li>\\(E[\\bar{X}] = \\mu\\) (unbiased)</li> <li>\\(Var(\\bar{X}) = \\frac{\\sigma^2}{n}\\)</li> <li>\\(SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\)</li> </ol>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#5-important-inequalities","title":"5. Important Inequalities","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#jensens-inequality","title":"Jensen's Inequality","text":"<p>For convex function g: \\(g(E[X]) \\leq E[g(X)]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#markovs-inequality","title":"Markov's Inequality","text":"<p>For non-negative X and a &gt; 0: \\(P(X \\geq a) \\leq \\frac{E[X]}{a}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#chebyshevs-inequality","title":"Chebyshev's Inequality","text":"<p>For any k &gt; 0: \\(P(|X-\\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#6-relationship-with-characteristic-function","title":"6. Relationship with Characteristic Function","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#definition","title":"Definition","text":"<p>\\(\\phi_X(t) = E[e^{itX}]\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#moments-from-characteristic-function","title":"Moments from Characteristic Function","text":"<p>\\(E[X^n] = i^{-n}\\frac{d^n}{dt^n}\\phi_X(t)|_{t=0}\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#7-computational-considerations","title":"7. Computational Considerations","text":""},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#monte-carlo-estimation","title":"Monte Carlo Estimation","text":"<p>\\(E[X] \\approx \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)</p>"},{"location":"grassroots/statistics/1_descriptive_statistics/expected_value_variance_relationship/#error-in-estimation","title":"Error in Estimation","text":"<p>Standard Error = \\(\\frac{\\sigma}{\\sqrt{n}}\\)</p> <p>95% Confidence Interval \u2248 \\(\\bar{X} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/","title":"Probability Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/#continuous-probability-distributions","title":"Continuous Probability Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/#normalgaussian-distribution","title":"Normal/Gaussian Distribution","text":"<p>The normal distribution is fundamental to statistics, arising naturally in many phenomena due to the Central Limit Theorem. Its mathematical elegance and theoretical properties make it the cornerstone of statistical inference.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The probability density function is given by:</p> <p>\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)</p> <p>where: - \u03bc determines the center (location) - \u03c3 controls the spread (scale)</p> <p>The standard normal distribution (\u03bc = 0, \u03c3 = 1) simplifies this to:</p> <p>\\(f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/#key-properties","title":"Key Properties","text":"<ol> <li>Symmetry: The distribution is perfectly symmetric around \u03bc</li> <li>Empirical Rule:</li> <li>\u03bc \u00b1 \u03c3 contains \u2248 68% of data</li> <li>\u03bc \u00b1 2\u03c3 contains \u2248 95% of data</li> <li>\u03bc \u00b1 3\u03c3 contains \u2248 99.7% of data</li> </ol>"},{"location":"grassroots/statistics/2_probability_distributions/#implementation","title":"Implementation","text":"<p>For practical applications, we can use Python:</p> <pre><code>import scipy.stats as stats\n\n# Calculate probability between -1 and 1 standard deviations\nprob = stats.norm.cdf(1) - stats.norm.cdf(-1)\nprint(f\"Probability within 1\u03c3: {prob:.4f}\")  # \u2248 0.6827\n</code></pre>"},{"location":"grassroots/statistics/2_probability_distributions/#students-t-distribution","title":"Student's t-Distribution","text":"<p>The t-distribution emerges when estimating the mean of a normally distributed population when the sample size is small and the population standard deviation is unknown.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#mathematical-foundation_1","title":"Mathematical Foundation","text":"<p>The probability density function is:</p> <p>\\(f(t) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\\)</p> <p>where \u03bd represents the degrees of freedom.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#intuition","title":"Intuition","text":"<p>Think of the t-distribution as a \"more uncertain\" normal distribution. As sample size increases (\u03bd increases), we become more certain about our estimates, and the t-distribution approaches the normal distribution.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#critical-values","title":"Critical Values","text":"<p>For hypothesis testing, we often need critical values. The relationship between confidence levels and t-values depends on \u03bd:</p> <pre><code>def get_t_critical(confidence_level, df):\n    \"\"\"Calculate two-tailed critical t-value\"\"\"\n    alpha = 1 - confidence_level\n    return stats.t.ppf(1 - alpha/2, df)\n</code></pre>"},{"location":"grassroots/statistics/2_probability_distributions/#chi-square-distribution","title":"Chi-Square Distribution","text":"<p>The chi-square distribution represents the sum of squared standard normal variables. It's crucial for variance-related inference and categorical data analysis.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#mathematical-foundation_2","title":"Mathematical Foundation","text":"<p>For k degrees of freedom:</p> <p>\\(f(x) = \\frac{1}{2^{k/2}\\Gamma(k/2)} x^{k/2-1}e^{-x/2}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/#properties","title":"Properties","text":"<p>Expected value: E(X) = k Variance: Var(X) = 2k</p> <p>The distribution becomes more symmetric as k increases, approaching normality.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#application-example-testing-variance","title":"Application Example: Testing Variance","text":"<p>To test if a sample comes from a population with a specified variance \u03c3\u00b2\u2080:</p> <p>\\(\\chi^2 = \\frac{(n-1)s^2}{\\sigma_0^2}\\)</p> <p>where s\u00b2 is the sample variance.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#f-distribution","title":"F-Distribution","text":"<p>The F-distribution represents the ratio of two chi-square distributions divided by their respective degrees of freedom. It's fundamental for comparing variances and in ANOVA.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>For a ratio of chi-square variables with d\u2081 and d\u2082 degrees of freedom:</p> <p>\\(f(x) = \\frac{\\sqrt{\\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}{xB(d_1/2,d_2/2)}\\)</p> <p>where B is the beta function.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#anova-application","title":"ANOVA Application","text":"<p>In one-way ANOVA, the F-statistic is:</p> <p>\\(F = \\frac{\\text{Between-group variability}}{\\text{Within-group variability}} = \\frac{MS_{\\text{between}}}{MS_{\\text{within}}}\\)</p> <pre><code>def calculate_f_statistic(groups):\n    \"\"\"Calculate F-statistic for one-way ANOVA\"\"\"\n    f_stat, p_val = stats.f_oneway(*groups)\n    return f_stat, p_val\n</code></pre>"},{"location":"grassroots/statistics/2_probability_distributions/#exponential-and-gamma-distributions","title":"Exponential and Gamma Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/#exponential-distribution","title":"Exponential Distribution","text":"<p>Models the time between events in a Poisson process. Its memoryless property makes it unique.</p> <p>Mathematical form: \\(f(x) = \\lambda e^{-\\lambda x}\\), x \u2265 0</p> <p>The mean is 1/\u03bb and variance is 1/\u03bb\u00b2.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#gamma-distribution","title":"Gamma Distribution","text":"<p>Generalizes the exponential distribution. If X\u2081, ..., X\u2096 are independent exponential(\u03bb), their sum follows Gamma(k,\u03bb).</p> <p>PDF: \\(f(x) = \\frac{\\lambda^k x^{k-1} e^{-\\lambda x}}{\\Gamma(k)}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/#beta-distribution","title":"Beta Distribution","text":"<p>The beta distribution is defined on [0,1], making it perfect for modeling probabilities and proportions.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#mathematical-form","title":"Mathematical Form","text":"<p>\\(f(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/#shape-parameters","title":"Shape Parameters","text":"<p>\u03b1 and \u03b2 control the distribution shape: - \u03b1, \u03b2 &gt; 1: unimodal - \u03b1 = \u03b2 = 1: uniform - \u03b1 &lt; 1: J-shaped - \u03b2 &lt; 1: reverse J-shaped</p>"},{"location":"grassroots/statistics/2_probability_distributions/#bayesian-application","title":"Bayesian Application","text":"<p>In Bayesian inference, beta serves as a conjugate prior for binomial probability: - Prior: Beta(\u03b1,\u03b2) - Data: Binomial(n,p) - Posterior: Beta(\u03b1+successes, \u03b2+failures)</p> <pre><code>def update_beta_parameters(prior_alpha, prior_beta, successes, failures):\n    \"\"\"Update beta parameters with new data\"\"\"\n    post_alpha = prior_alpha + successes\n    post_beta = prior_beta + failures\n    return post_alpha, post_beta\n</code></pre> <p>Remember: 1. Distribution choice should be guided by data properties and theoretical considerations 2. Many distributions are interconnected through transformations 3. Computational tools help, but understanding the mathematical foundations is crucial 4. Visual inspection and formal tests should complement each other in distribution analysis</p>"},{"location":"grassroots/statistics/2_probability_distributions/#discrete-probability-distributions","title":"Discrete Probability Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/#bernoulli-and-binomial-distributions","title":"Bernoulli and Binomial Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>The Bernoulli distribution is the fundamental building block for many discrete distributions, modeling a single binary outcome. Think of it as a single coin flip or any yes/no experiment.</p> <p>Mathematical Formulation: Let X be a Bernoulli random variable with parameter p. Then:</p> <p>P(X = x) = p^x * (1-p)^(1-x), x \u2208 {0,1}</p> <p>The elegance of this formulation lies in how it captures both outcomes in a single expression: - When x = 1: P(X = 1) = p - When x = 0: P(X = 0) = 1-p</p> <p>Key Properties: - E[X] = p - Var(X) = p(1-p) - All higher moments can be derived from p - Moment Generating Function: M(t) = 1-p + pe^t</p> <p>For practical implementation, we can use Python's built-in random module for simple cases: <pre><code>import random\ndef bernoulli_trial(p):\n    return 1 if random.random() &lt; p else 0\n</code></pre></p>"},{"location":"grassroots/statistics/2_probability_distributions/#binomial-distribution","title":"Binomial Distribution","text":"<p>The binomial distribution naturally extends the Bernoulli to model the sum of n independent trials. Imagine flipping a coin n times and counting the total number of heads.</p> <p>Mathematical Formulation: For n trials with success probability p:</p> <p>P(X = k) = (n choose k) * p^k * (1-p)^(n-k)</p> <p>where (n choose k) = n!/(k!(n-k)!)</p> <p>The binomial coefficient (n choose k) represents the number of ways to choose k successes from n trials, making this a beautiful combination of combinatorics and probability.</p> <p>Probability Calculation Example: For small values, we can compute this directly: <pre><code>from math import comb\n\ndef binomial_probability(n, k, p):\n    return comb(n, k) * (p**k) * ((1-p)**(n-k))\n</code></pre></p> <p>For larger values where numerical stability is important, we should use specialized libraries: <pre><code>from scipy import stats\nstats.binom.pmf(k, n, p)\n</code></pre></p>"},{"location":"grassroots/statistics/2_probability_distributions/#poisson-distribution","title":"Poisson Distribution","text":"<p>The Poisson distribution models rare events occurring in a fixed interval. What makes it special is that we only need to know the average rate \u03bb to describe the entire distribution.</p> <p>Mathematical Foundation: P(X = k) = (\u03bb^k * e^(-\u03bb)) / k!</p> <p>This elegant formula emerges as the limit of a binomial distribution when n \u2192 \u221e and p \u2192 0 while np = \u03bb remains constant. This limiting relationship provides deep insight into why the Poisson distribution appears so often in nature.</p> <p>Moment Properties: - E[X] = \u03bb - Var(X) = \u03bb - All cumulants = \u03bb</p> <p>This equality of mean and variance is a defining characteristic that can help identify Poisson processes in real data.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#geometric-and-negative-binomial","title":"Geometric and Negative Binomial","text":""},{"location":"grassroots/statistics/2_probability_distributions/#geometric-distribution","title":"Geometric Distribution","text":"<p>The geometric distribution models the waiting time until the first success in repeated trials. Its memoryless property makes it unique among discrete distributions.</p> <p>Mathematical Insight: P(X = k) = p(1-p)^(k-1)</p> <p>The memoryless property means: P(X &gt; m + n | X &gt; m) = P(X &gt; n)</p> <p>This counterintuitive property tells us that the distribution \"forgets\" its past attempts.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#negative-binomial","title":"Negative Binomial","text":"<p>Generalizing the geometric distribution, the negative binomial models the number of trials until r successes. </p> <p>Mathematical Form: P(X = k) = ((k-1) choose (r-1)) * p^r * (1-p)^(k-r)</p> <p>This can be understood as waiting for the rth success, with k-r failures along the way. The combinatorial term accounts for all possible arrangements of these failures.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#hypergeometric-distribution","title":"Hypergeometric Distribution","text":"<p>Unlike the binomial, the hypergeometric distribution models sampling without replacement, making each draw dependent on previous draws.</p> <p>Mathematical Foundation:</p> <p>P(X = k) = [C(K,k) * C(N-K,n-k)] / C(N,n)</p> <p>Where: - N = population size - K = success states in population - n = sample size - k = observed successes</p> <p>The denominator C(N,n) represents all possible samples, while the numerator counts favorable outcomes through a clever application of the multiplication principle.</p> <p>Expected Value: E[X] = n(K/N)</p> <p>This intuitive result shows that the expected proportion of successes in the sample equals the proportion in the population.</p>"},{"location":"grassroots/statistics/2_probability_distributions/#distribution-relationships","title":"Distribution Relationships","text":"<p>The relationships between these distributions reveal deep mathematical connections:</p> <ol> <li> <p>Binomial and Poisson: When n is large and p is small: Binomial(n,p) \u2248 Poisson(np)</p> </li> <li> <p>Geometric and Negative Binomial: Geometric(p) = NegativeBinomial(1,p)</p> </li> <li> <p>Binomial and Hypergeometric: As N \u2192 \u221e, Hypergeometric(N,K,n) \u2192 Binomial(n,K/N)</p> </li> </ol> <p>For computational work, these relationships often suggest efficient approximations: <pre><code>def approximate_large_binomial(n, p, k):\n    \"\"\"Use Poisson approximation for large n, small p\"\"\"\n    if n &gt; 100 and p &lt; 0.05:\n        return stats.poisson.pmf(k, n*p)\n    return stats.binom.pmf(k, n, p)\n</code></pre></p> <p>Remember: - The choice between mathematical and computational approaches should be guided by both theoretical considerations and practical constraints - Understanding the mathematical foundations helps in selecting appropriate approximations - Modern computational tools make exact calculations feasible in many cases where approximations were historically necessary - The elegance of these distributions lies in their ability to model complex phenomena with simple parameters</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/","title":"Continuous Probability Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#normalgaussian-distribution","title":"Normal/Gaussian Distribution","text":"<p>The normal distribution is fundamental to statistics, arising naturally in many phenomena due to the Central Limit Theorem. Its mathematical elegance and theoretical properties make it the cornerstone of statistical inference.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The probability density function is given by:</p> <p>\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)</p> <p>where: - \u03bc determines the center (location) - \u03c3 controls the spread (scale)</p> <p>The standard normal distribution (\u03bc = 0, \u03c3 = 1) simplifies this to:</p> <p>\\(f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#key-properties","title":"Key Properties","text":"<ol> <li>Symmetry: The distribution is perfectly symmetric around \u03bc</li> <li>Empirical Rule:</li> <li>\u03bc \u00b1 \u03c3 contains \u2248 68% of data</li> <li>\u03bc \u00b1 2\u03c3 contains \u2248 95% of data</li> <li>\u03bc \u00b1 3\u03c3 contains \u2248 99.7% of data</li> </ol>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#implementation","title":"Implementation","text":"<p>For practical applications, we can use Python:</p> <pre><code>import scipy.stats as stats\n\n# Calculate probability between -1 and 1 standard deviations\nprob = stats.norm.cdf(1) - stats.norm.cdf(-1)\nprint(f\"Probability within 1\u03c3: {prob:.4f}\")  # \u2248 0.6827\n</code></pre>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#students-t-distribution","title":"Student's t-Distribution","text":"<p>The t-distribution emerges when estimating the mean of a normally distributed population when the sample size is small and the population standard deviation is unknown.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#mathematical-foundation_1","title":"Mathematical Foundation","text":"<p>The probability density function is:</p> <p>\\(f(t) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{t^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\\)</p> <p>where \u03bd represents the degrees of freedom.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#intuition","title":"Intuition","text":"<p>Think of the t-distribution as a \"more uncertain\" normal distribution. As sample size increases (\u03bd increases), we become more certain about our estimates, and the t-distribution approaches the normal distribution.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#critical-values","title":"Critical Values","text":"<p>For hypothesis testing, we often need critical values. The relationship between confidence levels and t-values depends on \u03bd:</p> <pre><code>def get_t_critical(confidence_level, df):\n    \"\"\"Calculate two-tailed critical t-value\"\"\"\n    alpha = 1 - confidence_level\n    return stats.t.ppf(1 - alpha/2, df)\n</code></pre>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#chi-square-distribution","title":"Chi-Square Distribution","text":"<p>The chi-square distribution represents the sum of squared standard normal variables. It's crucial for variance-related inference and categorical data analysis.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#mathematical-foundation_2","title":"Mathematical Foundation","text":"<p>For k degrees of freedom:</p> <p>\\(f(x) = \\frac{1}{2^{k/2}\\Gamma(k/2)} x^{k/2-1}e^{-x/2}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#properties","title":"Properties","text":"<p>Expected value: E(X) = k Variance: Var(X) = 2k</p> <p>The distribution becomes more symmetric as k increases, approaching normality.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#application-example-testing-variance","title":"Application Example: Testing Variance","text":"<p>To test if a sample comes from a population with a specified variance \u03c3\u00b2\u2080:</p> <p>\\(\\chi^2 = \\frac{(n-1)s^2}{\\sigma_0^2}\\)</p> <p>where s\u00b2 is the sample variance.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#f-distribution","title":"F-Distribution","text":"<p>The F-distribution represents the ratio of two chi-square distributions divided by their respective degrees of freedom. It's fundamental for comparing variances and in ANOVA.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>For a ratio of chi-square variables with d\u2081 and d\u2082 degrees of freedom:</p> <p>\\(f(x) = \\frac{\\sqrt{\\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}{xB(d_1/2,d_2/2)}\\)</p> <p>where B is the beta function.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#anova-application","title":"ANOVA Application","text":"<p>In one-way ANOVA, the F-statistic is:</p> <p>\\(F = \\frac{\\text{Between-group variability}}{\\text{Within-group variability}} = \\frac{MS_{\\text{between}}}{MS_{\\text{within}}}\\)</p> <pre><code>def calculate_f_statistic(groups):\n    \"\"\"Calculate F-statistic for one-way ANOVA\"\"\"\n    f_stat, p_val = stats.f_oneway(*groups)\n    return f_stat, p_val\n</code></pre>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#exponential-and-gamma-distributions","title":"Exponential and Gamma Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#exponential-distribution","title":"Exponential Distribution","text":"<p>Models the time between events in a Poisson process. Its memoryless property makes it unique.</p> <p>Mathematical form: \\(f(x) = \\lambda e^{-\\lambda x}\\), x \u2265 0</p> <p>The mean is 1/\u03bb and variance is 1/\u03bb\u00b2.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#gamma-distribution","title":"Gamma Distribution","text":"<p>Generalizes the exponential distribution. If X\u2081, ..., X\u2096 are independent exponential(\u03bb), their sum follows Gamma(k,\u03bb).</p> <p>PDF: \\(f(x) = \\frac{\\lambda^k x^{k-1} e^{-\\lambda x}}{\\Gamma(k)}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#beta-distribution","title":"Beta Distribution","text":"<p>The beta distribution is defined on [0,1], making it perfect for modeling probabilities and proportions.</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#mathematical-form","title":"Mathematical Form","text":"<p>\\(f(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\)</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#shape-parameters","title":"Shape Parameters","text":"<p>\u03b1 and \u03b2 control the distribution shape: - \u03b1, \u03b2 &gt; 1: unimodal - \u03b1 = \u03b2 = 1: uniform - \u03b1 &lt; 1: J-shaped - \u03b2 &lt; 1: reverse J-shaped</p>"},{"location":"grassroots/statistics/2_probability_distributions/continuous_distributions/#bayesian-application","title":"Bayesian Application","text":"<p>In Bayesian inference, beta serves as a conjugate prior for binomial probability: - Prior: Beta(\u03b1,\u03b2) - Data: Binomial(n,p) - Posterior: Beta(\u03b1+successes, \u03b2+failures)</p> <pre><code>def update_beta_parameters(prior_alpha, prior_beta, successes, failures):\n    \"\"\"Update beta parameters with new data\"\"\"\n    post_alpha = prior_alpha + successes\n    post_beta = prior_beta + failures\n    return post_alpha, post_beta\n</code></pre> <p>Remember: 1. Distribution choice should be guided by data properties and theoretical considerations 2. Many distributions are interconnected through transformations 3. Computational tools help, but understanding the mathematical foundations is crucial 4. Visual inspection and formal tests should complement each other in distribution analysis</p>"},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/","title":"Discrete Probability Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#bernoulli-and-binomial-distributions","title":"Bernoulli and Binomial Distributions","text":""},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>The Bernoulli distribution is the fundamental building block for many discrete distributions, modeling a single binary outcome. Think of it as a single coin flip or any yes/no experiment.</p> <p>Mathematical Formulation: Let X be a Bernoulli random variable with parameter p. Then:</p> <p>P(X = x) = p^x * (1-p)^(1-x), x \u2208 {0,1}</p> <p>The elegance of this formulation lies in how it captures both outcomes in a single expression: - When x = 1: P(X = 1) = p - When x = 0: P(X = 0) = 1-p</p> <p>Key Properties: - E[X] = p - Var(X) = p(1-p) - All higher moments can be derived from p - Moment Generating Function: M(t) = 1-p + pe^t</p> <p>For practical implementation, we can use Python's built-in random module for simple cases: <pre><code>import random\ndef bernoulli_trial(p):\n    return 1 if random.random() &lt; p else 0\n</code></pre></p>"},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#binomial-distribution","title":"Binomial Distribution","text":"<p>The binomial distribution naturally extends the Bernoulli to model the sum of n independent trials. Imagine flipping a coin n times and counting the total number of heads.</p> <p>Mathematical Formulation: For n trials with success probability p:</p> <p>P(X = k) = (n choose k) * p^k * (1-p)^(n-k)</p> <p>where (n choose k) = n!/(k!(n-k)!)</p> <p>The binomial coefficient (n choose k) represents the number of ways to choose k successes from n trials, making this a beautiful combination of combinatorics and probability.</p> <p>Probability Calculation Example: For small values, we can compute this directly: <pre><code>from math import comb\n\ndef binomial_probability(n, k, p):\n    return comb(n, k) * (p**k) * ((1-p)**(n-k))\n</code></pre></p> <p>For larger values where numerical stability is important, we should use specialized libraries: <pre><code>from scipy import stats\nstats.binom.pmf(k, n, p)\n</code></pre></p>"},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#poisson-distribution","title":"Poisson Distribution","text":"<p>The Poisson distribution models rare events occurring in a fixed interval. What makes it special is that we only need to know the average rate \u03bb to describe the entire distribution.</p> <p>Mathematical Foundation: P(X = k) = (\u03bb^k * e^(-\u03bb)) / k!</p> <p>This elegant formula emerges as the limit of a binomial distribution when n \u2192 \u221e and p \u2192 0 while np = \u03bb remains constant. This limiting relationship provides deep insight into why the Poisson distribution appears so often in nature.</p> <p>Moment Properties: - E[X] = \u03bb - Var(X) = \u03bb - All cumulants = \u03bb</p> <p>This equality of mean and variance is a defining characteristic that can help identify Poisson processes in real data.</p>"},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#geometric-and-negative-binomial","title":"Geometric and Negative Binomial","text":""},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#geometric-distribution","title":"Geometric Distribution","text":"<p>The geometric distribution models the waiting time until the first success in repeated trials. Its memoryless property makes it unique among discrete distributions.</p> <p>Mathematical Insight: P(X = k) = p(1-p)^(k-1)</p> <p>The memoryless property means: P(X &gt; m + n | X &gt; m) = P(X &gt; n)</p> <p>This counterintuitive property tells us that the distribution \"forgets\" its past attempts.</p>"},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#negative-binomial","title":"Negative Binomial","text":"<p>Generalizing the geometric distribution, the negative binomial models the number of trials until r successes. </p> <p>Mathematical Form: P(X = k) = ((k-1) choose (r-1)) * p^r * (1-p)^(k-r)</p> <p>This can be understood as waiting for the rth success, with k-r failures along the way. The combinatorial term accounts for all possible arrangements of these failures.</p>"},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#hypergeometric-distribution","title":"Hypergeometric Distribution","text":"<p>Unlike the binomial, the hypergeometric distribution models sampling without replacement, making each draw dependent on previous draws.</p> <p>Mathematical Foundation:</p> <p>P(X = k) = [C(K,k) * C(N-K,n-k)] / C(N,n)</p> <p>Where: - N = population size - K = success states in population - n = sample size - k = observed successes</p> <p>The denominator C(N,n) represents all possible samples, while the numerator counts favorable outcomes through a clever application of the multiplication principle.</p> <p>Expected Value: E[X] = n(K/N)</p> <p>This intuitive result shows that the expected proportion of successes in the sample equals the proportion in the population.</p>"},{"location":"grassroots/statistics/2_probability_distributions/discrete_distributions/#distribution-relationships","title":"Distribution Relationships","text":"<p>The relationships between these distributions reveal deep mathematical connections:</p> <ol> <li> <p>Binomial and Poisson: When n is large and p is small: Binomial(n,p) \u2248 Poisson(np)</p> </li> <li> <p>Geometric and Negative Binomial: Geometric(p) = NegativeBinomial(1,p)</p> </li> <li> <p>Binomial and Hypergeometric: As N \u2192 \u221e, Hypergeometric(N,K,n) \u2192 Binomial(n,K/N)</p> </li> </ol> <p>For computational work, these relationships often suggest efficient approximations: <pre><code>def approximate_large_binomial(n, p, k):\n    \"\"\"Use Poisson approximation for large n, small p\"\"\"\n    if n &gt; 100 and p &lt; 0.05:\n        return stats.poisson.pmf(k, n*p)\n    return stats.binom.pmf(k, n, p)\n</code></pre></p> <p>Remember: - The choice between mathematical and computational approaches should be guided by both theoretical considerations and practical constraints - Understanding the mathematical foundations helps in selecting appropriate approximations - Modern computational tools make exact calculations feasible in many cases where approximations were historically necessary - The elegance of these distributions lies in their ability to model complex phenomena with simple parameters</p>"},{"location":"grassroots/statistics/3_computational_rules/","title":"Basic Probability Computation Rules","text":""},{"location":"grassroots/statistics/3_computational_rules/#1-fundamental-concepts","title":"1. Fundamental Concepts","text":""},{"location":"grassroots/statistics/3_computational_rules/#sample-space","title":"Sample Space","text":"<p>The sample space (\u03a9) represents all possible outcomes of a random experiment. For example, when rolling a die, \u03a9 = {1, 2, 3, 4, 5, 6}.</p>"},{"location":"grassroots/statistics/3_computational_rules/#events","title":"Events","text":"<p>An event is a subset of the sample space. We typically denote events with capital letters (A, B, etc.).</p>"},{"location":"grassroots/statistics/3_computational_rules/#probability-measure","title":"Probability Measure","text":"<p>For any event A, the probability P(A) must satisfy: - 0 \u2264 P(A) \u2264 1 - P(\u03a9) = 1 - P(\u2205) = 0</p>"},{"location":"grassroots/statistics/3_computational_rules/#2-addition-rules","title":"2. Addition Rules","text":""},{"location":"grassroots/statistics/3_computational_rules/#basic-addition-rule","title":"Basic Addition Rule","text":"<p>For mutually exclusive events A and B: P(A \u222a B) = P(A) + P(B)</p>"},{"location":"grassroots/statistics/3_computational_rules/#general-addition-rule","title":"General Addition Rule","text":"<p>For any two events A and B: P(A \u222a B) = P(A) + P(B) - P(A \u2229 B)</p>"},{"location":"grassroots/statistics/3_computational_rules/#extension-to-multiple-events","title":"Extension to Multiple Events","text":"<p>For three events A, B, and C: P(A \u222a B \u222a C) = P(A) + P(B) + P(C) - P(A \u2229 B) - P(A \u2229 C) - P(B \u2229 C) + P(A \u2229 B \u2229 C)</p>"},{"location":"grassroots/statistics/3_computational_rules/#3-multiplication-rules","title":"3. Multiplication Rules","text":""},{"location":"grassroots/statistics/3_computational_rules/#independent-events","title":"Independent Events","text":"<p>Two events A and B are independent if: P(A \u2229 B) = P(A) \u00d7 P(B)</p>"},{"location":"grassroots/statistics/3_computational_rules/#dependent-events","title":"Dependent Events","text":"<p>For dependent events, use conditional probability: P(A \u2229 B) = P(A) \u00d7 P(B|A)</p>"},{"location":"grassroots/statistics/3_computational_rules/#chain-rule","title":"Chain Rule","text":"<p>For multiple events: P(A \u2229 B \u2229 C) = P(A) \u00d7 P(B|A) \u00d7 P(C|A \u2229 B)</p>"},{"location":"grassroots/statistics/3_computational_rules/#4-important-relationships","title":"4. Important Relationships","text":""},{"location":"grassroots/statistics/3_computational_rules/#mutually-exclusive-events","title":"Mutually Exclusive Events","text":"<p>Events A and B are mutually exclusive if: - A \u2229 B = \u2205 - P(A \u2229 B) = 0</p>"},{"location":"grassroots/statistics/3_computational_rules/#complement-rule","title":"Complement Rule","text":"<p>For any event A: P(A') = 1 - P(A)</p>"},{"location":"grassroots/statistics/3_computational_rules/#conditional-probability","title":"Conditional Probability","text":"<p>P(B|A) = P(A \u2229 B) / P(A), where P(A) &gt; 0</p>"},{"location":"grassroots/statistics/3_computational_rules/#5-common-mistakes-and-pitfalls","title":"5. Common Mistakes and Pitfalls","text":""},{"location":"grassroots/statistics/3_computational_rules/#independence-vs-mutual-exclusivity","title":"Independence vs. Mutual Exclusivity","text":"<ul> <li>Independent events CAN occur together</li> <li>Mutually exclusive events CANNOT occur together</li> <li>Two events cannot be both independent and mutually exclusive (unless one has probability 0)</li> </ul>"},{"location":"grassroots/statistics/3_computational_rules/#addition-rule-selection","title":"Addition Rule Selection","text":"<ul> <li>Use basic addition rule ONLY for mutually exclusive events</li> <li>Always use general addition rule when events can overlap</li> </ul>"},{"location":"grassroots/statistics/3_computational_rules/#6-practical-applications","title":"6. Practical Applications","text":""},{"location":"grassroots/statistics/3_computational_rules/#example-card-drawing","title":"Example: Card Drawing","text":"<p>Drawing cards from a standard deck: - P(Drawing a King) = 4/52 = 1/13 - P(Drawing a Heart) = 13/52 = 1/4 - P(Drawing a King of Hearts) = 1/52 - P(Drawing a King OR a Heart) = 15/52 (using general addition rule)</p>"},{"location":"grassroots/statistics/3_computational_rules/#example-rolling-dice","title":"Example: Rolling Dice","text":"<p>Rolling two dice: - P(Sum = 7) = 6/36 = 1/6 - P(First die = 6) = 1/6 - These events are independent: knowing the sum doesn't tell us the value of the first die</p>"},{"location":"grassroots/statistics/3_computational_rules/#7-verification-methods","title":"7. Verification Methods","text":""},{"location":"grassroots/statistics/3_computational_rules/#testing-for-independence","title":"Testing for Independence","text":"<p>To verify if events A and B are independent, check if: P(A \u2229 B) = P(A) \u00d7 P(B)</p>"},{"location":"grassroots/statistics/3_computational_rules/#testing-for-mutual-exclusivity","title":"Testing for Mutual Exclusivity","text":"<p>To verify if events A and B are mutually exclusive, check if: P(A \u2229 B) = 0</p>"},{"location":"grassroots/statistics/3_computational_rules/#8-additional-considerations","title":"8. Additional Considerations","text":""},{"location":"grassroots/statistics/3_computational_rules/#law-of-total-probability","title":"Law of Total Probability","text":"<p>For a partition of events B\u2081, B\u2082, ..., B\u2099: P(A) = P(A|B\u2081)P(B\u2081) + P(A|B\u2082)P(B\u2082) + ... + P(A|B\u2099)P(B\u2099)</p>"},{"location":"grassroots/statistics/3_computational_rules/#bayes-theorem","title":"Bayes' Theorem","text":"<p>P(A|B) = P(B|A)P(A) / P(B)</p>"},{"location":"grassroots/statistics/3_computational_rules/#practice-problems","title":"Practice Problems","text":"<ol> <li>If P(A) = 0.3, P(B) = 0.4, and P(A \u2229 B) = 0.12, calculate:</li> <li>P(A \u222a B)</li> <li> <p>Are A and B independent?</p> </li> <li> <p>Three cards are drawn from a deck without replacement. Calculate:</p> </li> <li>P(All are aces)</li> <li>P(All are the same suit)</li> </ol> <p>[Solutions provided in separate section]</p>"},{"location":"grassroots/statistics/4_statistical_inference/","title":"Statistical Inference","text":""},{"location":"grassroots/statistics/4_statistical_inference/#statistical-hypothesis-testing","title":"Statistical Hypothesis Testing","text":""},{"location":"grassroots/statistics/4_statistical_inference/#fundamental-concepts","title":"Fundamental Concepts","text":""},{"location":"grassroots/statistics/4_statistical_inference/#null-and-alternative-hypotheses","title":"Null and Alternative Hypotheses","text":"<p>The foundation of hypothesis testing lies in formulating two competing claims about a population parameter:</p> <ol> <li>Null Hypothesis (H\u2080): The default position, typically expressing \"no effect\" or \"no difference\"</li> <li>Alternative Hypothesis (H\u2081 or H\u2090): The research claim we want to support with evidence</li> </ol> <p>For a population parameter \u03b8, these are typically expressed in one of three ways:</p> <p>Two-sided test: <pre><code>H\u2080: \u03b8 = \u03b8\u2080\nH\u2081: \u03b8 \u2260 \u03b8\u2080\n</code></pre></p> <p>One-sided tests: <pre><code>Upper-tailed:          Lower-tailed:\nH\u2080: \u03b8 \u2264 \u03b8\u2080            H\u2080: \u03b8 \u2265 \u03b8\u2080\nH\u2081: \u03b8 &gt; \u03b8\u2080            H\u2081: \u03b8 &lt; \u03b8\u2080\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/#test-statistics-and-sampling-distributions","title":"Test Statistics and Sampling Distributions","text":"<p>Most test statistics follow the general form:</p> <pre><code>test statistic = (sample estimate - null value) / (standard error)\n</code></pre> <p>For example, the z-test statistic for a population mean:</p> <pre><code>z = (x\u0304 - \u03bc\u2080) / (\u03c3/\u221an)\n</code></pre> <p>where: - x\u0304 is the sample mean - \u03bc\u2080 is the hypothesized population mean - \u03c3 is the population standard deviation - n is the sample size</p> <p>When \u03c3 is unknown and estimated by s, we use the t-statistic:</p> <pre><code>t = (x\u0304 - \u03bc\u2080) / (s/\u221an)\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/#type-i-and-type-ii-errors","title":"Type I and Type II Errors","text":"<p>The decision process in hypothesis testing can lead to two types of errors:</p> H\u2080 True H\u2080 False Reject H\u2080 Type I Error (\u03b1) Correct Decision Fail to Reject H\u2080 Correct Decision Type II Error (\u03b2) <p>The probability relationships: <pre><code>P(Type I Error) = \u03b1 = P(Reject H\u2080 | H\u2080 true)\nP(Type II Error) = \u03b2 = P(Fail to Reject H\u2080 | H\u2081 true)\nPower = 1 - \u03b2 = P(Reject H\u2080 | H\u2081 true)\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/#p-values-and-statistical-power","title":"P-values and Statistical Power","text":""},{"location":"grassroots/statistics/4_statistical_inference/#p-value","title":"P-value","text":"<p>The p-value is defined mathematically as:</p> <p>For a test statistic T and observed value t*: <pre><code>Two-sided: p = 2 * P(T \u2265 |t*| | H\u2080)\nUpper-tailed: p = P(T \u2265 t* | H\u2080)\nLower-tailed: p = P(T \u2264 t* | H\u2080)\n</code></pre></p> <p>For practical computation in Python: <pre><code>def calculate_p_value(test_statistic, distribution='normal', two_sided=True):\n    if distribution == 'normal':\n        if two_sided:\n            return 2 * (1 - stats.norm.cdf(abs(test_statistic)))\n        return 1 - stats.norm.cdf(test_statistic)\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/#statistical-power","title":"Statistical Power","text":"<p>Power depends on four interrelated quantities: 1. Effect size (\u03b4) 2. Sample size (n) 3. Significance level (\u03b1) 4. Power (1-\u03b2)</p> <p>For a two-sided z-test, the power function is:</p> <pre><code>Power = 1 - \u03b2 = \u03a6(z\u03b1/2 + \u03b4\u221an/\u03c3) + \u03a6(-z\u03b1/2 + \u03b4\u221an/\u03c3)\n</code></pre> <p>where: - \u03a6 is the standard normal CDF - z\u03b1/2 is the critical value - \u03b4 is the true difference from null - \u03c3 is the population standard deviation</p>"},{"location":"grassroots/statistics/4_statistical_inference/#multiple-testing-problem","title":"Multiple Testing Problem","text":"<p>When conducting m independent tests at significance level \u03b1, the probability of at least one Type I error (Family-Wise Error Rate, FWER) is:</p> <pre><code>FWER = 1 - (1-\u03b1)\u1d50\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/#bonferroni-correction","title":"Bonferroni Correction","text":"<p>Controls FWER by adjusting the significance level: <pre><code>\u03b1_adjusted = \u03b1/m\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/#benjamini-hochberg-procedure","title":"Benjamini-Hochberg Procedure","text":"<p>Controls False Discovery Rate (FDR). For ordered p-values p\u2081 \u2264 p\u2082 \u2264 ... \u2264 p\u2098: 1. Find largest k where p_k \u2264 (k/m)\u03b1 2. Reject all hypotheses H\u208d\u1d62\u208e for i = 1,...,k</p> <p>Implementation combining mathematical insight with computation: <pre><code>def benjamini_hochberg(p_values, alpha=0.05):\n    \"\"\"\n    Implements Benjamini-Hochberg procedure\n    Returns: boolean array of rejected null hypotheses\n    \"\"\"\n    m = len(p_values)\n    ranked = stats.rankdata(p_values, method='min')\n    critical_values = (ranked / m) * alpha\n    sorted_p_values = np.sort(p_values)\n\n    # Find largest k where p_k \u2264 (k/m)\u03b1\n    significant = p_values &lt;= critical_values\n    return significant\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/#power-analysis-example","title":"Power Analysis Example","text":"<p>Let's combine mathematical formulation with computation for a t-test power analysis:</p> <pre><code>def power_analysis(effect_size, n, alpha=0.05, two_sided=True):\n    \"\"\"\n    Calculate power for two-sample t-test\n\n    Parameters:\n    effect_size (d) = (\u03bc\u2081 - \u03bc\u2082)/\u03c3\n    n = sample size per group\n    \"\"\"\n    # Critical value\n    df = 2*n - 2  # degrees of freedom\n    if two_sided:\n        t_crit = stats.t.ppf(1 - alpha/2, df)\n    else:\n        t_crit = stats.t.ppf(1 - alpha, df)\n\n    # Non-centrality parameter\n    ncp = effect_size * np.sqrt(n/2)\n\n    # Power calculation\n    if two_sided:\n        power = (1 - stats.nct.cdf(t_crit, df, ncp) + \n                stats.nct.cdf(-t_crit, df, ncp))\n    else:\n        power = 1 - stats.nct.cdf(t_crit, df, ncp)\n\n    return power\n</code></pre> <p>This balanced approach shows both the mathematical foundation and its practical implementation, helping users understand both the theory and application of hypothesis testing.</p> <p>Remember: 1. Start with clear mathematical formulation 2. Provide intuitive explanations 3. Implement solutions efficiently 4. Consider computational aspects 5. Document assumptions and limitations</p>"},{"location":"grassroots/statistics/4_statistical_inference/#statistical-estimation","title":"Statistical Estimation","text":""},{"location":"grassroots/statistics/4_statistical_inference/#point-estimates","title":"Point Estimates","text":"<p>A point estimate is a single value that serves as our \"best guess\" for an unknown population parameter. The theory behind point estimation helps us understand what makes a good estimator.</p>"},{"location":"grassroots/statistics/4_statistical_inference/#properties-of-estimators","title":"Properties of Estimators","text":""},{"location":"grassroots/statistics/4_statistical_inference/#unbiasedness","title":"Unbiasedness","text":"<p>An estimator \u03b8\u0302 is unbiased if its expected value equals the true parameter:</p> <p>E[\u03b8\u0302] = \u03b8</p> <p>For example, the sample mean X\u0304 is an unbiased estimator of the population mean \u03bc:</p> <p>E[X\u0304] = E[\u2211X\u1d62/n] = \u2211E[X\u1d62]/n = \u03bc</p>"},{"location":"grassroots/statistics/4_statistical_inference/#consistency","title":"Consistency","text":"<p>An estimator is consistent if it converges to the true parameter as sample size increases:</p> <p>lim(n\u2192\u221e) P(|\u03b8\u0302\u2099 - \u03b8| &gt; \u03b5) = 0 for any \u03b5 &gt; 0</p>"},{"location":"grassroots/statistics/4_statistical_inference/#efficiency","title":"Efficiency","text":"<p>Among unbiased estimators, the most efficient one has the smallest variance. The Cram\u00e9r-Rao bound gives us the theoretical minimum variance:</p> <p>Var(\u03b8\u0302) \u2265 1/I(\u03b8)</p> <p>where I(\u03b8) is the Fisher Information.</p>"},{"location":"grassroots/statistics/4_statistical_inference/#implementation-of-basic-estimators","title":"Implementation of Basic Estimators","text":"<pre><code>def calculate_estimators(data):\n    \"\"\"Calculate common point estimates with their standard errors\"\"\"\n    n = len(data)\n    mean = np.mean(data)\n    variance = np.var(data, ddof=1)  # Using n-1 for unbiased estimation\n\n    return {\n        'mean': mean,\n        'se_mean': np.sqrt(variance/n),  # Standard error of mean\n        'variance': variance,\n        'se_variance': variance * np.sqrt(2/(n-1))  # SE of variance\n    }\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/#confidence-intervals","title":"Confidence Intervals","text":"<p>A confidence interval provides a range of plausible values for a parameter, along with a measure of uncertainty. The mathematical foundation comes from the sampling distribution of the estimator.</p>"},{"location":"grassroots/statistics/4_statistical_inference/#for-population-mean","title":"For Population Mean","text":"<p>Under normality assumption, the pivotal quantity:</p> <p>(X\u0304 - \u03bc)/(s/\u221an) ~ t(n-1)</p> <p>leads to the confidence interval:</p> <p>X\u0304 \u00b1 t(\u03b1/2,n-1) * s/\u221an</p> <p>where: - t(\u03b1/2,n-1) is the critical value from t-distribution - s is the sample standard deviation - n is the sample size</p> <pre><code>def mean_confidence_interval(data, confidence=0.95):\n    \"\"\"Calculate CI for mean using t-distribution\"\"\"\n    n = len(data)\n    mean = np.mean(data)\n    se = stats.sem(data)\n    ci = stats.t.interval(confidence, df=n-1, loc=mean, scale=se)\n    return mean, ci\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<p>MLE finds parameter values that maximize the probability of observing the data. For independent observations, the likelihood function is:</p> <p>L(\u03b8; x\u2081, ..., x\u2099) = \u220f\u1d62 f(x\u1d62; \u03b8)</p> <p>We typically maximize the log-likelihood:</p> <p>\u2113(\u03b8) = \u2211\u1d62 log f(x\u1d62; \u03b8)</p>"},{"location":"grassroots/statistics/4_statistical_inference/#example-normal-distribution","title":"Example: Normal Distribution","text":"<p>For normal distribution, the log-likelihood is:</p> <p>\u2113(\u03bc,\u03c3\u00b2) = -n/2 * log(2\u03c0\u03c3\u00b2) - \u2211(x\u1d62 - \u03bc)\u00b2/(2\u03c3\u00b2)</p> <p>The MLEs are: \u03bc\u0302 = X\u0304 \u03c3\u0302\u00b2 = \u2211(x\u1d62 - X\u0304)\u00b2/n</p> <pre><code>def normal_mle_with_uncertainty(data):\n    \"\"\"MLE for normal distribution with standard errors\"\"\"\n    n = len(data)\n    mu = np.mean(data)\n    sigma2 = np.sum((data - mu)**2)/n  # MLE of variance\n\n    # Fisher Information Matrix derivatives\n    se_mu = np.sqrt(sigma2/n)\n    se_sigma = np.sqrt(sigma2/(2*n))\n\n    return {\n        'mu': mu, \n        'sigma': np.sqrt(sigma2),\n        'se_mu': se_mu,\n        'se_sigma': se_sigma\n    }\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>The expected prediction error can be decomposed into:</p> <p>E[(y - f\u0302(x))\u00b2] = (Bias[f\u0302(x)])\u00b2 + Var[f\u0302(x)] + \u03c3\u00b2</p> <p>where: - Bias[f\u0302(x)] = E[f\u0302(x)] - f(x) - Var[f\u0302(x)] = E[(f\u0302(x) - E[f\u0302(x)])\u00b2] - \u03c3\u00b2 is irreducible error</p> <p>This decomposition helps understand the fundamental tradeoff in model complexity: - Simple models: High bias, low variance - Complex models: Low bias, high variance</p>"},{"location":"grassroots/statistics/4_statistical_inference/#visual-demonstration","title":"Visual Demonstration","text":"<pre><code>def plot_bias_variance_tradeoff(complexity_range, bias_values, variance_values):\n    \"\"\"Plot bias-variance tradeoff across model complexities\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(complexity_range, bias_values, label='Bias\u00b2')\n    plt.plot(complexity_range, variance_values, label='Variance')\n    plt.plot(complexity_range, \n            np.array(bias_values) + np.array(variance_values), \n            label='Total Error')\n    plt.xlabel('Model Complexity')\n    plt.ylabel('Error')\n    plt.legend()\n    plt.title('Bias-Variance Tradeoff')\n    return plt\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Point Estimation</li> <li>Balance between different estimator properties</li> <li> <p>Consider both theoretical properties and practical constraints</p> </li> <li> <p>Interval Estimation</p> </li> <li>Provides measure of uncertainty</li> <li>Based on sampling distribution theory</li> <li> <p>Requires careful interpretation</p> </li> <li> <p>Maximum Likelihood</p> </li> <li>Powerful, general-purpose method</li> <li>Asymptotically optimal properties</li> <li> <p>Can be computationally intensive</p> </li> <li> <p>Bias-Variance</p> </li> <li>Fundamental tradeoff in statistical learning</li> <li>Guides model complexity selection</li> <li>Helps understand overfitting/underfitting</li> </ol> <p>Remember: - Theory guides the choice of methods - Practical considerations often require compromises - Understanding uncertainty is crucial - Multiple approaches often provide better insight</p>"},{"location":"grassroots/statistics/4_statistical_inference/#sampling-theory","title":"Sampling Theory","text":""},{"location":"grassroots/statistics/4_statistical_inference/#population-vs-sample","title":"Population vs Sample","text":"<p>The foundation of statistical inference lies in the relationship between populations and samples:</p> <ul> <li>Population: The complete set of all elements we want to study</li> <li>Sample: A subset of the population used to make inferences</li> </ul>"},{"location":"grassroots/statistics/4_statistical_inference/#mathematical-notation","title":"Mathematical Notation","text":"<ul> <li>Population parameters: \u03b8, \u03bc, \u03c3, \u03c0</li> <li>Sample statistics: \u03b8\u0302, x\u0304, s, p\u0302</li> </ul> <p>The relationship between population parameters and sample statistics can be expressed through expected values: * E[x\u0304] = \u03bc (unbiased estimator of mean) * E[s\u00b2] = \u03c3\u00b2 (unbiased estimator of variance)</p>"},{"location":"grassroots/statistics/4_statistical_inference/#sampling-distributions","title":"Sampling Distributions","text":"<p>The sampling distribution describes the probability distribution of a statistic across all possible samples of size n.</p>"},{"location":"grassroots/statistics/4_statistical_inference/#sample-mean","title":"Sample Mean","text":"<p>For a random sample X\u2081, X\u2082, ..., X\u2099: * Sample mean: x\u0304 = (1/n)\u2211X\u1d62 * Expected value: E[x\u0304] = \u03bc * Variance: Var(x\u0304) = \u03c3\u00b2/n</p> <pre><code># Implementation for empirical sampling distribution\ndef sample_mean_distribution(population, n_samples, sample_size):\n    means = [np.mean(np.random.choice(population, sample_size)) \n             for _ in range(n_samples)]\n    return np.array(means)\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/#central-limit-theorem-clt","title":"Central Limit Theorem (CLT)","text":""},{"location":"grassroots/statistics/4_statistical_inference/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>For independent, identically distributed random variables X\u2081, X\u2082, ..., X\u2099 with mean \u03bc and variance \u03c3\u00b2, the standardized sample mean:</p> <p>Z = (x\u0304 - \u03bc)/(\u03c3/\u221an) \u2192 N(0,1) as n \u2192 \u221e</p> <p>This means that for large n: x\u0304 \u223c N(\u03bc, \u03c3\u00b2/n)</p>"},{"location":"grassroots/statistics/4_statistical_inference/#intuition","title":"Intuition","text":"<p>The CLT tells us that regardless of the underlying distribution: 1. The sampling distribution of the mean becomes approximately normal 2. The spread of this distribution shrinks with \u221an 3. The center remains at the population mean</p>"},{"location":"grassroots/statistics/4_statistical_inference/#law-of-large-numbers-lln","title":"Law of Large Numbers (LLN)","text":""},{"location":"grassroots/statistics/4_statistical_inference/#weak-law-wlln","title":"Weak Law (WLLN)","text":"<p>For any \u03b5 &gt; 0: P(|x\u0304\u2099 - \u03bc| &gt; \u03b5) \u2192 0 as n \u2192 \u221e</p>"},{"location":"grassroots/statistics/4_statistical_inference/#strong-law-slln","title":"Strong Law (SLLN)","text":"<p>P(lim\u2099\u2192\u221e x\u0304\u2099 = \u03bc) = 1</p>"},{"location":"grassroots/statistics/4_statistical_inference/#intuition_1","title":"Intuition","text":"<ul> <li>WLLN: The probability of a large deviation from \u03bc becomes small</li> <li>SLLN: The sample mean will converge to \u03bc with probability 1</li> </ul>"},{"location":"grassroots/statistics/4_statistical_inference/#standard-error","title":"Standard Error","text":""},{"location":"grassroots/statistics/4_statistical_inference/#mathematical-definition","title":"Mathematical Definition","text":"<p>For any statistic \u03b8\u0302: SE(\u03b8\u0302) = \u221aVar(\u03b8\u0302)</p>"},{"location":"grassroots/statistics/4_statistical_inference/#common-forms","title":"Common Forms","text":"<ol> <li>Mean: SE(x\u0304) = \u03c3/\u221an</li> <li>Proportion: SE(p\u0302) = \u221a(p(1-p)/n)</li> <li>Difference of Means: SE(x\u0304\u2081 - x\u0304\u2082) = \u221a(\u03c3\u2081\u00b2/n\u2081 + \u03c3\u2082\u00b2/n\u2082)</li> </ol> <p>When population parameters are unknown, we use sample estimates: s/\u221an, \u221a(p\u0302(1-p\u0302)/n), etc.</p>"},{"location":"grassroots/statistics/4_statistical_inference/#practical-sampling-methods","title":"Practical Sampling Methods","text":""},{"location":"grassroots/statistics/4_statistical_inference/#simple-random-sampling","title":"Simple Random Sampling","text":"<p>Each subset of size n has equal probability of selection: P(selecting specific sample) = 1/\u208d\u2099\u1d3a\u208e</p> <pre><code>def simple_random_sample(population, size):\n    return np.random.choice(population, size=size, replace=False)\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/#stratified-sampling","title":"Stratified Sampling","text":"<p>For L strata with N\u2095 units in stratum h: * Stratum weight: W\u2095 = N\u2095/N * Stratified mean: x\u0304\u209b\u209c = \u2211W\u2095x\u0304\u2095 * Variance: Var(x\u0304\u209b\u209c) = \u2211W\u2095\u00b2\u03c3\u2095\u00b2/n\u2095</p>"},{"location":"grassroots/statistics/4_statistical_inference/#sample-size-determination","title":"Sample Size Determination","text":"<p>For desired margin of error E and confidence level \u03b1: * For means: n = (z\u03b1/2 \u00d7 \u03c3/E)\u00b2 * For proportions: n = (z\u03b1/2)\u00b2 \u00d7 p(1-p)/E\u00b2</p>"},{"location":"grassroots/statistics/4_statistical_inference/#relationship-to-statistical-inference","title":"Relationship to Statistical Inference","text":"<p>The theoretical foundations of sampling connect directly to inference through:</p> <ol> <li> <p>Confidence Intervals \u03b8\u0302 \u00b1 z\u03b1/2 \u00d7 SE(\u03b8\u0302)</p> </li> <li> <p>Hypothesis Tests Test statistic = (\u03b8\u0302 - \u03b8\u2080)/SE(\u03b8\u0302)</p> </li> </ol> <p>These applications rely on the sampling distribution theory developed above.</p>"},{"location":"grassroots/statistics/4_statistical_inference/#example-one-sample-t-test","title":"Example: One-Sample t-test","text":"<p>Under H\u2080: \u03bc = \u03bc\u2080 t = (x\u0304 - \u03bc\u2080)/(s/\u221an) \u223c t(n-1)</p> <pre><code>def t_test_statistic(sample, null_mean):\n    return (np.mean(sample) - null_mean)/(np.std(sample, ddof=1)/np.sqrt(len(sample)))\n</code></pre> <p>Remember: 1. The mathematical theory provides the foundation 2. Computational methods help verify and visualize 3. Understanding both perspectives enhances statistical practice 4. Real-world applications often require both theoretical and practical tools</p>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/","title":"Statistical Estimation","text":""},{"location":"grassroots/statistics/4_statistical_inference/estimation/#point-estimates","title":"Point Estimates","text":"<p>A point estimate is a single value that serves as our \"best guess\" for an unknown population parameter. The theory behind point estimation helps us understand what makes a good estimator.</p>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#properties-of-estimators","title":"Properties of Estimators","text":""},{"location":"grassroots/statistics/4_statistical_inference/estimation/#unbiasedness","title":"Unbiasedness","text":"<p>An estimator \u03b8\u0302 is unbiased if its expected value equals the true parameter:</p> <p>E[\u03b8\u0302] = \u03b8</p> <p>For example, the sample mean X\u0304 is an unbiased estimator of the population mean \u03bc:</p> <p>E[X\u0304] = E[\u2211X\u1d62/n] = \u2211E[X\u1d62]/n = \u03bc</p>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#consistency","title":"Consistency","text":"<p>An estimator is consistent if it converges to the true parameter as sample size increases:</p> <p>lim(n\u2192\u221e) P(|\u03b8\u0302\u2099 - \u03b8| &gt; \u03b5) = 0 for any \u03b5 &gt; 0</p>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#efficiency","title":"Efficiency","text":"<p>Among unbiased estimators, the most efficient one has the smallest variance. The Cram\u00e9r-Rao bound gives us the theoretical minimum variance:</p> <p>Var(\u03b8\u0302) \u2265 1/I(\u03b8)</p> <p>where I(\u03b8) is the Fisher Information.</p>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#implementation-of-basic-estimators","title":"Implementation of Basic Estimators","text":"<pre><code>def calculate_estimators(data):\n    \"\"\"Calculate common point estimates with their standard errors\"\"\"\n    n = len(data)\n    mean = np.mean(data)\n    variance = np.var(data, ddof=1)  # Using n-1 for unbiased estimation\n\n    return {\n        'mean': mean,\n        'se_mean': np.sqrt(variance/n),  # Standard error of mean\n        'variance': variance,\n        'se_variance': variance * np.sqrt(2/(n-1))  # SE of variance\n    }\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#confidence-intervals","title":"Confidence Intervals","text":"<p>A confidence interval provides a range of plausible values for a parameter, along with a measure of uncertainty. The mathematical foundation comes from the sampling distribution of the estimator.</p>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#for-population-mean","title":"For Population Mean","text":"<p>Under normality assumption, the pivotal quantity:</p> <p>(X\u0304 - \u03bc)/(s/\u221an) ~ t(n-1)</p> <p>leads to the confidence interval:</p> <p>X\u0304 \u00b1 t(\u03b1/2,n-1) * s/\u221an</p> <p>where: - t(\u03b1/2,n-1) is the critical value from t-distribution - s is the sample standard deviation - n is the sample size</p> <pre><code>def mean_confidence_interval(data, confidence=0.95):\n    \"\"\"Calculate CI for mean using t-distribution\"\"\"\n    n = len(data)\n    mean = np.mean(data)\n    se = stats.sem(data)\n    ci = stats.t.interval(confidence, df=n-1, loc=mean, scale=se)\n    return mean, ci\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<p>MLE finds parameter values that maximize the probability of observing the data. For independent observations, the likelihood function is:</p> <p>L(\u03b8; x\u2081, ..., x\u2099) = \u220f\u1d62 f(x\u1d62; \u03b8)</p> <p>We typically maximize the log-likelihood:</p> <p>\u2113(\u03b8) = \u2211\u1d62 log f(x\u1d62; \u03b8)</p>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#example-normal-distribution","title":"Example: Normal Distribution","text":"<p>For normal distribution, the log-likelihood is:</p> <p>\u2113(\u03bc,\u03c3\u00b2) = -n/2 * log(2\u03c0\u03c3\u00b2) - \u2211(x\u1d62 - \u03bc)\u00b2/(2\u03c3\u00b2)</p> <p>The MLEs are: \u03bc\u0302 = X\u0304 \u03c3\u0302\u00b2 = \u2211(x\u1d62 - X\u0304)\u00b2/n</p> <pre><code>def normal_mle_with_uncertainty(data):\n    \"\"\"MLE for normal distribution with standard errors\"\"\"\n    n = len(data)\n    mu = np.mean(data)\n    sigma2 = np.sum((data - mu)**2)/n  # MLE of variance\n\n    # Fisher Information Matrix derivatives\n    se_mu = np.sqrt(sigma2/n)\n    se_sigma = np.sqrt(sigma2/(2*n))\n\n    return {\n        'mu': mu, \n        'sigma': np.sqrt(sigma2),\n        'se_mu': se_mu,\n        'se_sigma': se_sigma\n    }\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>The expected prediction error can be decomposed into:</p> <p>E[(y - f\u0302(x))\u00b2] = (Bias[f\u0302(x)])\u00b2 + Var[f\u0302(x)] + \u03c3\u00b2</p> <p>where: - Bias[f\u0302(x)] = E[f\u0302(x)] - f(x) - Var[f\u0302(x)] = E[(f\u0302(x) - E[f\u0302(x)])\u00b2] - \u03c3\u00b2 is irreducible error</p> <p>This decomposition helps understand the fundamental tradeoff in model complexity: - Simple models: High bias, low variance - Complex models: Low bias, high variance</p>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#visual-demonstration","title":"Visual Demonstration","text":"<pre><code>def plot_bias_variance_tradeoff(complexity_range, bias_values, variance_values):\n    \"\"\"Plot bias-variance tradeoff across model complexities\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(complexity_range, bias_values, label='Bias\u00b2')\n    plt.plot(complexity_range, variance_values, label='Variance')\n    plt.plot(complexity_range, \n            np.array(bias_values) + np.array(variance_values), \n            label='Total Error')\n    plt.xlabel('Model Complexity')\n    plt.ylabel('Error')\n    plt.legend()\n    plt.title('Bias-Variance Tradeoff')\n    return plt\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/estimation/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Point Estimation</li> <li>Balance between different estimator properties</li> <li> <p>Consider both theoretical properties and practical constraints</p> </li> <li> <p>Interval Estimation</p> </li> <li>Provides measure of uncertainty</li> <li>Based on sampling distribution theory</li> <li> <p>Requires careful interpretation</p> </li> <li> <p>Maximum Likelihood</p> </li> <li>Powerful, general-purpose method</li> <li>Asymptotically optimal properties</li> <li> <p>Can be computationally intensive</p> </li> <li> <p>Bias-Variance</p> </li> <li>Fundamental tradeoff in statistical learning</li> <li>Guides model complexity selection</li> <li>Helps understand overfitting/underfitting</li> </ol> <p>Remember: - Theory guides the choice of methods - Practical considerations often require compromises - Understanding uncertainty is crucial - Multiple approaches often provide better insight</p>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/","title":"Statistical Hypothesis Testing","text":""},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#fundamental-concepts","title":"Fundamental Concepts","text":""},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#null-and-alternative-hypotheses","title":"Null and Alternative Hypotheses","text":"<p>The foundation of hypothesis testing lies in formulating two competing claims about a population parameter:</p> <ol> <li>Null Hypothesis (H\u2080): The default position, typically expressing \"no effect\" or \"no difference\"</li> <li>Alternative Hypothesis (H\u2081 or H\u2090): The research claim we want to support with evidence</li> </ol> <p>For a population parameter \u03b8, these are typically expressed in one of three ways:</p> <p>Two-sided test: <pre><code>H\u2080: \u03b8 = \u03b8\u2080\nH\u2081: \u03b8 \u2260 \u03b8\u2080\n</code></pre></p> <p>One-sided tests: <pre><code>Upper-tailed:          Lower-tailed:\nH\u2080: \u03b8 \u2264 \u03b8\u2080            H\u2080: \u03b8 \u2265 \u03b8\u2080\nH\u2081: \u03b8 &gt; \u03b8\u2080            H\u2081: \u03b8 &lt; \u03b8\u2080\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#test-statistics-and-sampling-distributions","title":"Test Statistics and Sampling Distributions","text":"<p>Most test statistics follow the general form:</p> <pre><code>test statistic = (sample estimate - null value) / (standard error)\n</code></pre> <p>For example, the z-test statistic for a population mean:</p> <pre><code>z = (x\u0304 - \u03bc\u2080) / (\u03c3/\u221an)\n</code></pre> <p>where: - x\u0304 is the sample mean - \u03bc\u2080 is the hypothesized population mean - \u03c3 is the population standard deviation - n is the sample size</p> <p>When \u03c3 is unknown and estimated by s, we use the t-statistic:</p> <pre><code>t = (x\u0304 - \u03bc\u2080) / (s/\u221an)\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#type-i-and-type-ii-errors","title":"Type I and Type II Errors","text":"<p>The decision process in hypothesis testing can lead to two types of errors:</p> H\u2080 True H\u2080 False Reject H\u2080 Type I Error (\u03b1) Correct Decision Fail to Reject H\u2080 Correct Decision Type II Error (\u03b2) <p>The probability relationships: <pre><code>P(Type I Error) = \u03b1 = P(Reject H\u2080 | H\u2080 true)\nP(Type II Error) = \u03b2 = P(Fail to Reject H\u2080 | H\u2081 true)\nPower = 1 - \u03b2 = P(Reject H\u2080 | H\u2081 true)\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#p-values-and-statistical-power","title":"P-values and Statistical Power","text":""},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#p-value","title":"P-value","text":"<p>The p-value is defined mathematically as:</p> <p>For a test statistic T and observed value t*: <pre><code>Two-sided: p = 2 * P(T \u2265 |t*| | H\u2080)\nUpper-tailed: p = P(T \u2265 t* | H\u2080)\nLower-tailed: p = P(T \u2264 t* | H\u2080)\n</code></pre></p> <p>For practical computation in Python: <pre><code>def calculate_p_value(test_statistic, distribution='normal', two_sided=True):\n    if distribution == 'normal':\n        if two_sided:\n            return 2 * (1 - stats.norm.cdf(abs(test_statistic)))\n        return 1 - stats.norm.cdf(test_statistic)\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#statistical-power","title":"Statistical Power","text":"<p>Power depends on four interrelated quantities: 1. Effect size (\u03b4) 2. Sample size (n) 3. Significance level (\u03b1) 4. Power (1-\u03b2)</p> <p>For a two-sided z-test, the power function is:</p> <pre><code>Power = 1 - \u03b2 = \u03a6(z\u03b1/2 + \u03b4\u221an/\u03c3) + \u03a6(-z\u03b1/2 + \u03b4\u221an/\u03c3)\n</code></pre> <p>where: - \u03a6 is the standard normal CDF - z\u03b1/2 is the critical value - \u03b4 is the true difference from null - \u03c3 is the population standard deviation</p>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#multiple-testing-problem","title":"Multiple Testing Problem","text":"<p>When conducting m independent tests at significance level \u03b1, the probability of at least one Type I error (Family-Wise Error Rate, FWER) is:</p> <pre><code>FWER = 1 - (1-\u03b1)\u1d50\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#bonferroni-correction","title":"Bonferroni Correction","text":"<p>Controls FWER by adjusting the significance level: <pre><code>\u03b1_adjusted = \u03b1/m\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#benjamini-hochberg-procedure","title":"Benjamini-Hochberg Procedure","text":"<p>Controls False Discovery Rate (FDR). For ordered p-values p\u2081 \u2264 p\u2082 \u2264 ... \u2264 p\u2098: 1. Find largest k where p_k \u2264 (k/m)\u03b1 2. Reject all hypotheses H\u208d\u1d62\u208e for i = 1,...,k</p> <p>Implementation combining mathematical insight with computation: <pre><code>def benjamini_hochberg(p_values, alpha=0.05):\n    \"\"\"\n    Implements Benjamini-Hochberg procedure\n    Returns: boolean array of rejected null hypotheses\n    \"\"\"\n    m = len(p_values)\n    ranked = stats.rankdata(p_values, method='min')\n    critical_values = (ranked / m) * alpha\n    sorted_p_values = np.sort(p_values)\n\n    # Find largest k where p_k \u2264 (k/m)\u03b1\n    significant = p_values &lt;= critical_values\n    return significant\n</code></pre></p>"},{"location":"grassroots/statistics/4_statistical_inference/hypothesis_testing/#power-analysis-example","title":"Power Analysis Example","text":"<p>Let's combine mathematical formulation with computation for a t-test power analysis:</p> <pre><code>def power_analysis(effect_size, n, alpha=0.05, two_sided=True):\n    \"\"\"\n    Calculate power for two-sample t-test\n\n    Parameters:\n    effect_size (d) = (\u03bc\u2081 - \u03bc\u2082)/\u03c3\n    n = sample size per group\n    \"\"\"\n    # Critical value\n    df = 2*n - 2  # degrees of freedom\n    if two_sided:\n        t_crit = stats.t.ppf(1 - alpha/2, df)\n    else:\n        t_crit = stats.t.ppf(1 - alpha, df)\n\n    # Non-centrality parameter\n    ncp = effect_size * np.sqrt(n/2)\n\n    # Power calculation\n    if two_sided:\n        power = (1 - stats.nct.cdf(t_crit, df, ncp) + \n                stats.nct.cdf(-t_crit, df, ncp))\n    else:\n        power = 1 - stats.nct.cdf(t_crit, df, ncp)\n\n    return power\n</code></pre> <p>This balanced approach shows both the mathematical foundation and its practical implementation, helping users understand both the theory and application of hypothesis testing.</p> <p>Remember: 1. Start with clear mathematical formulation 2. Provide intuitive explanations 3. Implement solutions efficiently 4. Consider computational aspects 5. Document assumptions and limitations</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/","title":"Sampling Theory","text":""},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#population-vs-sample","title":"Population vs Sample","text":"<p>The foundation of statistical inference lies in the relationship between populations and samples:</p> <ul> <li>Population: The complete set of all elements we want to study</li> <li>Sample: A subset of the population used to make inferences</li> </ul>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#mathematical-notation","title":"Mathematical Notation","text":"<ul> <li>Population parameters: \u03b8, \u03bc, \u03c3, \u03c0</li> <li>Sample statistics: \u03b8\u0302, x\u0304, s, p\u0302</li> </ul> <p>The relationship between population parameters and sample statistics can be expressed through expected values: * E[x\u0304] = \u03bc (unbiased estimator of mean) * E[s\u00b2] = \u03c3\u00b2 (unbiased estimator of variance)</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#sampling-distributions","title":"Sampling Distributions","text":"<p>The sampling distribution describes the probability distribution of a statistic across all possible samples of size n.</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#sample-mean","title":"Sample Mean","text":"<p>For a random sample X\u2081, X\u2082, ..., X\u2099: * Sample mean: x\u0304 = (1/n)\u2211X\u1d62 * Expected value: E[x\u0304] = \u03bc * Variance: Var(x\u0304) = \u03c3\u00b2/n</p> <pre><code># Implementation for empirical sampling distribution\ndef sample_mean_distribution(population, n_samples, sample_size):\n    means = [np.mean(np.random.choice(population, sample_size)) \n             for _ in range(n_samples)]\n    return np.array(means)\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#central-limit-theorem-clt","title":"Central Limit Theorem (CLT)","text":""},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>For independent, identically distributed random variables X\u2081, X\u2082, ..., X\u2099 with mean \u03bc and variance \u03c3\u00b2, the standardized sample mean:</p> <p>Z = (x\u0304 - \u03bc)/(\u03c3/\u221an) \u2192 N(0,1) as n \u2192 \u221e</p> <p>This means that for large n: x\u0304 \u223c N(\u03bc, \u03c3\u00b2/n)</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#intuition","title":"Intuition","text":"<p>The CLT tells us that regardless of the underlying distribution: 1. The sampling distribution of the mean becomes approximately normal 2. The spread of this distribution shrinks with \u221an 3. The center remains at the population mean</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#law-of-large-numbers-lln","title":"Law of Large Numbers (LLN)","text":""},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#weak-law-wlln","title":"Weak Law (WLLN)","text":"<p>For any \u03b5 &gt; 0: P(|x\u0304\u2099 - \u03bc| &gt; \u03b5) \u2192 0 as n \u2192 \u221e</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#strong-law-slln","title":"Strong Law (SLLN)","text":"<p>P(lim\u2099\u2192\u221e x\u0304\u2099 = \u03bc) = 1</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#intuition_1","title":"Intuition","text":"<ul> <li>WLLN: The probability of a large deviation from \u03bc becomes small</li> <li>SLLN: The sample mean will converge to \u03bc with probability 1</li> </ul>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#standard-error","title":"Standard Error","text":""},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#mathematical-definition","title":"Mathematical Definition","text":"<p>For any statistic \u03b8\u0302: SE(\u03b8\u0302) = \u221aVar(\u03b8\u0302)</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#common-forms","title":"Common Forms","text":"<ol> <li>Mean: SE(x\u0304) = \u03c3/\u221an</li> <li>Proportion: SE(p\u0302) = \u221a(p(1-p)/n)</li> <li>Difference of Means: SE(x\u0304\u2081 - x\u0304\u2082) = \u221a(\u03c3\u2081\u00b2/n\u2081 + \u03c3\u2082\u00b2/n\u2082)</li> </ol> <p>When population parameters are unknown, we use sample estimates: s/\u221an, \u221a(p\u0302(1-p\u0302)/n), etc.</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#practical-sampling-methods","title":"Practical Sampling Methods","text":""},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#simple-random-sampling","title":"Simple Random Sampling","text":"<p>Each subset of size n has equal probability of selection: P(selecting specific sample) = 1/\u208d\u2099\u1d3a\u208e</p> <pre><code>def simple_random_sample(population, size):\n    return np.random.choice(population, size=size, replace=False)\n</code></pre>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#stratified-sampling","title":"Stratified Sampling","text":"<p>For L strata with N\u2095 units in stratum h: * Stratum weight: W\u2095 = N\u2095/N * Stratified mean: x\u0304\u209b\u209c = \u2211W\u2095x\u0304\u2095 * Variance: Var(x\u0304\u209b\u209c) = \u2211W\u2095\u00b2\u03c3\u2095\u00b2/n\u2095</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#sample-size-determination","title":"Sample Size Determination","text":"<p>For desired margin of error E and confidence level \u03b1: * For means: n = (z\u03b1/2 \u00d7 \u03c3/E)\u00b2 * For proportions: n = (z\u03b1/2)\u00b2 \u00d7 p(1-p)/E\u00b2</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#relationship-to-statistical-inference","title":"Relationship to Statistical Inference","text":"<p>The theoretical foundations of sampling connect directly to inference through:</p> <ol> <li> <p>Confidence Intervals \u03b8\u0302 \u00b1 z\u03b1/2 \u00d7 SE(\u03b8\u0302)</p> </li> <li> <p>Hypothesis Tests Test statistic = (\u03b8\u0302 - \u03b8\u2080)/SE(\u03b8\u0302)</p> </li> </ol> <p>These applications rely on the sampling distribution theory developed above.</p>"},{"location":"grassroots/statistics/4_statistical_inference/sampling_theory/#example-one-sample-t-test","title":"Example: One-Sample t-test","text":"<p>Under H\u2080: \u03bc = \u03bc\u2080 t = (x\u0304 - \u03bc\u2080)/(s/\u221an) \u223c t(n-1)</p> <pre><code>def t_test_statistic(sample, null_mean):\n    return (np.mean(sample) - null_mean)/(np.std(sample, ddof=1)/np.sqrt(len(sample)))\n</code></pre> <p>Remember: 1. The mathematical theory provides the foundation 2. Computational methods help verify and visualize 3. Understanding both perspectives enhances statistical practice 4. Real-world applications often require both theoretical and practical tools</p>"},{"location":"grassroots/statistics/5_regression_analysis/","title":"Regression Analysis","text":""},{"location":"grassroots/statistics/5_regression_analysis/#simple-linear-regression","title":"Simple Linear Regression","text":""},{"location":"grassroots/statistics/5_regression_analysis/#model-fundamentals","title":"Model Fundamentals","text":"<p>Linear regression models the relationship between a dependent variable y and a single independent variable x: <pre><code>y = \u03b2\u2080 + \u03b2\u2081x + \u03b5\n</code></pre> where: - \u03b2\u2080 is the intercept - \u03b2\u2081 is the slope - \u03b5 is the error term</p>"},{"location":"grassroots/statistics/5_regression_analysis/#assumptions","title":"Assumptions","text":""},{"location":"grassroots/statistics/5_regression_analysis/#1-linearity","title":"1. Linearity","text":"<ul> <li>Relationship between x and y is linear</li> <li>Can be checked through scatter plots</li> </ul> <pre><code>def check_linearity(X, y):\n    \"\"\"\n    Plot data and check linearity assumption\n    \"\"\"\n    plt.scatter(X, y)\n    plt.xlabel('Independent Variable')\n    plt.ylabel('Dependent Variable')\n\n    # Add lowess smoother for comparison\n    from statsmodels.nonparametric.smoothers_lowess import lowess\n    smooth = lowess(y, X.ravel(), frac=2/3)\n    plt.plot(smooth[:, 0], smooth[:, 1], 'r-', label='LOWESS')\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#2-independence","title":"2. Independence","text":"<ul> <li>Observations are independent of each other</li> <li>No autocorrelation in residuals</li> </ul> <pre><code>def check_independence(residuals):\n    \"\"\"\n    Check independence using Durbin-Watson test\n    \"\"\"\n    from statsmodels.stats.stattools import durbin_watson\n    dw_stat = durbin_watson(residuals)\n\n    return {\n        'dw_statistic': dw_stat,\n        'is_independent': 1.5 &lt; dw_stat &lt; 2.5\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#3-homoscedasticity","title":"3. Homoscedasticity","text":"<ul> <li>Constant variance of residuals</li> <li>Can be checked through residual plots</li> </ul> <pre><code>def check_homoscedasticity(model, X, residuals):\n    \"\"\"\n    Plot residuals vs fitted values\n    \"\"\"\n    fitted_values = model.predict(X)\n    plt.scatter(fitted_values, residuals)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('Fitted Values')\n    plt.ylabel('Residuals')\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#4-normality-of-residuals","title":"4. Normality of Residuals","text":"<ul> <li>Residuals should be normally distributed</li> <li>Can be checked through QQ plots and tests</li> </ul> <pre><code>def check_normality(residuals):\n    \"\"\"\n    Check normality of residuals\n    \"\"\"\n    from scipy import stats\n\n    # QQ plot\n    stats.probplot(residuals, dist=\"norm\", plot=plt)\n\n    # Shapiro-Wilk test\n    _, p_value = stats.shapiro(residuals)\n\n    return {\n        'p_value': p_value,\n        'is_normal': p_value &gt; 0.05\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#least-squares-estimation","title":"Least Squares Estimation","text":""},{"location":"grassroots/statistics/5_regression_analysis/#ordinary-least-squares-ols","title":"Ordinary Least Squares (OLS)","text":"<pre><code>def fit_ols(X, y):\n    \"\"\"\n    Fit simple linear regression using OLS\n    \"\"\"\n    from sklearn.linear_model import LinearRegression\n    model = LinearRegression()\n    model.fit(X.reshape(-1, 1), y)\n\n    return {\n        'intercept': model.intercept_,\n        'slope': model.coef_[0],\n        'model': model\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#manual-implementation","title":"Manual Implementation","text":"<pre><code>def manual_ols(X, y):\n    \"\"\"\n    Manual implementation of OLS\n    \"\"\"\n    X_mean = np.mean(X)\n    y_mean = np.mean(y)\n\n    # Calculate slope\n    numerator = np.sum((X - X_mean) * (y - y_mean))\n    denominator = np.sum((X - X_mean)**2)\n    slope = numerator / denominator\n\n    # Calculate intercept\n    intercept = y_mean - slope * X_mean\n\n    return {\n        'intercept': intercept,\n        'slope': slope\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#r-squared-and-adjusted-r-squared","title":"R-squared and Adjusted R-squared","text":""},{"location":"grassroots/statistics/5_regression_analysis/#r-squared","title":"R-squared","text":"<pre><code>def calculate_r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate R-squared\n    \"\"\"\n    ss_total = np.sum((y_true - np.mean(y_true))**2)\n    ss_residual = np.sum((y_true - y_pred)**2)\n\n    r_squared = 1 - (ss_residual / ss_total)\n    return r_squared\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#adjusted-r-squared","title":"Adjusted R-squared","text":"<pre><code>def calculate_adjusted_r_squared(r_squared, n, p):\n    \"\"\"\n    Calculate adjusted R-squared\n    n: number of observations\n    p: number of predictors (1 for simple linear regression)\n    \"\"\"\n    adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n    return adjusted_r_squared\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#residual-analysis","title":"Residual Analysis","text":""},{"location":"grassroots/statistics/5_regression_analysis/#comprehensive-residual-analysis","title":"Comprehensive Residual Analysis","text":"<pre><code>class ResidualAnalyzer:\n    def __init__(self, model, X, y):\n        self.model = model\n        self.X = X\n        self.y = y\n        self.residuals = self.calculate_residuals()\n\n    def calculate_residuals(self):\n        \"\"\"Calculate residuals\"\"\"\n        y_pred = self.model.predict(self.X.reshape(-1, 1))\n        return self.y - y_pred\n\n    def standardized_residuals(self):\n        \"\"\"Calculate standardized residuals\"\"\"\n        return stats.zscore(self.residuals)\n\n    def run_all_checks(self):\n        \"\"\"Run all residual diagnostics\"\"\"\n        results = {\n            'normality': check_normality(self.residuals),\n            'independence': check_independence(self.residuals),\n            'homoscedasticity': self.check_homoscedasticity(),\n            'outliers': self.check_outliers()\n        }\n        return results\n\n    def check_outliers(self, threshold=3):\n        \"\"\"Check for outliers using standardized residuals\"\"\"\n        std_resid = self.standardized_residuals()\n        outliers = np.abs(std_resid) &gt; threshold\n        return {\n            'n_outliers': sum(outliers),\n            'outlier_indices': np.where(outliers)[0]\n        }\n\n    def check_homoscedasticity(self):\n        \"\"\"\n        Breusch-Pagan test for homoscedasticity\n        \"\"\"\n        from statsmodels.stats.diagnostic import het_breuschpagan\n\n        y_pred = self.model.predict(self.X.reshape(-1, 1))\n        _, p_value, _ = het_breuschpagan(self.residuals, self.X)\n\n        return {\n            'p_value': p_value,\n            'is_homoscedastic': p_value &gt; 0.05\n        }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#complete-regression-analysis","title":"Complete Regression Analysis","text":"<pre><code>def complete_regression_analysis(X, y):\n    \"\"\"\n    Perform complete regression analysis\n    \"\"\"\n    # Fit model\n    model = fit_ols(X, y)\n\n    # Get predictions\n    y_pred = model['model'].predict(X.reshape(-1, 1))\n\n    # Calculate metrics\n    r_squared = calculate_r_squared(y, y_pred)\n    adj_r_squared = calculate_adjusted_r_squared(r_squared, len(y), 1)\n\n    # Analyze residuals\n    analyzer = ResidualAnalyzer(model['model'], X, y)\n    residual_analysis = analyzer.run_all_checks()\n\n    return {\n        'model': model,\n        'r_squared': r_squared,\n        'adj_r_squared': adj_r_squared,\n        'residual_analysis': residual_analysis\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#best-practices","title":"Best Practices","text":""},{"location":"grassroots/statistics/5_regression_analysis/#model-building-process","title":"Model Building Process","text":"<ol> <li> <p>Data Preparation</p> <ul> <li>Check for missing values</li> <li>Handle outliers</li> <li>Scale if necessary</li> </ul> </li> <li> <p>Model Fitting</p> <ul> <li>Fit model using OLS</li> <li>Calculate confidence intervals</li> <li>Assess significance</li> </ul> </li> <li> <p>Model Validation</p> <ul> <li>Check assumptions</li> <li>Analyze residuals</li> <li>Assess fit metrics</li> </ul> </li> <li> <p>Reporting</p> <ul> <li>Coefficient estimates</li> <li>Standard errors</li> <li>R-squared values</li> <li>Assumption validations</li> </ul> </li> </ol> <p>Remember: 1. Always check assumptions 2. Look for influential points 3. Consider transformations if needed 4. Report comprehensive results 5. Interpret findings in context</p>"},{"location":"grassroots/statistics/5_regression_analysis/#simple-linear-regression-mathematical-foundations","title":"Simple Linear Regression: Mathematical Foundations","text":""},{"location":"grassroots/statistics/5_regression_analysis/#model-specification","title":"Model Specification","text":""},{"location":"grassroots/statistics/5_regression_analysis/#basic-form","title":"Basic Form","text":"<p>The simple linear regression model is expressed as: <pre><code>Y = \u03b2\u2080 + \u03b2\u2081X + \u03b5\n</code></pre> where: - Y is the dependent variable - X is the independent variable - \u03b2\u2080 is the y-intercept (value of Y when X = 0) - \u03b2\u2081 is the slope (change in Y for one unit change in X) - \u03b5 is the error term (random disturbance)</p>"},{"location":"grassroots/statistics/5_regression_analysis/#model-assumptions","title":"Model Assumptions","text":""},{"location":"grassroots/statistics/5_regression_analysis/#1-linearity_1","title":"1. Linearity","text":"<ul> <li>Mathematical Form: E[Y|X] = \u03b2\u2080 + \u03b2\u2081X</li> <li>Interpretation: The conditional expectation of Y given X is a linear function</li> <li>Violation Impact: Biased estimates, poor predictions</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#2-independence_1","title":"2. Independence","text":"<ul> <li>Mathematical Form: cov(\u03b5\u1d62, \u03b5\u2c7c) = 0 for all i \u2260 j</li> <li>Interpretation: No relationship between error terms</li> <li>Detection: Through autocorrelation function: \u03c1\u2096 = cov(\u03b5\u209c, \u03b5\u209c\u208a\u2096)/var(\u03b5)</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#3-homoscedasticity_1","title":"3. Homoscedasticity","text":"<ul> <li>Mathematical Form: var(\u03b5|X) = \u03c3\u00b2</li> <li>Interpretation: Constant variance of errors across all X values</li> <li>Violation Impact: Inefficient estimates, invalid standard errors</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#4-normality","title":"4. Normality","text":"<ul> <li>Mathematical Form: \u03b5 ~ N(0, \u03c3\u00b2)</li> <li>Interpretation: Errors follow normal distribution with mean 0 and constant variance</li> <li>Importance: Required for valid inference (t-tests, F-tests)</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#least-squares-estimation_1","title":"Least Squares Estimation","text":""},{"location":"grassroots/statistics/5_regression_analysis/#optimization-problem","title":"Optimization Problem","text":"<p>Find \u03b2\u2080 and \u03b2\u2081 that minimize the sum of squared residuals: <pre><code>min \u03a3(Y\u1d62 - \u03b2\u2080 - \u03b2\u2081X\u1d62)\u00b2\n</code></pre></p>"},{"location":"grassroots/statistics/5_regression_analysis/#solutions","title":"Solutions","text":"<p>The optimal estimates are given by: <pre><code>\u03b2\u0302\u2081 = \u03a3((X\u1d62 - X\u0304)(Y\u1d62 - \u0232)) / \u03a3(X\u1d62 - X\u0304)\u00b2\n\u03b2\u0302\u2080 = \u0232 - \u03b2\u0302\u2081X\u0304\n</code></pre> where X\u0304 and \u0232 are sample means</p>"},{"location":"grassroots/statistics/5_regression_analysis/#properties-of-ols-estimators","title":"Properties of OLS Estimators","text":"<ol> <li>Unbiasedness: E[\u03b2\u0302] = \u03b2</li> <li>Consistency: \u03b2\u0302 \u2192 \u03b2 as n \u2192 \u221e</li> <li>Efficiency: Minimum variance among linear unbiased estimators (BLUE)</li> <li>Sampling Distributions:    <pre><code>\u03b2\u0302\u2081 ~ N(\u03b2\u2081, \u03c3\u00b2/\u03a3(X\u1d62 - X\u0304)\u00b2)\n\u03b2\u0302\u2080 ~ N(\u03b2\u2080, \u03c3\u00b2(1/n + X\u0304\u00b2/\u03a3(X\u1d62 - X\u0304)\u00b2))\n</code></pre></li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/#measures-of-fit","title":"Measures of Fit","text":""},{"location":"grassroots/statistics/5_regression_analysis/#r-squared_1","title":"R-squared","text":"<ul> <li>Formula: R\u00b2 = 1 - SSE/SST   where:</li> <li>SSE = \u03a3(Y\u1d62 - \u0176\u1d62)\u00b2 (Sum of Squared Errors)</li> <li> <p>SST = \u03a3(Y\u1d62 - \u0232)\u00b2 (Total Sum of Squares)</p> </li> <li> <p>Interpretation: Proportion of variance in Y explained by X</p> </li> <li>Range: [0,1], with 1 indicating perfect fit</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#adjusted-r-squared_1","title":"Adjusted R-squared","text":"<ul> <li>Formula: R\u0304\u00b2 = 1 - (1-R\u00b2)(n-1)/(n-p-1)   where:</li> <li>n = sample size</li> <li> <p>p = number of predictors (1 for simple regression)</p> </li> <li> <p>Interpretation: R\u00b2 adjusted for model complexity</p> </li> <li>Advantage: Penalizes unnecessary predictors</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#residual-analysis_1","title":"Residual Analysis","text":""},{"location":"grassroots/statistics/5_regression_analysis/#residual-definition","title":"Residual Definition","text":"<pre><code>e\u1d62 = Y\u1d62 - \u0176\u1d62\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#standardized-residuals","title":"Standardized Residuals","text":"<p><pre><code>r\u1d62 = e\u1d62/(s\u221a(1-h\u1d62\u1d62))\n</code></pre> where: - s = standard error of regression - h\u1d62\u1d62 = leverage of observation i</p>"},{"location":"grassroots/statistics/5_regression_analysis/#key-properties","title":"Key Properties","text":"<ol> <li>Sum: \u03a3e\u1d62 = 0</li> <li>Correlation: \u03a3(X\u1d62e\u1d62) = 0</li> <li>Distribution: Under assumptions, e\u1d62/\u03c3 ~ N(0,1-h\u1d62\u1d62)</li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/#statistical-inference","title":"Statistical Inference","text":""},{"location":"grassroots/statistics/5_regression_analysis/#standard-errors","title":"Standard Errors","text":"<pre><code>SE(\u03b2\u0302\u2081) = \u03c3/\u221a(\u03a3(X\u1d62 - X\u0304)\u00b2)\nSE(\u03b2\u0302\u2080) = \u03c3\u221a(1/n + X\u0304\u00b2/\u03a3(X\u1d62 - X\u0304)\u00b2)\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>For slope: <pre><code>H\u2080: \u03b2\u2081 = 0\nH\u2081: \u03b2\u2081 \u2260 0\nt = \u03b2\u0302\u2081/SE(\u03b2\u0302\u2081) ~ t(n-2)\n</code></pre></p>"},{"location":"grassroots/statistics/5_regression_analysis/#confidence-intervals","title":"Confidence Intervals","text":"<pre><code>\u03b2\u0302\u2081 \u00b1 t(\u03b1/2,n-2)SE(\u03b2\u0302\u2081)\n\u03b2\u0302\u2080 \u00b1 t(\u03b1/2,n-2)SE(\u03b2\u0302\u2080)\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#prediction","title":"Prediction","text":""},{"location":"grassroots/statistics/5_regression_analysis/#point-prediction","title":"Point Prediction","text":"<p>For new observation X\u2080: <pre><code>\u0176\u2080 = \u03b2\u0302\u2080 + \u03b2\u0302\u2081X\u2080\n</code></pre></p>"},{"location":"grassroots/statistics/5_regression_analysis/#prediction-interval","title":"Prediction Interval","text":"<pre><code>\u0176\u2080 \u00b1 t(\u03b1/2,n-2)s\u221a(1 + 1/n + (X\u2080-X\u0304)\u00b2/\u03a3(X\u1d62-X\u0304)\u00b2)\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/#confidence-interval-for-mean-response","title":"Confidence Interval for Mean Response","text":"<pre><code>\u0176\u2080 \u00b1 t(\u03b1/2,n-2)s\u221a(1/n + (X\u2080-X\u0304)\u00b2/\u03a3(X\u1d62-X\u0304)\u00b2)\n</code></pre> <p>Remember: 1. Assumptions are crucial for valid inference 2. R\u00b2 alone is insufficient for model assessment 3. Residual analysis provides insight into model adequacy 4. Prediction intervals are wider than confidence intervals 5. Model simplicity aids interpretation</p>"},{"location":"grassroots/statistics/5_regression_analysis/#generalized-linear-models","title":"Generalized Linear Models","text":""},{"location":"grassroots/statistics/5_regression_analysis/#general-framework","title":"General Framework","text":""},{"location":"grassroots/statistics/5_regression_analysis/#components-of-glms","title":"Components of GLMs","text":"<ol> <li>Random Component: Response distribution from exponential family</li> <li>Systematic Component: Linear predictor \u03b7 = X\u03b2</li> <li>Link Function: g(\u03bc) = \u03b7, connecting mean to linear predictor</li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/#exponential-family-form","title":"Exponential Family Form","text":"<p><pre><code>f(y;\u03b8,\u03d5) = exp{[y\u03b8 - b(\u03b8)]/a(\u03d5) + c(y,\u03d5)}\n</code></pre> where: - \u03b8 is the natural parameter - \u03d5 is the dispersion parameter - b(\u03b8) is the cumulant function - a(\u03d5) is typically \u03d5/w (w is a known weight)</p>"},{"location":"grassroots/statistics/5_regression_analysis/#logistic-regression","title":"Logistic Regression","text":""},{"location":"grassroots/statistics/5_regression_analysis/#model-specification_1","title":"Model Specification","text":"<p>For binary response Y \u2208 {0,1}: <pre><code>logit(\u03c0) = log(\u03c0/(1-\u03c0)) = \u03b2\u2080 + \u03b2\u2081x\u2081 + ... + \u03b2\u2096x\u2096\n</code></pre> where \u03c0 = P(Y=1|X)</p>"},{"location":"grassroots/statistics/5_regression_analysis/#properties","title":"Properties","text":"<ul> <li>Response Distribution: Bernoulli/Binomial</li> <li>Link Function: logit(\u03c0) = log(\u03c0/(1-\u03c0))</li> <li>Mean Function: E(Y|X) = \u03c0 = exp(X\u03b2)/(1 + exp(X\u03b2))</li> <li>Variance Function: var(Y|X) = \u03c0(1-\u03c0)</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#interpretation","title":"Interpretation","text":"<ul> <li>Odds: \u03c0/(1-\u03c0)</li> <li>Odds Ratio: exp(\u03b2\u1d62) for unit change in x\u1d62</li> <li>Probability: \u03c0 = exp(X\u03b2)/(1 + exp(X\u03b2))</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","text":"<p>Log-likelihood: <pre><code>l(\u03b2) = \u03a3[y\u1d62log(\u03c0\u1d62) + (1-y\u1d62)log(1-\u03c0\u1d62)]\n</code></pre> Solved iteratively using Newton-Raphson or Fisher scoring</p>"},{"location":"grassroots/statistics/5_regression_analysis/#poisson-regression","title":"Poisson Regression","text":""},{"location":"grassroots/statistics/5_regression_analysis/#model-specification_2","title":"Model Specification","text":"<p>For count response Y: <pre><code>log(\u03bc) = \u03b2\u2080 + \u03b2\u2081x\u2081 + ... + \u03b2\u2096x\u2096\n</code></pre> where \u03bc = E(Y|X)</p>"},{"location":"grassroots/statistics/5_regression_analysis/#properties_1","title":"Properties","text":"<ul> <li>Response Distribution: Poisson</li> <li>Link Function: log(\u03bc)</li> <li>Mean Function: E(Y|X) = \u03bc = exp(X\u03b2)</li> <li>Variance Function: var(Y|X) = \u03bc</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#interpretation_1","title":"Interpretation","text":"<ul> <li>Rate Ratio: exp(\u03b2\u1d62) for unit change in x\u1d62</li> <li>Expected Count: exp(X\u03b2)</li> <li>Percent Change: 100(exp(\u03b2\u1d62)-1)% for unit change in x\u1d62</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#maximum-likelihood-estimation_1","title":"Maximum Likelihood Estimation","text":"<p>Log-likelihood: <pre><code>l(\u03b2) = \u03a3[y\u1d62log(\u03bc\u1d62) - \u03bc\u1d62 - log(y\u1d62!)]\n</code></pre></p>"},{"location":"grassroots/statistics/5_regression_analysis/#link-functions","title":"Link Functions","text":""},{"location":"grassroots/statistics/5_regression_analysis/#common-link-functions","title":"Common Link Functions","text":"<ol> <li>Identity: g(\u03bc) = \u03bc</li> <li>Used in linear regression</li> <li> <p>Range: (-\u221e, \u221e)</p> </li> <li> <p>Logit: g(\u03bc) = log(\u03bc/(1-\u03bc))</p> </li> <li>Used in logistic regression</li> <li> <p>Range: [0,1]</p> </li> <li> <p>Log: g(\u03bc) = log(\u03bc)</p> </li> <li>Used in Poisson regression</li> <li> <p>Range: (0, \u221e)</p> </li> <li> <p>Probit: g(\u03bc) = \u03a6\u207b\u00b9(\u03bc)</p> </li> <li>Alternative for binary data</li> <li>Range: [0,1]</li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/#canonical-links","title":"Canonical Links","text":"<p>For exponential family distributions: * Binomial: logit * Poisson: log * Normal: identity * Gamma: inverse</p>"},{"location":"grassroots/statistics/5_regression_analysis/#model-assessment","title":"Model Assessment","text":""},{"location":"grassroots/statistics/5_regression_analysis/#deviance","title":"Deviance","text":"<p><pre><code>D = 2[l(y;y) - l(\u03b2\u0302;y)]\n</code></pre> where l(y;y) is log-likelihood of saturated model</p>"},{"location":"grassroots/statistics/5_regression_analysis/#residuals","title":"Residuals","text":"<ol> <li> <p>Pearson Residuals: <pre><code>r\u209a = (y - \u03bc\u0302)/\u221a(V(\u03bc\u0302))\n</code></pre></p> </li> <li> <p>Deviance Residuals: <pre><code>r\u1d48 = sign(y - \u03bc\u0302)\u221a(d)\n</code></pre> where d is contribution to deviance</p> </li> <li> <p>Working Residuals: <pre><code>r\u02b7 = (y - \u03bc\u0302)g'(\u03bc\u0302)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/#goodness-of-fit","title":"Goodness of Fit","text":"<ol> <li>Deviance Test:</li> <li>Compare deviance to \u03c7\u00b2 distribution</li> <li> <p>Degrees of freedom = n - p</p> </li> <li> <p>Pearson \u03c7\u00b2 Test: <pre><code>X\u00b2 = \u03a3(y - \u03bc\u0302)\u00b2/V(\u03bc\u0302)\n</code></pre></p> </li> <li> <p>AIC: <pre><code>AIC = -2l(\u03b2\u0302) + 2p\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/#inference","title":"Inference","text":""},{"location":"grassroots/statistics/5_regression_analysis/#parameter-estimation","title":"Parameter Estimation","text":"<ul> <li>Standard Errors: \u221a(diagonal of (X'WX)\u207b\u00b9)</li> <li>Confidence Intervals: \u03b2\u0302\u1d62 \u00b1 z\u2081\u208b\u03b1/\u2082SE(\u03b2\u0302\u1d62)</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#hypothesis-testing_1","title":"Hypothesis Testing","text":"<ul> <li> <p>Wald Test: <pre><code>W = \u03b2\u0302/SE(\u03b2\u0302) ~ N(0,1)\n</code></pre></p> </li> <li> <p>Likelihood Ratio Test: <pre><code>LR = 2[l(\u03b2\u0302\u2081) - l(\u03b2\u0302\u2080)] ~ \u03c7\u00b2(df)\n</code></pre></p> </li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/#model-selection","title":"Model Selection","text":""},{"location":"grassroots/statistics/5_regression_analysis/#criteria","title":"Criteria","text":"<ol> <li>AIC: -2l(\u03b2\u0302) + 2p</li> <li>BIC: -2l(\u03b2\u0302) + p log(n)</li> <li>Deviance: Measure of model fit</li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/#stepwise-procedures","title":"Stepwise Procedures","text":"<ol> <li>Forward Selection: Add terms sequentially</li> <li>Backward Elimination: Remove terms sequentially</li> <li>Stepwise: Combination of forward and backward</li> </ol> <p>Remember: 1. Choice of link function affects interpretation 2. Check for overdispersion in count data 3. Assess model fit through residuals 4. Consider theoretical justification for model choice 5. Validate assumptions appropriate to chosen model</p>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/","title":"Generalized Linear Models","text":""},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#general-framework","title":"General Framework","text":""},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#components-of-glms","title":"Components of GLMs","text":"<ol> <li>Random Component: Response distribution from exponential family</li> <li>Systematic Component: Linear predictor \u03b7 = X\u03b2</li> <li>Link Function: g(\u03bc) = \u03b7, connecting mean to linear predictor</li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#exponential-family-form","title":"Exponential Family Form","text":"<p><pre><code>f(y;\u03b8,\u03d5) = exp{[y\u03b8 - b(\u03b8)]/a(\u03d5) + c(y,\u03d5)}\n</code></pre> where: - \u03b8 is the natural parameter - \u03d5 is the dispersion parameter - b(\u03b8) is the cumulant function - a(\u03d5) is typically \u03d5/w (w is a known weight)</p>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#logistic-regression","title":"Logistic Regression","text":""},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#model-specification","title":"Model Specification","text":"<p>For binary response Y \u2208 {0,1}: <pre><code>logit(\u03c0) = log(\u03c0/(1-\u03c0)) = \u03b2\u2080 + \u03b2\u2081x\u2081 + ... + \u03b2\u2096x\u2096\n</code></pre> where \u03c0 = P(Y=1|X)</p>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#properties","title":"Properties","text":"<ul> <li>Response Distribution: Bernoulli/Binomial</li> <li>Link Function: logit(\u03c0) = log(\u03c0/(1-\u03c0))</li> <li>Mean Function: E(Y|X) = \u03c0 = exp(X\u03b2)/(1 + exp(X\u03b2))</li> <li>Variance Function: var(Y|X) = \u03c0(1-\u03c0)</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#interpretation","title":"Interpretation","text":"<ul> <li>Odds: \u03c0/(1-\u03c0)</li> <li>Odds Ratio: exp(\u03b2\u1d62) for unit change in x\u1d62</li> <li>Probability: \u03c0 = exp(X\u03b2)/(1 + exp(X\u03b2))</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","text":"<p>Log-likelihood: <pre><code>l(\u03b2) = \u03a3[y\u1d62log(\u03c0\u1d62) + (1-y\u1d62)log(1-\u03c0\u1d62)]\n</code></pre> Solved iteratively using Newton-Raphson or Fisher scoring</p>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#poisson-regression","title":"Poisson Regression","text":""},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#model-specification_1","title":"Model Specification","text":"<p>For count response Y: <pre><code>log(\u03bc) = \u03b2\u2080 + \u03b2\u2081x\u2081 + ... + \u03b2\u2096x\u2096\n</code></pre> where \u03bc = E(Y|X)</p>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#properties_1","title":"Properties","text":"<ul> <li>Response Distribution: Poisson</li> <li>Link Function: log(\u03bc)</li> <li>Mean Function: E(Y|X) = \u03bc = exp(X\u03b2)</li> <li>Variance Function: var(Y|X) = \u03bc</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#interpretation_1","title":"Interpretation","text":"<ul> <li>Rate Ratio: exp(\u03b2\u1d62) for unit change in x\u1d62</li> <li>Expected Count: exp(X\u03b2)</li> <li>Percent Change: 100(exp(\u03b2\u1d62)-1)% for unit change in x\u1d62</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#maximum-likelihood-estimation_1","title":"Maximum Likelihood Estimation","text":"<p>Log-likelihood: <pre><code>l(\u03b2) = \u03a3[y\u1d62log(\u03bc\u1d62) - \u03bc\u1d62 - log(y\u1d62!)]\n</code></pre></p>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#link-functions","title":"Link Functions","text":""},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#common-link-functions","title":"Common Link Functions","text":"<ol> <li>Identity: g(\u03bc) = \u03bc</li> <li>Used in linear regression</li> <li> <p>Range: (-\u221e, \u221e)</p> </li> <li> <p>Logit: g(\u03bc) = log(\u03bc/(1-\u03bc))</p> </li> <li>Used in logistic regression</li> <li> <p>Range: [0,1]</p> </li> <li> <p>Log: g(\u03bc) = log(\u03bc)</p> </li> <li>Used in Poisson regression</li> <li> <p>Range: (0, \u221e)</p> </li> <li> <p>Probit: g(\u03bc) = \u03a6\u207b\u00b9(\u03bc)</p> </li> <li>Alternative for binary data</li> <li>Range: [0,1]</li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#canonical-links","title":"Canonical Links","text":"<p>For exponential family distributions: * Binomial: logit * Poisson: log * Normal: identity * Gamma: inverse</p>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#model-assessment","title":"Model Assessment","text":""},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#deviance","title":"Deviance","text":"<p><pre><code>D = 2[l(y;y) - l(\u03b2\u0302;y)]\n</code></pre> where l(y;y) is log-likelihood of saturated model</p>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#residuals","title":"Residuals","text":"<ol> <li> <p>Pearson Residuals: <pre><code>r\u209a = (y - \u03bc\u0302)/\u221a(V(\u03bc\u0302))\n</code></pre></p> </li> <li> <p>Deviance Residuals: <pre><code>r\u1d48 = sign(y - \u03bc\u0302)\u221a(d)\n</code></pre> where d is contribution to deviance</p> </li> <li> <p>Working Residuals: <pre><code>r\u02b7 = (y - \u03bc\u0302)g'(\u03bc\u0302)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#goodness-of-fit","title":"Goodness of Fit","text":"<ol> <li>Deviance Test:</li> <li>Compare deviance to \u03c7\u00b2 distribution</li> <li> <p>Degrees of freedom = n - p</p> </li> <li> <p>Pearson \u03c7\u00b2 Test: <pre><code>X\u00b2 = \u03a3(y - \u03bc\u0302)\u00b2/V(\u03bc\u0302)\n</code></pre></p> </li> <li> <p>AIC: <pre><code>AIC = -2l(\u03b2\u0302) + 2p\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#inference","title":"Inference","text":""},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#parameter-estimation","title":"Parameter Estimation","text":"<ul> <li>Standard Errors: \u221a(diagonal of (X'WX)\u207b\u00b9)</li> <li>Confidence Intervals: \u03b2\u0302\u1d62 \u00b1 z\u2081\u208b\u03b1/\u2082SE(\u03b2\u0302\u1d62)</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#hypothesis-testing","title":"Hypothesis Testing","text":"<ul> <li> <p>Wald Test: <pre><code>W = \u03b2\u0302/SE(\u03b2\u0302) ~ N(0,1)\n</code></pre></p> </li> <li> <p>Likelihood Ratio Test: <pre><code>LR = 2[l(\u03b2\u0302\u2081) - l(\u03b2\u0302\u2080)] ~ \u03c7\u00b2(df)\n</code></pre></p> </li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#model-selection","title":"Model Selection","text":""},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#criteria","title":"Criteria","text":"<ol> <li>AIC: -2l(\u03b2\u0302) + 2p</li> <li>BIC: -2l(\u03b2\u0302) + p log(n)</li> <li>Deviance: Measure of model fit</li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/generalized_linear_models/#stepwise-procedures","title":"Stepwise Procedures","text":"<ol> <li>Forward Selection: Add terms sequentially</li> <li>Backward Elimination: Remove terms sequentially</li> <li>Stepwise: Combination of forward and backward</li> </ol> <p>Remember: 1. Choice of link function affects interpretation 2. Check for overdispersion in count data 3. Assess model fit through residuals 4. Consider theoretical justification for model choice 5. Validate assumptions appropriate to chosen model</p>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/","title":"Simple Linear Regression: Mathematical Foundations","text":""},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#model-specification","title":"Model Specification","text":""},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#basic-form","title":"Basic Form","text":"<p>The simple linear regression model is expressed as: <pre><code>Y = \u03b2\u2080 + \u03b2\u2081X + \u03b5\n</code></pre> where: - Y is the dependent variable - X is the independent variable - \u03b2\u2080 is the y-intercept (value of Y when X = 0) - \u03b2\u2081 is the slope (change in Y for one unit change in X) - \u03b5 is the error term (random disturbance)</p>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#model-assumptions","title":"Model Assumptions","text":""},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#1-linearity","title":"1. Linearity","text":"<ul> <li>Mathematical Form: E[Y|X] = \u03b2\u2080 + \u03b2\u2081X</li> <li>Interpretation: The conditional expectation of Y given X is a linear function</li> <li>Violation Impact: Biased estimates, poor predictions</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#2-independence","title":"2. Independence","text":"<ul> <li>Mathematical Form: cov(\u03b5\u1d62, \u03b5\u2c7c) = 0 for all i \u2260 j</li> <li>Interpretation: No relationship between error terms</li> <li>Detection: Through autocorrelation function: \u03c1\u2096 = cov(\u03b5\u209c, \u03b5\u209c\u208a\u2096)/var(\u03b5)</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#3-homoscedasticity","title":"3. Homoscedasticity","text":"<ul> <li>Mathematical Form: var(\u03b5|X) = \u03c3\u00b2</li> <li>Interpretation: Constant variance of errors across all X values</li> <li>Violation Impact: Inefficient estimates, invalid standard errors</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#4-normality","title":"4. Normality","text":"<ul> <li>Mathematical Form: \u03b5 ~ N(0, \u03c3\u00b2)</li> <li>Interpretation: Errors follow normal distribution with mean 0 and constant variance</li> <li>Importance: Required for valid inference (t-tests, F-tests)</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#least-squares-estimation","title":"Least Squares Estimation","text":""},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#optimization-problem","title":"Optimization Problem","text":"<p>Find \u03b2\u2080 and \u03b2\u2081 that minimize the sum of squared residuals: <pre><code>min \u03a3(Y\u1d62 - \u03b2\u2080 - \u03b2\u2081X\u1d62)\u00b2\n</code></pre></p>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#solutions","title":"Solutions","text":"<p>The optimal estimates are given by: <pre><code>\u03b2\u0302\u2081 = \u03a3((X\u1d62 - X\u0304)(Y\u1d62 - \u0232)) / \u03a3(X\u1d62 - X\u0304)\u00b2\n\u03b2\u0302\u2080 = \u0232 - \u03b2\u0302\u2081X\u0304\n</code></pre> where X\u0304 and \u0232 are sample means</p>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#properties-of-ols-estimators","title":"Properties of OLS Estimators","text":"<ol> <li>Unbiasedness: E[\u03b2\u0302] = \u03b2</li> <li>Consistency: \u03b2\u0302 \u2192 \u03b2 as n \u2192 \u221e</li> <li>Efficiency: Minimum variance among linear unbiased estimators (BLUE)</li> <li>Sampling Distributions:    <pre><code>\u03b2\u0302\u2081 ~ N(\u03b2\u2081, \u03c3\u00b2/\u03a3(X\u1d62 - X\u0304)\u00b2)\n\u03b2\u0302\u2080 ~ N(\u03b2\u2080, \u03c3\u00b2(1/n + X\u0304\u00b2/\u03a3(X\u1d62 - X\u0304)\u00b2))\n</code></pre></li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#measures-of-fit","title":"Measures of Fit","text":""},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#r-squared","title":"R-squared","text":"<ul> <li>Formula: R\u00b2 = 1 - SSE/SST   where:</li> <li>SSE = \u03a3(Y\u1d62 - \u0176\u1d62)\u00b2 (Sum of Squared Errors)</li> <li> <p>SST = \u03a3(Y\u1d62 - \u0232)\u00b2 (Total Sum of Squares)</p> </li> <li> <p>Interpretation: Proportion of variance in Y explained by X</p> </li> <li>Range: [0,1], with 1 indicating perfect fit</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#adjusted-r-squared","title":"Adjusted R-squared","text":"<ul> <li>Formula: R\u0304\u00b2 = 1 - (1-R\u00b2)(n-1)/(n-p-1)   where:</li> <li>n = sample size</li> <li> <p>p = number of predictors (1 for simple regression)</p> </li> <li> <p>Interpretation: R\u00b2 adjusted for model complexity</p> </li> <li>Advantage: Penalizes unnecessary predictors</li> </ul>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#residual-analysis","title":"Residual Analysis","text":""},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#residual-definition","title":"Residual Definition","text":"<pre><code>e\u1d62 = Y\u1d62 - \u0176\u1d62\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#standardized-residuals","title":"Standardized Residuals","text":"<p><pre><code>r\u1d62 = e\u1d62/(s\u221a(1-h\u1d62\u1d62))\n</code></pre> where: - s = standard error of regression - h\u1d62\u1d62 = leverage of observation i</p>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#key-properties","title":"Key Properties","text":"<ol> <li>Sum: \u03a3e\u1d62 = 0</li> <li>Correlation: \u03a3(X\u1d62e\u1d62) = 0</li> <li>Distribution: Under assumptions, e\u1d62/\u03c3 ~ N(0,1-h\u1d62\u1d62)</li> </ol>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#statistical-inference","title":"Statistical Inference","text":""},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#standard-errors","title":"Standard Errors","text":"<pre><code>SE(\u03b2\u0302\u2081) = \u03c3/\u221a(\u03a3(X\u1d62 - X\u0304)\u00b2)\nSE(\u03b2\u0302\u2080) = \u03c3\u221a(1/n + X\u0304\u00b2/\u03a3(X\u1d62 - X\u0304)\u00b2)\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>For slope: <pre><code>H\u2080: \u03b2\u2081 = 0\nH\u2081: \u03b2\u2081 \u2260 0\nt = \u03b2\u0302\u2081/SE(\u03b2\u0302\u2081) ~ t(n-2)\n</code></pre></p>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#confidence-intervals","title":"Confidence Intervals","text":"<pre><code>\u03b2\u0302\u2081 \u00b1 t(\u03b1/2,n-2)SE(\u03b2\u0302\u2081)\n\u03b2\u0302\u2080 \u00b1 t(\u03b1/2,n-2)SE(\u03b2\u0302\u2080)\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#prediction","title":"Prediction","text":""},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#point-prediction","title":"Point Prediction","text":"<p>For new observation X\u2080: <pre><code>\u0176\u2080 = \u03b2\u0302\u2080 + \u03b2\u0302\u2081X\u2080\n</code></pre></p>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#prediction-interval","title":"Prediction Interval","text":"<pre><code>\u0176\u2080 \u00b1 t(\u03b1/2,n-2)s\u221a(1 + 1/n + (X\u2080-X\u0304)\u00b2/\u03a3(X\u1d62-X\u0304)\u00b2)\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/multiple_linear_regression/#confidence-interval-for-mean-response","title":"Confidence Interval for Mean Response","text":"<pre><code>\u0176\u2080 \u00b1 t(\u03b1/2,n-2)s\u221a(1/n + (X\u2080-X\u0304)\u00b2/\u03a3(X\u1d62-X\u0304)\u00b2)\n</code></pre> <p>Remember: 1. Assumptions are crucial for valid inference 2. R\u00b2 alone is insufficient for model assessment 3. Residual analysis provides insight into model adequacy 4. Prediction intervals are wider than confidence intervals 5. Model simplicity aids interpretation</p>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/","title":"Simple Linear Regression","text":""},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#model-fundamentals","title":"Model Fundamentals","text":"<p>Linear regression models the relationship between a dependent variable y and a single independent variable x: <pre><code>y = \u03b2\u2080 + \u03b2\u2081x + \u03b5\n</code></pre> where: - \u03b2\u2080 is the intercept - \u03b2\u2081 is the slope - \u03b5 is the error term</p>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#assumptions","title":"Assumptions","text":""},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#1-linearity","title":"1. Linearity","text":"<ul> <li>Relationship between x and y is linear</li> <li>Can be checked through scatter plots</li> </ul> <pre><code>def check_linearity(X, y):\n    \"\"\"\n    Plot data and check linearity assumption\n    \"\"\"\n    plt.scatter(X, y)\n    plt.xlabel('Independent Variable')\n    plt.ylabel('Dependent Variable')\n\n    # Add lowess smoother for comparison\n    from statsmodels.nonparametric.smoothers_lowess import lowess\n    smooth = lowess(y, X.ravel(), frac=2/3)\n    plt.plot(smooth[:, 0], smooth[:, 1], 'r-', label='LOWESS')\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#2-independence","title":"2. Independence","text":"<ul> <li>Observations are independent of each other</li> <li>No autocorrelation in residuals</li> </ul> <pre><code>def check_independence(residuals):\n    \"\"\"\n    Check independence using Durbin-Watson test\n    \"\"\"\n    from statsmodels.stats.stattools import durbin_watson\n    dw_stat = durbin_watson(residuals)\n\n    return {\n        'dw_statistic': dw_stat,\n        'is_independent': 1.5 &lt; dw_stat &lt; 2.5\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#3-homoscedasticity","title":"3. Homoscedasticity","text":"<ul> <li>Constant variance of residuals</li> <li>Can be checked through residual plots</li> </ul> <pre><code>def check_homoscedasticity(model, X, residuals):\n    \"\"\"\n    Plot residuals vs fitted values\n    \"\"\"\n    fitted_values = model.predict(X)\n    plt.scatter(fitted_values, residuals)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.xlabel('Fitted Values')\n    plt.ylabel('Residuals')\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#4-normality-of-residuals","title":"4. Normality of Residuals","text":"<ul> <li>Residuals should be normally distributed</li> <li>Can be checked through QQ plots and tests</li> </ul> <pre><code>def check_normality(residuals):\n    \"\"\"\n    Check normality of residuals\n    \"\"\"\n    from scipy import stats\n\n    # QQ plot\n    stats.probplot(residuals, dist=\"norm\", plot=plt)\n\n    # Shapiro-Wilk test\n    _, p_value = stats.shapiro(residuals)\n\n    return {\n        'p_value': p_value,\n        'is_normal': p_value &gt; 0.05\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#least-squares-estimation","title":"Least Squares Estimation","text":""},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#ordinary-least-squares-ols","title":"Ordinary Least Squares (OLS)","text":"<pre><code>def fit_ols(X, y):\n    \"\"\"\n    Fit simple linear regression using OLS\n    \"\"\"\n    from sklearn.linear_model import LinearRegression\n    model = LinearRegression()\n    model.fit(X.reshape(-1, 1), y)\n\n    return {\n        'intercept': model.intercept_,\n        'slope': model.coef_[0],\n        'model': model\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#manual-implementation","title":"Manual Implementation","text":"<pre><code>def manual_ols(X, y):\n    \"\"\"\n    Manual implementation of OLS\n    \"\"\"\n    X_mean = np.mean(X)\n    y_mean = np.mean(y)\n\n    # Calculate slope\n    numerator = np.sum((X - X_mean) * (y - y_mean))\n    denominator = np.sum((X - X_mean)**2)\n    slope = numerator / denominator\n\n    # Calculate intercept\n    intercept = y_mean - slope * X_mean\n\n    return {\n        'intercept': intercept,\n        'slope': slope\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#r-squared-and-adjusted-r-squared","title":"R-squared and Adjusted R-squared","text":""},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#r-squared","title":"R-squared","text":"<pre><code>def calculate_r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate R-squared\n    \"\"\"\n    ss_total = np.sum((y_true - np.mean(y_true))**2)\n    ss_residual = np.sum((y_true - y_pred)**2)\n\n    r_squared = 1 - (ss_residual / ss_total)\n    return r_squared\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#adjusted-r-squared","title":"Adjusted R-squared","text":"<pre><code>def calculate_adjusted_r_squared(r_squared, n, p):\n    \"\"\"\n    Calculate adjusted R-squared\n    n: number of observations\n    p: number of predictors (1 for simple linear regression)\n    \"\"\"\n    adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n    return adjusted_r_squared\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#residual-analysis","title":"Residual Analysis","text":""},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#comprehensive-residual-analysis","title":"Comprehensive Residual Analysis","text":"<pre><code>class ResidualAnalyzer:\n    def __init__(self, model, X, y):\n        self.model = model\n        self.X = X\n        self.y = y\n        self.residuals = self.calculate_residuals()\n\n    def calculate_residuals(self):\n        \"\"\"Calculate residuals\"\"\"\n        y_pred = self.model.predict(self.X.reshape(-1, 1))\n        return self.y - y_pred\n\n    def standardized_residuals(self):\n        \"\"\"Calculate standardized residuals\"\"\"\n        return stats.zscore(self.residuals)\n\n    def run_all_checks(self):\n        \"\"\"Run all residual diagnostics\"\"\"\n        results = {\n            'normality': check_normality(self.residuals),\n            'independence': check_independence(self.residuals),\n            'homoscedasticity': self.check_homoscedasticity(),\n            'outliers': self.check_outliers()\n        }\n        return results\n\n    def check_outliers(self, threshold=3):\n        \"\"\"Check for outliers using standardized residuals\"\"\"\n        std_resid = self.standardized_residuals()\n        outliers = np.abs(std_resid) &gt; threshold\n        return {\n            'n_outliers': sum(outliers),\n            'outlier_indices': np.where(outliers)[0]\n        }\n\n    def check_homoscedasticity(self):\n        \"\"\"\n        Breusch-Pagan test for homoscedasticity\n        \"\"\"\n        from statsmodels.stats.diagnostic import het_breuschpagan\n\n        y_pred = self.model.predict(self.X.reshape(-1, 1))\n        _, p_value, _ = het_breuschpagan(self.residuals, self.X)\n\n        return {\n            'p_value': p_value,\n            'is_homoscedastic': p_value &gt; 0.05\n        }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#complete-regression-analysis","title":"Complete Regression Analysis","text":"<pre><code>def complete_regression_analysis(X, y):\n    \"\"\"\n    Perform complete regression analysis\n    \"\"\"\n    # Fit model\n    model = fit_ols(X, y)\n\n    # Get predictions\n    y_pred = model['model'].predict(X.reshape(-1, 1))\n\n    # Calculate metrics\n    r_squared = calculate_r_squared(y, y_pred)\n    adj_r_squared = calculate_adjusted_r_squared(r_squared, len(y), 1)\n\n    # Analyze residuals\n    analyzer = ResidualAnalyzer(model['model'], X, y)\n    residual_analysis = analyzer.run_all_checks()\n\n    return {\n        'model': model,\n        'r_squared': r_squared,\n        'adj_r_squared': adj_r_squared,\n        'residual_analysis': residual_analysis\n    }\n</code></pre>"},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#best-practices","title":"Best Practices","text":""},{"location":"grassroots/statistics/5_regression_analysis/simple_linear_regression/#model-building-process","title":"Model Building Process","text":"<ol> <li> <p>Data Preparation</p> <ul> <li>Check for missing values</li> <li>Handle outliers</li> <li>Scale if necessary</li> </ul> </li> <li> <p>Model Fitting</p> <ul> <li>Fit model using OLS</li> <li>Calculate confidence intervals</li> <li>Assess significance</li> </ul> </li> <li> <p>Model Validation</p> <ul> <li>Check assumptions</li> <li>Analyze residuals</li> <li>Assess fit metrics</li> </ul> </li> <li> <p>Reporting</p> <ul> <li>Coefficient estimates</li> <li>Standard errors</li> <li>R-squared values</li> <li>Assumption validations</li> </ul> </li> </ol> <p>Remember: 1. Always check assumptions 2. Look for influential points 3. Consider transformations if needed 4. Report comprehensive results 5. Interpret findings in context</p>"},{"location":"grassroots/statistics/6_experimental_design/","title":"Experimental Design","text":""},{"location":"grassroots/statistics/6_experimental_design/#basic-principles-of-experimental-design","title":"Basic Principles of Experimental Design","text":""},{"location":"grassroots/statistics/6_experimental_design/#randomization","title":"Randomization","text":""},{"location":"grassroots/statistics/6_experimental_design/#principle","title":"Principle","text":"<p>Randomization is the random assignment of experimental units to treatments, ensuring: - Each unit has equal probability of receiving any treatment - Statistical independence of observations - Validity of statistical inference</p>"},{"location":"grassroots/statistics/6_experimental_design/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>For n experimental units and k treatments: * Probability of assignment = 1/k for each treatment * Number of possible assignments = n!/(n\u2081!n\u2082!...n\u2096!) where n\u1d62 is the number of units assigned to treatment i</p>"},{"location":"grassroots/statistics/6_experimental_design/#key-benefits","title":"Key Benefits","text":"<ol> <li>Controls for unknown confounding variables</li> <li>Reduces selection bias</li> <li>Allows valid statistical inference</li> <li>Balances uncontrolled variables</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#replication","title":"Replication","text":""},{"location":"grassroots/statistics/6_experimental_design/#definition","title":"Definition","text":"<p>True replication involves independent observations under the same treatment conditions.</p>"},{"location":"grassroots/statistics/6_experimental_design/#types-of-replication","title":"Types of Replication","text":"<ol> <li>True Replication</li> <li>Independent experimental units</li> <li>Same treatment conditions</li> <li> <p>Independent measurements</p> </li> <li> <p>Pseudo-replication</p> </li> <li>Multiple measurements on same unit</li> <li>Not true independent observations</li> <li>Limited statistical validity</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#statistical-importance","title":"Statistical Importance","text":"<ul> <li>Enables estimation of experimental error</li> <li>Improves precision of estimates</li> <li>Sample size determination: <pre><code>n = 2(z\u03b1/2 + z\u03b2)\u00b2\u03c3\u00b2/\u03b4\u00b2\n</code></pre> where:</li> <li>\u03c3\u00b2 is variance</li> <li>\u03b4 is minimum detectable difference</li> <li>\u03b1 is Type I error rate</li> <li>\u03b2 is Type II error rate</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#blocking","title":"Blocking","text":""},{"location":"grassroots/statistics/6_experimental_design/#principle_1","title":"Principle","text":"<p>Grouping experimental units into homogeneous blocks to: * Reduce known sources of variation * Increase precision of treatment comparisons * Control for nuisance factors</p>"},{"location":"grassroots/statistics/6_experimental_design/#mathematical-model","title":"Mathematical Model","text":"<p>For randomized complete block design: <pre><code>Y\u1d62\u2c7c = \u03bc + \u03c4\u1d62 + \u03b2\u2c7c + \u03b5\u1d62\u2c7c\n</code></pre> where: - Y\u1d62\u2c7c is response for treatment i in block j - \u03bc is overall mean - \u03c4\u1d62 is treatment effect - \u03b2\u2c7c is block effect - \u03b5\u1d62\u2c7c is random error</p>"},{"location":"grassroots/statistics/6_experimental_design/#efficiency","title":"Efficiency","text":"<ul> <li>Relative efficiency vs completely randomized design: <pre><code>RE = (EMS\u2081/EMS\u2082)(df\u2082/df\u2081)\n</code></pre> where EMS is error mean square</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#types-of-blocks","title":"Types of Blocks","text":"<ol> <li>Physical Blocks</li> <li>Spatial grouping</li> <li>Environmental conditions</li> <li> <p>Time periods</p> </li> <li> <p>Statistical Blocks</p> </li> <li>Covariates</li> <li>Matched pairs</li> <li>Repeated measures</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#factorial-designs","title":"Factorial Designs","text":""},{"location":"grassroots/statistics/6_experimental_design/#structure","title":"Structure","text":"<ul> <li>Multiple factors studied simultaneously</li> <li>All possible combinations of factor levels</li> <li>Enables study of interactions</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#mathematical-model_1","title":"Mathematical Model","text":"<p>For two-factor factorial: <pre><code>Y\u1d62\u2c7c\u2096 = \u03bc + \u03b1\u1d62 + \u03b2\u2c7c + (\u03b1\u03b2)\u1d62\u2c7c + \u03b5\u1d62\u2c7c\u2096\n</code></pre> where: - \u03b1\u1d62 is effect of factor A - \u03b2\u2c7c is effect of factor B - (\u03b1\u03b2)\u1d62\u2c7c is interaction effect - \u03b5\u1d62\u2c7c\u2096 is random error</p>"},{"location":"grassroots/statistics/6_experimental_design/#properties","title":"Properties","text":"<ol> <li>Main Effects</li> <li>Average effect of factor across levels of other factors</li> <li> <p>Marginal means comparison</p> </li> <li> <p>Interactions</p> </li> <li>Non-additive effects</li> <li>Factor effects depend on levels of other factors</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#design-efficiency","title":"Design Efficiency","text":"<ul> <li>Number of runs = product of factor levels</li> <li>Degrees of freedom partition: <pre><code>Total df = (a\u00d7b\u00d7r) - 1\n</code></pre> where:</li> <li>a is levels of factor A</li> <li>b is levels of factor B</li> <li>r is replications</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#design-considerations","title":"Design Considerations","text":""},{"location":"grassroots/statistics/6_experimental_design/#1-treatment-structure","title":"1. Treatment Structure","text":"<ul> <li>Number of treatments</li> <li>Factor levels</li> <li>Interactions of interest</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#2-experimental-unit","title":"2. Experimental Unit","text":"<ul> <li>Definition and size</li> <li>Independence</li> <li>Homogeneity</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#3-design-structure","title":"3. Design Structure","text":"<ul> <li>Randomization restrictions</li> <li>Block size and number</li> <li>Resource constraints</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#4-sample-size","title":"4. Sample Size","text":"<ul> <li>Power considerations</li> <li>Resource limitations</li> <li>Practical constraints</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#analysis-principles","title":"Analysis Principles","text":""},{"location":"grassroots/statistics/6_experimental_design/#1-anova-decomposition","title":"1. ANOVA Decomposition","text":"<pre><code>SS_Total = SS_Treatment + SS_Block + SS_Error\n</code></pre>"},{"location":"grassroots/statistics/6_experimental_design/#2-error-structure","title":"2. Error Structure","text":"<ul> <li>Independent errors</li> <li>Constant variance</li> <li>Normal distribution</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/#3-testing-hierarchy","title":"3. Testing Hierarchy","text":"<ol> <li>Interaction tests</li> <li>Main effects (if interactions non-significant)</li> <li>Simple effects (if interactions significant)</li> </ol> <p>Remember: 1. Randomization provides validity 2. Replication provides precision 3. Blocking increases efficiency 4. Factorial designs study interactions 5. Design choice affects analysis options</p>"},{"location":"grassroots/statistics/6_experimental_design/#ab-testing","title":"A/B Testing","text":""},{"location":"grassroots/statistics/6_experimental_design/#sample-size-determination","title":"Sample Size Determination","text":""},{"location":"grassroots/statistics/6_experimental_design/#basic-formula","title":"Basic Formula","text":"<p>For comparing two proportions: <pre><code>n = 2(z\u03b1/2 + z\u03b2)\u00b2[p\u2081(1-p\u2081) + p\u2082(1-p\u2082)] / (p\u2081-p\u2082)\u00b2\n</code></pre> where: - n is sample size per group - \u03b1 is significance level - \u03b2 is Type II error rate (1-power) - p\u2081, p\u2082 are expected proportions - z\u03b1/2, z\u03b2 are standard normal quantiles</p>"},{"location":"grassroots/statistics/6_experimental_design/#for-continuous-outcomes","title":"For Continuous Outcomes","text":"<p><pre><code>n = 2(z\u03b1/2 + z\u03b2)\u00b2\u03c3\u00b2 / \u03b4\u00b2\n</code></pre> where: - \u03c3\u00b2 is pooled variance - \u03b4 is minimum detectable effect</p>"},{"location":"grassroots/statistics/6_experimental_design/#practical-considerations","title":"Practical Considerations","text":"<ol> <li>Effect Size Specification</li> <li>Minimum Detectable Effect (MDE)</li> <li>Business meaningful difference</li> <li> <p>Historical effect sizes</p> </li> <li> <p>Power Analysis Components</p> </li> <li>Baseline metrics</li> <li>Expected variance</li> <li>Desired power (typically 0.8 or 0.9)</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#statistical-significance","title":"Statistical Significance","text":""},{"location":"grassroots/statistics/6_experimental_design/#hypothesis-framework","title":"Hypothesis Framework","text":"<pre><code>H\u2080: p\u2081 = p\u2082 (or \u03bc\u2081 = \u03bc\u2082)\nH\u2081: p\u2081 \u2260 p\u2082 (or \u03bc\u2081 \u2260 \u03bc\u2082)\n</code></pre>"},{"location":"grassroots/statistics/6_experimental_design/#test-statistics","title":"Test Statistics","text":"<ol> <li> <p>For Proportions (z-test): <pre><code>z = (p\u0302\u2081 - p\u0302\u2082) / \u221a[p\u0302(1-p\u0302)(1/n\u2081 + 1/n\u2082)]\n</code></pre> where p\u0302 is pooled proportion</p> </li> <li> <p>For Means (t-test): <pre><code>t = (x\u0304\u2081 - x\u0304\u2082) / \u221a(s\u00b2\u2081/n\u2081 + s\u00b2\u2082/n\u2082)\n</code></pre> with Welch's correction for unequal variances</p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#sequential-analysis","title":"Sequential Analysis","text":"<p>For continuous monitoring: * O'Brien-Fleming Boundaries: <pre><code>z_k = C/\u221a(k/K)\n</code></pre> where: - k is current look - K is total looks - C is critical value</p>"},{"location":"grassroots/statistics/6_experimental_design/#effect-size","title":"Effect Size","text":""},{"location":"grassroots/statistics/6_experimental_design/#standardized-measures","title":"Standardized Measures","text":"<ol> <li> <p>Cohen's d: <pre><code>d = (\u03bc\u2081 - \u03bc\u2082) / \u03c3\u209a\n</code></pre> where \u03c3\u209a is pooled standard deviation</p> </li> <li> <p>Relative Difference: <pre><code>\u0394% = (\u03bc\u2081 - \u03bc\u2082) / \u03bc\u2082 \u00d7 100\n</code></pre></p> </li> <li> <p>Risk Ratio: <pre><code>RR = p\u2081/p\u2082\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#confidence-intervals","title":"Confidence Intervals","text":"<ol> <li> <p>For Difference in Proportions: <pre><code>(p\u0302\u2081 - p\u0302\u2082) \u00b1 z\u03b1/2\u221a[p\u0302\u2081(1-p\u0302\u2081)/n\u2081 + p\u0302\u2082(1-p\u0302\u2082)/n\u2082]\n</code></pre></p> </li> <li> <p>For Difference in Means: <pre><code>(x\u0304\u2081 - x\u0304\u2082) \u00b1 t\u03b1/2\u221a(s\u00b2\u2081/n\u2081 + s\u00b2\u2082/n\u2082)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#multiple-testing-corrections","title":"Multiple Testing Corrections","text":""},{"location":"grassroots/statistics/6_experimental_design/#family-wise-error-rate-fwer","title":"Family-Wise Error Rate (FWER)","text":"<ol> <li> <p>Bonferroni Correction: <pre><code>\u03b1' = \u03b1/m\n</code></pre> where m is number of tests</p> </li> <li> <p>\u0160id\u00e1k Correction: <pre><code>\u03b1' = 1 - (1-\u03b1)^(1/m)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#false-discovery-rate-fdr","title":"False Discovery Rate (FDR)","text":"<ol> <li>Benjamini-Hochberg Procedure:</li> <li>Order p-values: p\u2081 \u2264 p\u2082 \u2264 ... \u2264 p\u2098</li> <li> <p>Find largest k where: <pre><code>p\u2096 \u2264 (k/m)\u03b1\n</code></pre></p> </li> <li> <p>Critical Value Function: <pre><code>\u03b1(i) = (i/m)\u03b1\n</code></pre> where i is rank of p-value</p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#sequential-testing","title":"Sequential Testing","text":"<ol> <li>Alpha Spending Function: <pre><code>\u03b1*(t) = \u03b1 \u00d7 t^\u03c8\n</code></pre> where:</li> <li>t is information fraction</li> <li> <p>\u03c8 is spending parameter</p> </li> <li> <p>Error Spending Boundaries: <pre><code>b(t) = \u221a[2log(log(1/t))]\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#practical-implementation","title":"Practical Implementation","text":""},{"location":"grassroots/statistics/6_experimental_design/#design-considerations_1","title":"Design Considerations","text":"<ol> <li>Pre-experiment</li> <li>Define success metrics</li> <li>Specify MDE</li> <li>Calculate duration</li> <li> <p>Set stopping rules</p> </li> <li> <p>During Experiment</p> </li> <li>Monitor implementation</li> <li>Check randomization</li> <li>Track sample sizes</li> <li> <p>Watch for validity threats</p> </li> <li> <p>Post-experiment</p> </li> <li>Check assumptions</li> <li>Apply corrections</li> <li>Document findings</li> <li>Make recommendations</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Statistical</li> <li>Peeking at results</li> <li>Insufficient power</li> <li>Multiple testing</li> <li> <p>Simpson's paradox</p> </li> <li> <p>Practical</p> </li> <li>Selection bias</li> <li>Network effects</li> <li>Seasonality</li> <li>Novelty effects</li> </ol> <p>Remember: 1. Power analysis before testing 2. Clear success criteria 3. Appropriate corrections for multiple tests 4. Context-appropriate effect size measures 5. Document all decisions and assumptions</p>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/","title":"A/B Testing","text":""},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#sample-size-determination","title":"Sample Size Determination","text":""},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#basic-formula","title":"Basic Formula","text":"<p>For comparing two proportions: <pre><code>n = 2(z\u03b1/2 + z\u03b2)\u00b2[p\u2081(1-p\u2081) + p\u2082(1-p\u2082)] / (p\u2081-p\u2082)\u00b2\n</code></pre> where: - n is sample size per group - \u03b1 is significance level - \u03b2 is Type II error rate (1-power) - p\u2081, p\u2082 are expected proportions - z\u03b1/2, z\u03b2 are standard normal quantiles</p>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#for-continuous-outcomes","title":"For Continuous Outcomes","text":"<p><pre><code>n = 2(z\u03b1/2 + z\u03b2)\u00b2\u03c3\u00b2 / \u03b4\u00b2\n</code></pre> where: - \u03c3\u00b2 is pooled variance - \u03b4 is minimum detectable effect</p>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#practical-considerations","title":"Practical Considerations","text":"<ol> <li>Effect Size Specification</li> <li>Minimum Detectable Effect (MDE)</li> <li>Business meaningful difference</li> <li> <p>Historical effect sizes</p> </li> <li> <p>Power Analysis Components</p> </li> <li>Baseline metrics</li> <li>Expected variance</li> <li>Desired power (typically 0.8 or 0.9)</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#statistical-significance","title":"Statistical Significance","text":""},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#hypothesis-framework","title":"Hypothesis Framework","text":"<pre><code>H\u2080: p\u2081 = p\u2082 (or \u03bc\u2081 = \u03bc\u2082)\nH\u2081: p\u2081 \u2260 p\u2082 (or \u03bc\u2081 \u2260 \u03bc\u2082)\n</code></pre>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#test-statistics","title":"Test Statistics","text":"<ol> <li> <p>For Proportions (z-test): <pre><code>z = (p\u0302\u2081 - p\u0302\u2082) / \u221a[p\u0302(1-p\u0302)(1/n\u2081 + 1/n\u2082)]\n</code></pre> where p\u0302 is pooled proportion</p> </li> <li> <p>For Means (t-test): <pre><code>t = (x\u0304\u2081 - x\u0304\u2082) / \u221a(s\u00b2\u2081/n\u2081 + s\u00b2\u2082/n\u2082)\n</code></pre> with Welch's correction for unequal variances</p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#sequential-analysis","title":"Sequential Analysis","text":"<p>For continuous monitoring: * O'Brien-Fleming Boundaries: <pre><code>z_k = C/\u221a(k/K)\n</code></pre> where: - k is current look - K is total looks - C is critical value</p>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#effect-size","title":"Effect Size","text":""},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#standardized-measures","title":"Standardized Measures","text":"<ol> <li> <p>Cohen's d: <pre><code>d = (\u03bc\u2081 - \u03bc\u2082) / \u03c3\u209a\n</code></pre> where \u03c3\u209a is pooled standard deviation</p> </li> <li> <p>Relative Difference: <pre><code>\u0394% = (\u03bc\u2081 - \u03bc\u2082) / \u03bc\u2082 \u00d7 100\n</code></pre></p> </li> <li> <p>Risk Ratio: <pre><code>RR = p\u2081/p\u2082\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#confidence-intervals","title":"Confidence Intervals","text":"<ol> <li> <p>For Difference in Proportions: <pre><code>(p\u0302\u2081 - p\u0302\u2082) \u00b1 z\u03b1/2\u221a[p\u0302\u2081(1-p\u0302\u2081)/n\u2081 + p\u0302\u2082(1-p\u0302\u2082)/n\u2082]\n</code></pre></p> </li> <li> <p>For Difference in Means: <pre><code>(x\u0304\u2081 - x\u0304\u2082) \u00b1 t\u03b1/2\u221a(s\u00b2\u2081/n\u2081 + s\u00b2\u2082/n\u2082)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#multiple-testing-corrections","title":"Multiple Testing Corrections","text":""},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#family-wise-error-rate-fwer","title":"Family-Wise Error Rate (FWER)","text":"<ol> <li> <p>Bonferroni Correction: <pre><code>\u03b1' = \u03b1/m\n</code></pre> where m is number of tests</p> </li> <li> <p>\u0160id\u00e1k Correction: <pre><code>\u03b1' = 1 - (1-\u03b1)^(1/m)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#false-discovery-rate-fdr","title":"False Discovery Rate (FDR)","text":"<ol> <li>Benjamini-Hochberg Procedure:</li> <li>Order p-values: p\u2081 \u2264 p\u2082 \u2264 ... \u2264 p\u2098</li> <li> <p>Find largest k where: <pre><code>p\u2096 \u2264 (k/m)\u03b1\n</code></pre></p> </li> <li> <p>Critical Value Function: <pre><code>\u03b1(i) = (i/m)\u03b1\n</code></pre> where i is rank of p-value</p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#sequential-testing","title":"Sequential Testing","text":"<ol> <li>Alpha Spending Function: <pre><code>\u03b1*(t) = \u03b1 \u00d7 t^\u03c8\n</code></pre> where:</li> <li>t is information fraction</li> <li> <p>\u03c8 is spending parameter</p> </li> <li> <p>Error Spending Boundaries: <pre><code>b(t) = \u221a[2log(log(1/t))]\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#practical-implementation","title":"Practical Implementation","text":""},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#design-considerations","title":"Design Considerations","text":"<ol> <li>Pre-experiment</li> <li>Define success metrics</li> <li>Specify MDE</li> <li>Calculate duration</li> <li> <p>Set stopping rules</p> </li> <li> <p>During Experiment</p> </li> <li>Monitor implementation</li> <li>Check randomization</li> <li>Track sample sizes</li> <li> <p>Watch for validity threats</p> </li> <li> <p>Post-experiment</p> </li> <li>Check assumptions</li> <li>Apply corrections</li> <li>Document findings</li> <li>Make recommendations</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/a_b_testing/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Statistical</li> <li>Peeking at results</li> <li>Insufficient power</li> <li>Multiple testing</li> <li> <p>Simpson's paradox</p> </li> <li> <p>Practical</p> </li> <li>Selection bias</li> <li>Network effects</li> <li>Seasonality</li> <li>Novelty effects</li> </ol> <p>Remember: 1. Power analysis before testing 2. Clear success criteria 3. Appropriate corrections for multiple tests 4. Context-appropriate effect size measures 5. Document all decisions and assumptions</p>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/","title":"Basic Principles of Experimental Design","text":""},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#randomization","title":"Randomization","text":""},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#principle","title":"Principle","text":"<p>Randomization is the random assignment of experimental units to treatments, ensuring: - Each unit has equal probability of receiving any treatment - Statistical independence of observations - Validity of statistical inference</p>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>For n experimental units and k treatments: * Probability of assignment = 1/k for each treatment * Number of possible assignments = n!/(n\u2081!n\u2082!...n\u2096!) where n\u1d62 is the number of units assigned to treatment i</p>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#key-benefits","title":"Key Benefits","text":"<ol> <li>Controls for unknown confounding variables</li> <li>Reduces selection bias</li> <li>Allows valid statistical inference</li> <li>Balances uncontrolled variables</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#replication","title":"Replication","text":""},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#definition","title":"Definition","text":"<p>True replication involves independent observations under the same treatment conditions.</p>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#types-of-replication","title":"Types of Replication","text":"<ol> <li>True Replication</li> <li>Independent experimental units</li> <li>Same treatment conditions</li> <li> <p>Independent measurements</p> </li> <li> <p>Pseudo-replication</p> </li> <li>Multiple measurements on same unit</li> <li>Not true independent observations</li> <li>Limited statistical validity</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#statistical-importance","title":"Statistical Importance","text":"<ul> <li>Enables estimation of experimental error</li> <li>Improves precision of estimates</li> <li>Sample size determination: <pre><code>n = 2(z\u03b1/2 + z\u03b2)\u00b2\u03c3\u00b2/\u03b4\u00b2\n</code></pre> where:</li> <li>\u03c3\u00b2 is variance</li> <li>\u03b4 is minimum detectable difference</li> <li>\u03b1 is Type I error rate</li> <li>\u03b2 is Type II error rate</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#blocking","title":"Blocking","text":""},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#principle_1","title":"Principle","text":"<p>Grouping experimental units into homogeneous blocks to: * Reduce known sources of variation * Increase precision of treatment comparisons * Control for nuisance factors</p>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#mathematical-model","title":"Mathematical Model","text":"<p>For randomized complete block design: <pre><code>Y\u1d62\u2c7c = \u03bc + \u03c4\u1d62 + \u03b2\u2c7c + \u03b5\u1d62\u2c7c\n</code></pre> where: - Y\u1d62\u2c7c is response for treatment i in block j - \u03bc is overall mean - \u03c4\u1d62 is treatment effect - \u03b2\u2c7c is block effect - \u03b5\u1d62\u2c7c is random error</p>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#efficiency","title":"Efficiency","text":"<ul> <li>Relative efficiency vs completely randomized design: <pre><code>RE = (EMS\u2081/EMS\u2082)(df\u2082/df\u2081)\n</code></pre> where EMS is error mean square</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#types-of-blocks","title":"Types of Blocks","text":"<ol> <li>Physical Blocks</li> <li>Spatial grouping</li> <li>Environmental conditions</li> <li> <p>Time periods</p> </li> <li> <p>Statistical Blocks</p> </li> <li>Covariates</li> <li>Matched pairs</li> <li>Repeated measures</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#factorial-designs","title":"Factorial Designs","text":""},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#structure","title":"Structure","text":"<ul> <li>Multiple factors studied simultaneously</li> <li>All possible combinations of factor levels</li> <li>Enables study of interactions</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#mathematical-model_1","title":"Mathematical Model","text":"<p>For two-factor factorial: <pre><code>Y\u1d62\u2c7c\u2096 = \u03bc + \u03b1\u1d62 + \u03b2\u2c7c + (\u03b1\u03b2)\u1d62\u2c7c + \u03b5\u1d62\u2c7c\u2096\n</code></pre> where: - \u03b1\u1d62 is effect of factor A - \u03b2\u2c7c is effect of factor B - (\u03b1\u03b2)\u1d62\u2c7c is interaction effect - \u03b5\u1d62\u2c7c\u2096 is random error</p>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#properties","title":"Properties","text":"<ol> <li>Main Effects</li> <li>Average effect of factor across levels of other factors</li> <li> <p>Marginal means comparison</p> </li> <li> <p>Interactions</p> </li> <li>Non-additive effects</li> <li>Factor effects depend on levels of other factors</li> </ol>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#design-efficiency","title":"Design Efficiency","text":"<ul> <li>Number of runs = product of factor levels</li> <li>Degrees of freedom partition: <pre><code>Total df = (a\u00d7b\u00d7r) - 1\n</code></pre> where:</li> <li>a is levels of factor A</li> <li>b is levels of factor B</li> <li>r is replications</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#design-considerations","title":"Design Considerations","text":""},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#1-treatment-structure","title":"1. Treatment Structure","text":"<ul> <li>Number of treatments</li> <li>Factor levels</li> <li>Interactions of interest</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#2-experimental-unit","title":"2. Experimental Unit","text":"<ul> <li>Definition and size</li> <li>Independence</li> <li>Homogeneity</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#3-design-structure","title":"3. Design Structure","text":"<ul> <li>Randomization restrictions</li> <li>Block size and number</li> <li>Resource constraints</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#4-sample-size","title":"4. Sample Size","text":"<ul> <li>Power considerations</li> <li>Resource limitations</li> <li>Practical constraints</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#analysis-principles","title":"Analysis Principles","text":""},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#1-anova-decomposition","title":"1. ANOVA Decomposition","text":"<pre><code>SS_Total = SS_Treatment + SS_Block + SS_Error\n</code></pre>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#2-error-structure","title":"2. Error Structure","text":"<ul> <li>Independent errors</li> <li>Constant variance</li> <li>Normal distribution</li> </ul>"},{"location":"grassroots/statistics/6_experimental_design/basic_principles/#3-testing-hierarchy","title":"3. Testing Hierarchy","text":"<ol> <li>Interaction tests</li> <li>Main effects (if interactions non-significant)</li> <li>Simple effects (if interactions significant)</li> </ol> <p>Remember: 1. Randomization provides validity 2. Replication provides precision 3. Blocking increases efficiency 4. Factorial designs study interactions 5. Design choice affects analysis options</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/","title":"Bayesian Statistics","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#fundamentals-of-bayesian-statistics","title":"Fundamentals of Bayesian Statistics","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#bayes-theorem","title":"Bayes' Theorem","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#basic-form","title":"Basic Form","text":"<p><pre><code>P(\u03b8|X) = P(X|\u03b8)P(\u03b8) / P(X)\n</code></pre> where: - P(\u03b8|X) is posterior probability - P(X|\u03b8) is likelihood - P(\u03b8) is prior probability - P(X) is marginal likelihood (normalizing constant)</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/#expanded-form","title":"Expanded Form","text":"<p><pre><code>P(X) = \u222bP(X|\u03b8)P(\u03b8)d\u03b8\n</code></pre> or for discrete case: <pre><code>P(X) = \u03a3P(X|\u03b8)P(\u03b8)\n</code></pre></p>"},{"location":"grassroots/statistics/7_bayesian_statistics/#alternative-expression","title":"Alternative Expression","text":"<p><pre><code>Posterior \u221d Likelihood \u00d7 Prior\n</code></pre> avoiding computation of normalizing constant</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/#prior-and-posterior-distributions","title":"Prior and Posterior Distributions","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#prior-distributions","title":"Prior Distributions","text":"<ol> <li>Informative Priors</li> <li>Based on previous knowledge</li> <li>Expert opinion</li> <li>Historical data</li> <li> <p>Example: N(\u03bc\u2080, \u03c3\u2080\u00b2) for known mean</p> </li> <li> <p>Non-informative Priors</p> </li> <li>Uniform distribution</li> <li>Jeffreys prior: \u221a|I(\u03b8)|</li> <li>Reference priors</li> <li> <p>Example: P(\u03b8) \u221d 1 for location parameter</p> </li> <li> <p>Hierarchical Priors</p> </li> <li>Parameters have their own priors</li> <li>Hyperparameters    <pre><code>P(\u03b8) = \u222bP(\u03b8|\u03b7)P(\u03b7)d\u03b7\n</code></pre></li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#posterior-distributions","title":"Posterior Distributions","text":"<ol> <li>Point Estimates</li> <li>Posterior mean: E[\u03b8|X]</li> <li>Posterior median</li> <li> <p>Maximum a posteriori (MAP)</p> </li> <li> <p>Interval Estimates</p> </li> <li>Credible intervals</li> <li> <p>Highest posterior density (HPD)    <pre><code>P(a \u2264 \u03b8 \u2264 b|X) = 1-\u03b1\n</code></pre></p> </li> <li> <p>Posterior Predictive <pre><code>P(X\u0303|X) = \u222bP(X\u0303|\u03b8)P(\u03b8|X)d\u03b8\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#conjugate-priors","title":"Conjugate Priors","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#definition","title":"Definition","text":"<p>Prior and posterior from same family of distributions</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/#common-conjugate-pairs","title":"Common Conjugate Pairs","text":"<ol> <li>Binomial-Beta</li> <li>Likelihood: Binomial(n,\u03b8)</li> <li>Prior: Beta(\u03b1,\u03b2)</li> <li> <p>Posterior: Beta(\u03b1+x, \u03b2+n-x)    where x is number of successes</p> </li> <li> <p>Normal-Normal</p> </li> <li>Likelihood: N(\u03b8,\u03c3\u00b2)</li> <li>Prior: N(\u03bc\u2080,\u03c3\u2080\u00b2)</li> <li> <p>Posterior: N(\u03bc\u2099,\u03c3\u2099\u00b2)    where:    <pre><code>\u03bc\u2099 = (\u03c3\u207b\u00b2X\u0304n + \u03c3\u2080\u207b\u00b2\u03bc\u2080)/(\u03c3\u207b\u00b2n + \u03c3\u2080\u207b\u00b2)\n\u03c3\u2099\u00b2 = 1/(\u03c3\u207b\u00b2n + \u03c3\u2080\u207b\u00b2)\n</code></pre></p> </li> <li> <p>Poisson-Gamma</p> </li> <li>Likelihood: Poisson(\u03b8)</li> <li>Prior: Gamma(\u03b1,\u03b2)</li> <li>Posterior: Gamma(\u03b1+\u03a3x, \u03b2+n)</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#bayesian-inference","title":"Bayesian Inference","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#parameter-estimation","title":"Parameter Estimation","text":"<ol> <li> <p>Point Estimation <pre><code>\u03b8\u0302 = E[\u03b8|X] = \u222b\u03b8P(\u03b8|X)d\u03b8\n</code></pre></p> </li> <li> <p>Interval Estimation</p> </li> <li>Equal-tailed interval:    <pre><code>[\u03b8\u2097,\u03b8\u1d64]: P(\u03b8 &lt; \u03b8\u2097|X) = P(\u03b8 &gt; \u03b8\u1d64|X) = \u03b1/2\n</code></pre></li> <li>HPD interval:    <pre><code>P(\u03b8\u2208[\u03b8\u2097,\u03b8\u1d64]|X) = 1-\u03b1\n</code></pre>    minimizing \u03b8\u1d64-\u03b8\u2097</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#hypothesis-testing","title":"Hypothesis Testing","text":"<ol> <li>Bayes Factor: <pre><code>BF\u2081\u2080 = P(X|H\u2081)/P(X|H\u2080)\n</code></pre> Interpretation:</li> <li>BF\u2081\u2080 &gt; 1: Evidence for H\u2081</li> <li> <p>BF\u2081\u2080 &lt; 1: Evidence for H\u2080</p> </li> <li> <p>Posterior Probability: <pre><code>P(H\u2081|X) = P(X|H\u2081)P(H\u2081)/P(X)\n</code></pre></p> </li> <li> <p>Decision Theory: <pre><code>d* = argmin_d \u222bL(d,\u03b8)P(\u03b8|X)d\u03b8\n</code></pre> where L is loss function</p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#model-comparison","title":"Model Comparison","text":"<ol> <li> <p>Bayesian Model Averaging: <pre><code>P(\u03b8|X) = \u03a3P(\u03b8|M_k,X)P(M_k|X)\n</code></pre></p> </li> <li> <p>DIC (Deviance Information Criterion): <pre><code>DIC = D\u0304 + pD\n</code></pre> where:</p> </li> <li>D\u0304 is expected deviance</li> <li>pD is effective number of parameters</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#practical-considerations","title":"Practical Considerations","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#prior-selection","title":"Prior Selection","text":"<ol> <li>Sensitivity Analysis</li> <li>Multiple priors</li> <li>Impact on conclusions</li> <li> <p>Robustness checks</p> </li> <li> <p>Elicitation</p> </li> <li>Expert knowledge</li> <li>Historical data</li> <li>Meta-analysis</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#computation","title":"Computation","text":"<ol> <li>Analytical Solutions</li> <li>Conjugate priors</li> <li> <p>Simple models</p> </li> <li> <p>Numerical Methods</p> </li> <li>MCMC</li> <li>Variational inference</li> <li>Laplace approximation</li> </ol> <p>Remember: 1. Prior specification is crucial 2. Conjugate priors simplify computation 3. Interpretation differs from frequentist 4. Model checking is important 5. Consider computational feasibility</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/#bayesian-statistical-applications","title":"Bayesian Statistical Applications","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#bayesian-ab-testing","title":"Bayesian A/B Testing","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#model-formulation","title":"Model Formulation","text":"<ol> <li> <p>Binomial Model For conversion rates: <pre><code>Group A: y\u2090 ~ Binomial(n\u2090, \u03b8\u2090)\nGroup B: y\u1d66 ~ Binomial(n\u1d66, \u03b8\u1d66)\nPriors: \u03b8\u2090, \u03b8\u1d66 ~ Beta(\u03b1, \u03b2)\n</code></pre></p> </li> <li> <p>Normal Model For continuous metrics: <pre><code>Group A: y\u2090 ~ N(\u03bc\u2090, \u03c3\u00b2)\nGroup B: y\u1d66 ~ N(\u03bc\u1d66, \u03c3\u00b2)\nPriors: \u03bc\u2090, \u03bc\u1d66 ~ N(\u03bc\u2080, \u03c3\u2080\u00b2)\n        \u03c3\u00b2 ~ InvGamma(\u03b1, \u03b2)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#inference","title":"Inference","text":"<ol> <li> <p>Probability of Improvement <pre><code>P(\u03b8\u1d66 &gt; \u03b8\u2090|data) = \u222b\u222bI(\u03b8\u1d66 &gt; \u03b8\u2090)p(\u03b8\u2090,\u03b8\u1d66|data)d\u03b8\u2090d\u03b8\u1d66\n</code></pre></p> </li> <li> <p>Expected Lift <pre><code>E[\u03b8\u1d66 - \u03b8\u2090|data] = E[\u03b8\u1d66|data] - E[\u03b8\u2090|data]\n</code></pre></p> </li> <li> <p>Risk Assessment <pre><code>P(\u03b8\u1d66 - \u03b8\u2090 &gt; \u03b4|data)  # Probability of meaningful difference\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#bayesian-regression","title":"Bayesian Regression","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#linear-regression-model","title":"Linear Regression Model","text":"<pre><code>y = X\u03b2 + \u03b5\n\u03b5 ~ N(0, \u03c3\u00b2I)\n</code></pre>"},{"location":"grassroots/statistics/7_bayesian_statistics/#prior-specifications","title":"Prior Specifications","text":"<ol> <li> <p>Coefficients <pre><code>\u03b2 ~ N(\u03bc\u2080, \u03a3\u2080)  # Multivariate normal prior\n</code></pre></p> </li> <li> <p>Variance <pre><code>\u03c3\u00b2 ~ InvGamma(\u03b1, \u03b2)  # Inverse gamma prior\n</code></pre></p> </li> <li> <p>Joint Posterior <pre><code>p(\u03b2,\u03c3\u00b2|y) \u221d p(y|\u03b2,\u03c3\u00b2)p(\u03b2)p(\u03c3\u00b2)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#posterior-inference","title":"Posterior Inference","text":"<ol> <li> <p>Parameter Estimation <pre><code>E[\u03b2|y] = (X'X + \u03c3\u00b2\u03a3\u2080\u207b\u00b9)\u207b\u00b9(X'y + \u03c3\u00b2\u03a3\u2080\u207b\u00b9\u03bc\u2080)\nVar(\u03b2|y) = \u03c3\u00b2(X'X + \u03c3\u00b2\u03a3\u2080\u207b\u00b9)\u207b\u00b9\n</code></pre></p> </li> <li> <p>Prediction <pre><code>p(\u1ef9|x\u0303,y) = \u222b\u222bp(\u1ef9|x\u0303,\u03b2,\u03c3\u00b2)p(\u03b2,\u03c3\u00b2|y)d\u03b2d\u03c3\u00b2\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#hierarchical-models","title":"Hierarchical Models","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/#general-structure","title":"General Structure","text":"<pre><code>Level 1 (Data): y_i ~ p(y|\u03b8\u1d62)\nLevel 2 (Parameters): \u03b8\u1d62 ~ p(\u03b8|\u03b7)\nLevel 3 (Hyperparameters): \u03b7 ~ p(\u03b7)\n</code></pre>"},{"location":"grassroots/statistics/7_bayesian_statistics/#hierarchical-linear-model","title":"Hierarchical Linear Model","text":"<ol> <li> <p>Model Specification <pre><code>y\u1d62\u2c7c = \u03b2\u2080\u2c7c + \u03b2\u2081\u2c7cx\u1d62\u2c7c + \u03b5\u1d62\u2c7c\n\u03b2\u2080\u2c7c = \u03b3\u2080\u2080 + \u03b3\u2080\u2081w\u2c7c + u\u2080\u2c7c\n\u03b2\u2081\u2c7c = \u03b3\u2081\u2080 + \u03b3\u2081\u2081w\u2c7c + u\u2081\u2c7c\n</code></pre></p> </li> <li> <p>Distribution Assumptions <pre><code>\u03b5\u1d62\u2c7c ~ N(0, \u03c3\u00b2)\n[u\u2080\u2c7c, u\u2081\u2c7c]' ~ N(0, \u03a3)\n</code></pre></p> </li> <li> <p>Prior Specifications <pre><code>\u03b3 ~ N(\u03bc\u03b3, \u03a3\u03b3)\n\u03c3\u00b2 ~ InvGamma(\u03b1\u2081, \u03b2\u2081)\n\u03a3 ~ InvWishart(\u03bd, S)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#hierarchical-logistic-regression","title":"Hierarchical Logistic Regression","text":"<ol> <li> <p>Model Structure <pre><code>y\u1d62\u2c7c ~ Bernoulli(p\u1d62\u2c7c)\nlogit(p\u1d62\u2c7c) = \u03b2\u2080\u2c7c + \u03b2\u2081\u2c7cx\u1d62\u2c7c\n\u03b2\u2080\u2c7c ~ N(\u03bc\u2080, \u03c4\u2080\u00b2)\n\u03b2\u2081\u2c7c ~ N(\u03bc\u2081, \u03c4\u2081\u00b2)\n</code></pre></p> </li> <li> <p>Hyperpriors <pre><code>\u03bc\u2080,\u03bc\u2081 ~ N(0, \u03c3\u00b2)\n\u03c4\u2080\u00b2,\u03c4\u2081\u00b2 ~ InvGamma(\u03b1, \u03b2)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#implementation-considerations","title":"Implementation Considerations","text":"<ol> <li>Model Building</li> <li>Start simple</li> <li>Add complexity gradually</li> <li>Check convergence</li> <li> <p>Assess model fit</p> </li> <li> <p>Prior Selection</p> </li> <li>Weakly informative</li> <li>Domain knowledge</li> <li> <p>Sensitivity analysis</p> </li> <li> <p>Computational Methods</p> </li> <li>MCMC (Gibbs, Metropolis-Hastings)</li> <li>Hamiltonian Monte Carlo</li> <li>Variational inference</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/#diagnostics-and-model-checking","title":"Diagnostics and Model Checking","text":"<ol> <li> <p>MCMC Diagnostics <pre><code>R\u0302 (Gelman-Rubin statistic)\nEffective sample size\nTrace plots\nAutocorrelation\n</code></pre></p> </li> <li> <p>Posterior Predictive Checks <pre><code>yrep ~ p(y|\u03b8)  # Simulated data\nT(yrep) vs T(y)  # Test statistics comparison\n</code></pre></p> </li> <li> <p>Model Comparison <pre><code>WAIC = -2(lppd - pWAIC)\nDIC = D\u0304 + pD\nLOO-CV = \u03a3\u1d62log p(y\u1d62|y\u208b\u1d62)\n</code></pre></p> </li> </ol> <p>Remember: 1. Model complexity matches data structure 2. Check convergence and mixing 3. Validate assumptions 4. Consider computational efficiency 5. Use appropriate diagnostics</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/","title":"Bayesian Statistical Applications","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#bayesian-ab-testing","title":"Bayesian A/B Testing","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#model-formulation","title":"Model Formulation","text":"<ol> <li> <p>Binomial Model For conversion rates: <pre><code>Group A: y\u2090 ~ Binomial(n\u2090, \u03b8\u2090)\nGroup B: y\u1d66 ~ Binomial(n\u1d66, \u03b8\u1d66)\nPriors: \u03b8\u2090, \u03b8\u1d66 ~ Beta(\u03b1, \u03b2)\n</code></pre></p> </li> <li> <p>Normal Model For continuous metrics: <pre><code>Group A: y\u2090 ~ N(\u03bc\u2090, \u03c3\u00b2)\nGroup B: y\u1d66 ~ N(\u03bc\u1d66, \u03c3\u00b2)\nPriors: \u03bc\u2090, \u03bc\u1d66 ~ N(\u03bc\u2080, \u03c3\u2080\u00b2)\n        \u03c3\u00b2 ~ InvGamma(\u03b1, \u03b2)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#inference","title":"Inference","text":"<ol> <li> <p>Probability of Improvement <pre><code>P(\u03b8\u1d66 &gt; \u03b8\u2090|data) = \u222b\u222bI(\u03b8\u1d66 &gt; \u03b8\u2090)p(\u03b8\u2090,\u03b8\u1d66|data)d\u03b8\u2090d\u03b8\u1d66\n</code></pre></p> </li> <li> <p>Expected Lift <pre><code>E[\u03b8\u1d66 - \u03b8\u2090|data] = E[\u03b8\u1d66|data] - E[\u03b8\u2090|data]\n</code></pre></p> </li> <li> <p>Risk Assessment <pre><code>P(\u03b8\u1d66 - \u03b8\u2090 &gt; \u03b4|data)  # Probability of meaningful difference\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#bayesian-regression","title":"Bayesian Regression","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#linear-regression-model","title":"Linear Regression Model","text":"<pre><code>y = X\u03b2 + \u03b5\n\u03b5 ~ N(0, \u03c3\u00b2I)\n</code></pre>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#prior-specifications","title":"Prior Specifications","text":"<ol> <li> <p>Coefficients <pre><code>\u03b2 ~ N(\u03bc\u2080, \u03a3\u2080)  # Multivariate normal prior\n</code></pre></p> </li> <li> <p>Variance <pre><code>\u03c3\u00b2 ~ InvGamma(\u03b1, \u03b2)  # Inverse gamma prior\n</code></pre></p> </li> <li> <p>Joint Posterior <pre><code>p(\u03b2,\u03c3\u00b2|y) \u221d p(y|\u03b2,\u03c3\u00b2)p(\u03b2)p(\u03c3\u00b2)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#posterior-inference","title":"Posterior Inference","text":"<ol> <li> <p>Parameter Estimation <pre><code>E[\u03b2|y] = (X'X + \u03c3\u00b2\u03a3\u2080\u207b\u00b9)\u207b\u00b9(X'y + \u03c3\u00b2\u03a3\u2080\u207b\u00b9\u03bc\u2080)\nVar(\u03b2|y) = \u03c3\u00b2(X'X + \u03c3\u00b2\u03a3\u2080\u207b\u00b9)\u207b\u00b9\n</code></pre></p> </li> <li> <p>Prediction <pre><code>p(\u1ef9|x\u0303,y) = \u222b\u222bp(\u1ef9|x\u0303,\u03b2,\u03c3\u00b2)p(\u03b2,\u03c3\u00b2|y)d\u03b2d\u03c3\u00b2\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#hierarchical-models","title":"Hierarchical Models","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#general-structure","title":"General Structure","text":"<pre><code>Level 1 (Data): y_i ~ p(y|\u03b8\u1d62)\nLevel 2 (Parameters): \u03b8\u1d62 ~ p(\u03b8|\u03b7)\nLevel 3 (Hyperparameters): \u03b7 ~ p(\u03b7)\n</code></pre>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#hierarchical-linear-model","title":"Hierarchical Linear Model","text":"<ol> <li> <p>Model Specification <pre><code>y\u1d62\u2c7c = \u03b2\u2080\u2c7c + \u03b2\u2081\u2c7cx\u1d62\u2c7c + \u03b5\u1d62\u2c7c\n\u03b2\u2080\u2c7c = \u03b3\u2080\u2080 + \u03b3\u2080\u2081w\u2c7c + u\u2080\u2c7c\n\u03b2\u2081\u2c7c = \u03b3\u2081\u2080 + \u03b3\u2081\u2081w\u2c7c + u\u2081\u2c7c\n</code></pre></p> </li> <li> <p>Distribution Assumptions <pre><code>\u03b5\u1d62\u2c7c ~ N(0, \u03c3\u00b2)\n[u\u2080\u2c7c, u\u2081\u2c7c]' ~ N(0, \u03a3)\n</code></pre></p> </li> <li> <p>Prior Specifications <pre><code>\u03b3 ~ N(\u03bc\u03b3, \u03a3\u03b3)\n\u03c3\u00b2 ~ InvGamma(\u03b1\u2081, \u03b2\u2081)\n\u03a3 ~ InvWishart(\u03bd, S)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#hierarchical-logistic-regression","title":"Hierarchical Logistic Regression","text":"<ol> <li> <p>Model Structure <pre><code>y\u1d62\u2c7c ~ Bernoulli(p\u1d62\u2c7c)\nlogit(p\u1d62\u2c7c) = \u03b2\u2080\u2c7c + \u03b2\u2081\u2c7cx\u1d62\u2c7c\n\u03b2\u2080\u2c7c ~ N(\u03bc\u2080, \u03c4\u2080\u00b2)\n\u03b2\u2081\u2c7c ~ N(\u03bc\u2081, \u03c4\u2081\u00b2)\n</code></pre></p> </li> <li> <p>Hyperpriors <pre><code>\u03bc\u2080,\u03bc\u2081 ~ N(0, \u03c3\u00b2)\n\u03c4\u2080\u00b2,\u03c4\u2081\u00b2 ~ InvGamma(\u03b1, \u03b2)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#implementation-considerations","title":"Implementation Considerations","text":"<ol> <li>Model Building</li> <li>Start simple</li> <li>Add complexity gradually</li> <li>Check convergence</li> <li> <p>Assess model fit</p> </li> <li> <p>Prior Selection</p> </li> <li>Weakly informative</li> <li>Domain knowledge</li> <li> <p>Sensitivity analysis</p> </li> <li> <p>Computational Methods</p> </li> <li>MCMC (Gibbs, Metropolis-Hastings)</li> <li>Hamiltonian Monte Carlo</li> <li>Variational inference</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/applications/#diagnostics-and-model-checking","title":"Diagnostics and Model Checking","text":"<ol> <li> <p>MCMC Diagnostics <pre><code>R\u0302 (Gelman-Rubin statistic)\nEffective sample size\nTrace plots\nAutocorrelation\n</code></pre></p> </li> <li> <p>Posterior Predictive Checks <pre><code>yrep ~ p(y|\u03b8)  # Simulated data\nT(yrep) vs T(y)  # Test statistics comparison\n</code></pre></p> </li> <li> <p>Model Comparison <pre><code>WAIC = -2(lppd - pWAIC)\nDIC = D\u0304 + pD\nLOO-CV = \u03a3\u1d62log p(y\u1d62|y\u208b\u1d62)\n</code></pre></p> </li> </ol> <p>Remember: 1. Model complexity matches data structure 2. Check convergence and mixing 3. Validate assumptions 4. Consider computational efficiency 5. Use appropriate diagnostics</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/","title":"Fundamentals of Bayesian Statistics","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#bayes-theorem","title":"Bayes' Theorem","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#basic-form","title":"Basic Form","text":"<p><pre><code>P(\u03b8|X) = P(X|\u03b8)P(\u03b8) / P(X)\n</code></pre> where: - P(\u03b8|X) is posterior probability - P(X|\u03b8) is likelihood - P(\u03b8) is prior probability - P(X) is marginal likelihood (normalizing constant)</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#expanded-form","title":"Expanded Form","text":"<p><pre><code>P(X) = \u222bP(X|\u03b8)P(\u03b8)d\u03b8\n</code></pre> or for discrete case: <pre><code>P(X) = \u03a3P(X|\u03b8)P(\u03b8)\n</code></pre></p>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#alternative-expression","title":"Alternative Expression","text":"<p><pre><code>Posterior \u221d Likelihood \u00d7 Prior\n</code></pre> avoiding computation of normalizing constant</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#prior-and-posterior-distributions","title":"Prior and Posterior Distributions","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#prior-distributions","title":"Prior Distributions","text":"<ol> <li>Informative Priors</li> <li>Based on previous knowledge</li> <li>Expert opinion</li> <li>Historical data</li> <li> <p>Example: N(\u03bc\u2080, \u03c3\u2080\u00b2) for known mean</p> </li> <li> <p>Non-informative Priors</p> </li> <li>Uniform distribution</li> <li>Jeffreys prior: \u221a|I(\u03b8)|</li> <li>Reference priors</li> <li> <p>Example: P(\u03b8) \u221d 1 for location parameter</p> </li> <li> <p>Hierarchical Priors</p> </li> <li>Parameters have their own priors</li> <li>Hyperparameters    <pre><code>P(\u03b8) = \u222bP(\u03b8|\u03b7)P(\u03b7)d\u03b7\n</code></pre></li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#posterior-distributions","title":"Posterior Distributions","text":"<ol> <li>Point Estimates</li> <li>Posterior mean: E[\u03b8|X]</li> <li>Posterior median</li> <li> <p>Maximum a posteriori (MAP)</p> </li> <li> <p>Interval Estimates</p> </li> <li>Credible intervals</li> <li> <p>Highest posterior density (HPD)    <pre><code>P(a \u2264 \u03b8 \u2264 b|X) = 1-\u03b1\n</code></pre></p> </li> <li> <p>Posterior Predictive <pre><code>P(X\u0303|X) = \u222bP(X\u0303|\u03b8)P(\u03b8|X)d\u03b8\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#conjugate-priors","title":"Conjugate Priors","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#definition","title":"Definition","text":"<p>Prior and posterior from same family of distributions</p>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#common-conjugate-pairs","title":"Common Conjugate Pairs","text":"<ol> <li>Binomial-Beta</li> <li>Likelihood: Binomial(n,\u03b8)</li> <li>Prior: Beta(\u03b1,\u03b2)</li> <li> <p>Posterior: Beta(\u03b1+x, \u03b2+n-x)    where x is number of successes</p> </li> <li> <p>Normal-Normal</p> </li> <li>Likelihood: N(\u03b8,\u03c3\u00b2)</li> <li>Prior: N(\u03bc\u2080,\u03c3\u2080\u00b2)</li> <li> <p>Posterior: N(\u03bc\u2099,\u03c3\u2099\u00b2)    where:    <pre><code>\u03bc\u2099 = (\u03c3\u207b\u00b2X\u0304n + \u03c3\u2080\u207b\u00b2\u03bc\u2080)/(\u03c3\u207b\u00b2n + \u03c3\u2080\u207b\u00b2)\n\u03c3\u2099\u00b2 = 1/(\u03c3\u207b\u00b2n + \u03c3\u2080\u207b\u00b2)\n</code></pre></p> </li> <li> <p>Poisson-Gamma</p> </li> <li>Likelihood: Poisson(\u03b8)</li> <li>Prior: Gamma(\u03b1,\u03b2)</li> <li>Posterior: Gamma(\u03b1+\u03a3x, \u03b2+n)</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#bayesian-inference","title":"Bayesian Inference","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#parameter-estimation","title":"Parameter Estimation","text":"<ol> <li> <p>Point Estimation <pre><code>\u03b8\u0302 = E[\u03b8|X] = \u222b\u03b8P(\u03b8|X)d\u03b8\n</code></pre></p> </li> <li> <p>Interval Estimation</p> </li> <li>Equal-tailed interval:    <pre><code>[\u03b8\u2097,\u03b8\u1d64]: P(\u03b8 &lt; \u03b8\u2097|X) = P(\u03b8 &gt; \u03b8\u1d64|X) = \u03b1/2\n</code></pre></li> <li>HPD interval:    <pre><code>P(\u03b8\u2208[\u03b8\u2097,\u03b8\u1d64]|X) = 1-\u03b1\n</code></pre>    minimizing \u03b8\u1d64-\u03b8\u2097</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#hypothesis-testing","title":"Hypothesis Testing","text":"<ol> <li>Bayes Factor: <pre><code>BF\u2081\u2080 = P(X|H\u2081)/P(X|H\u2080)\n</code></pre> Interpretation:</li> <li>BF\u2081\u2080 &gt; 1: Evidence for H\u2081</li> <li> <p>BF\u2081\u2080 &lt; 1: Evidence for H\u2080</p> </li> <li> <p>Posterior Probability: <pre><code>P(H\u2081|X) = P(X|H\u2081)P(H\u2081)/P(X)\n</code></pre></p> </li> <li> <p>Decision Theory: <pre><code>d* = argmin_d \u222bL(d,\u03b8)P(\u03b8|X)d\u03b8\n</code></pre> where L is loss function</p> </li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#model-comparison","title":"Model Comparison","text":"<ol> <li> <p>Bayesian Model Averaging: <pre><code>P(\u03b8|X) = \u03a3P(\u03b8|M_k,X)P(M_k|X)\n</code></pre></p> </li> <li> <p>DIC (Deviance Information Criterion): <pre><code>DIC = D\u0304 + pD\n</code></pre> where:</p> </li> <li>D\u0304 is expected deviance</li> <li>pD is effective number of parameters</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#practical-considerations","title":"Practical Considerations","text":""},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#prior-selection","title":"Prior Selection","text":"<ol> <li>Sensitivity Analysis</li> <li>Multiple priors</li> <li>Impact on conclusions</li> <li> <p>Robustness checks</p> </li> <li> <p>Elicitation</p> </li> <li>Expert knowledge</li> <li>Historical data</li> <li>Meta-analysis</li> </ol>"},{"location":"grassroots/statistics/7_bayesian_statistics/fundamentals/#computation","title":"Computation","text":"<ol> <li>Analytical Solutions</li> <li>Conjugate priors</li> <li> <p>Simple models</p> </li> <li> <p>Numerical Methods</p> </li> <li>MCMC</li> <li>Variational inference</li> <li>Laplace approximation</li> </ol> <p>Remember: 1. Prior specification is crucial 2. Conjugate priors simplify computation 3. Interpretation differs from frequentist 4. Model checking is important 5. Consider computational feasibility</p>"},{"location":"grassroots/statistics/8_advanced_topics/","title":"Advanced topics","text":""},{"location":"grassroots/statistics/8_advanced_topics/#dimensionality-reduction","title":"Dimensionality Reduction","text":""},{"location":"grassroots/statistics/8_advanced_topics/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":""},{"location":"grassroots/statistics/8_advanced_topics/#mathematical-foundation","title":"Mathematical Foundation","text":"<ol> <li>Objective</li> <li>Find orthogonal directions maximizing variance</li> <li>Linear transformation of data</li> <li> <p>Minimize reconstruction error</p> </li> <li> <p>Formulation <pre><code>X = U\u03a3V'\n</code></pre> where:</p> </li> <li>X is centered data matrix (n \u00d7 p)</li> <li>U is left singular vectors (n \u00d7 p)</li> <li>\u03a3 is diagonal matrix of singular values</li> <li>V is right singular vectors (p \u00d7 p)</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#properties","title":"Properties","text":"<ol> <li>Principal Components</li> <li>First PC: w\u2081 = argmax ||w||=1 Var(Xw)</li> <li>Subsequent PCs: orthogonal to previous</li> <li>Loading vector: eigenvectors of X'X</li> <li> <p>Scores: Xw</p> </li> <li> <p>Variance Explained <pre><code>\u03bb\u1d62/\u03a3\u03bb\u1d62\n</code></pre> where \u03bb\u1d62 are eigenvalues of covariance matrix</p> </li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Data Preprocessing</li> <li>Center: X\u0303 = X - \u03bc</li> <li> <p>(Optional) Scale: X\u0303 = (X - \u03bc)/\u03c3</p> </li> <li> <p>Computation</p> </li> <li>Covariance matrix: S = X\u0303'X\u0303/n</li> <li>Eigendecomposition: S = V\u039bV'</li> <li>PC scores: Z = X\u0303V</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#factor-analysis","title":"Factor Analysis","text":""},{"location":"grassroots/statistics/8_advanced_topics/#model-specification","title":"Model Specification","text":"<ol> <li>Basic Model <pre><code>X = \u039bF + \u03b5\n</code></pre> where:</li> <li>X is p-dimensional observed variables</li> <li>\u039b is p \u00d7 k loading matrix</li> <li>F is k-dimensional factors</li> <li> <p>\u03b5 is unique factors</p> </li> <li> <p>Assumptions <pre><code>F ~ N(0, I)\n\u03b5 ~ N(0, \u03a8)\n</code></pre> where \u03a8 is diagonal</p> </li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#estimation-methods","title":"Estimation Methods","text":"<ol> <li>Maximum Likelihood</li> <li>Iterative procedure</li> <li> <p>Likelihood: <pre><code>L(\u039b,\u03a8|X) = -\u00bd[log|\u039b\u039b'+\u03a8| + tr((\u039b\u039b'+\u03a8)\u207b\u00b9S)]\n</code></pre></p> </li> <li> <p>Principal Factor</p> </li> <li>Initial estimate: \u03a8 = diag(1 - h\u00b2)</li> <li>h\u00b2 are communalities</li> <li>Iterate until convergence</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#factor-rotation","title":"Factor Rotation","text":"<ol> <li>Orthogonal Rotation</li> <li>Varimax: maximize variance of squared loadings</li> <li>Quartimax: simplify variables</li> <li> <p>Equamax: compromise between varimax and quartimax</p> </li> <li> <p>Oblique Rotation</p> </li> <li>Promax: start with varimax, allow correlation</li> <li>Direct oblimin: minimize cross-products of loadings</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#t-sne","title":"t-SNE","text":""},{"location":"grassroots/statistics/8_advanced_topics/#algorithm","title":"Algorithm","text":"<ol> <li>Similarity Computation</li> <li> <p>High-dimensional similarities: <pre><code>p\u2c7c|\u1d62 = exp(-||x\u1d62-x\u2c7c||\u00b2/2\u03c3\u1d62\u00b2)/\u03a3\u2096exp(-||x\u1d62-x\u2096||\u00b2/2\u03c3\u1d62\u00b2)\np\u1d62\u2c7c = (p\u2c7c|\u1d62 + p\u1d62|\u2c7c)/2n\n</code></pre></p> </li> <li> <p>Low-dimensional Similarities <pre><code>q\u1d62\u2c7c = (1 + ||y\u1d62-y\u2c7c||\u00b2)\u207b\u00b9/\u03a3\u2096,\u2097(1 + ||y\u2096-y\u2097||\u00b2)\u207b\u00b9\n</code></pre></p> </li> <li> <p>Objective Function <pre><code>KL(P||Q) = \u03a3\u1d62\u03a3\u2c7cp\u1d62\u2c7clog(p\u1d62\u2c7c/q\u1d62\u2c7c)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#implementation-details","title":"Implementation Details","text":"<ol> <li>Perplexity</li> <li>Controls effective number of neighbors</li> <li>Typically between 5 and 50</li> <li> <p>Adaptive \u03c3\u1d62 selection</p> </li> <li> <p>Optimization</p> </li> <li>Gradient descent with momentum</li> <li>Early exaggeration</li> <li>Learning rate annealing</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#advantageslimitations","title":"Advantages/Limitations","text":"<ol> <li>Advantages</li> <li>Preserves local structure</li> <li>Handles non-linear relationships</li> <li> <p>Reveals clusters</p> </li> <li> <p>Limitations</p> </li> <li>Non-parametric (no out-of-sample extension)</li> <li>Computationally intensive</li> <li>Non-convex optimization</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#comparison-and-selection","title":"Comparison and Selection","text":""},{"location":"grassroots/statistics/8_advanced_topics/#method-selection-criteria","title":"Method Selection Criteria","text":"<ol> <li>Data Characteristics</li> <li>Sample size</li> <li>Dimensionality</li> <li>Linear vs non-linear relationships</li> <li> <p>Sparsity</p> </li> <li> <p>Objectives</p> </li> <li>Visualization</li> <li>Feature extraction</li> <li>Data compression</li> <li>Structure discovery</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#performance-metrics","title":"Performance Metrics","text":"<ol> <li> <p>Reconstruction Error <pre><code>||X - X\u0302||\u00b2\n</code></pre></p> </li> <li> <p>Explained Variance <pre><code>R\u00b2 = 1 - SS_res/SS_tot\n</code></pre></p> </li> <li> <p>Structure Preservation</p> </li> <li>Trustworthiness</li> <li>Continuity</li> <li>Local structure preservation</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/#best-practices","title":"Best Practices","text":""},{"location":"grassroots/statistics/8_advanced_topics/#implementation-guidelines","title":"Implementation Guidelines","text":"<ol> <li>Data Preprocessing</li> <li>Scaling/standardization</li> <li>Missing value handling</li> <li> <p>Outlier detection</p> </li> <li> <p>Dimensionality Selection</p> </li> <li>Scree plot (PCA)</li> <li>Parallel analysis (FA)</li> <li> <p>Perplexity tuning (t-SNE)</p> </li> <li> <p>Validation</p> </li> <li>Cross-validation</li> <li>Stability analysis</li> <li>Visual inspection</li> </ol> <p>Remember: 1. Choose method based on objectives 2. Consider computational resources 3. Validate results 4. Understand assumptions 5. Document decisions</p>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/","title":"Dimensionality Reduction","text":""},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":""},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#mathematical-foundation","title":"Mathematical Foundation","text":"<ol> <li>Objective</li> <li>Find orthogonal directions maximizing variance</li> <li>Linear transformation of data</li> <li> <p>Minimize reconstruction error</p> </li> <li> <p>Formulation <pre><code>X = U\u03a3V'\n</code></pre> where:</p> </li> <li>X is centered data matrix (n \u00d7 p)</li> <li>U is left singular vectors (n \u00d7 p)</li> <li>\u03a3 is diagonal matrix of singular values</li> <li>V is right singular vectors (p \u00d7 p)</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#properties","title":"Properties","text":"<ol> <li>Principal Components</li> <li>First PC: w\u2081 = argmax ||w||=1 Var(Xw)</li> <li>Subsequent PCs: orthogonal to previous</li> <li>Loading vector: eigenvectors of X'X</li> <li> <p>Scores: Xw</p> </li> <li> <p>Variance Explained <pre><code>\u03bb\u1d62/\u03a3\u03bb\u1d62\n</code></pre> where \u03bb\u1d62 are eigenvalues of covariance matrix</p> </li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Data Preprocessing</li> <li>Center: X\u0303 = X - \u03bc</li> <li> <p>(Optional) Scale: X\u0303 = (X - \u03bc)/\u03c3</p> </li> <li> <p>Computation</p> </li> <li>Covariance matrix: S = X\u0303'X\u0303/n</li> <li>Eigendecomposition: S = V\u039bV'</li> <li>PC scores: Z = X\u0303V</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#factor-analysis","title":"Factor Analysis","text":""},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#model-specification","title":"Model Specification","text":"<ol> <li>Basic Model <pre><code>X = \u039bF + \u03b5\n</code></pre> where:</li> <li>X is p-dimensional observed variables</li> <li>\u039b is p \u00d7 k loading matrix</li> <li>F is k-dimensional factors</li> <li> <p>\u03b5 is unique factors</p> </li> <li> <p>Assumptions <pre><code>F ~ N(0, I)\n\u03b5 ~ N(0, \u03a8)\n</code></pre> where \u03a8 is diagonal</p> </li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#estimation-methods","title":"Estimation Methods","text":"<ol> <li>Maximum Likelihood</li> <li>Iterative procedure</li> <li> <p>Likelihood: <pre><code>L(\u039b,\u03a8|X) = -\u00bd[log|\u039b\u039b'+\u03a8| + tr((\u039b\u039b'+\u03a8)\u207b\u00b9S)]\n</code></pre></p> </li> <li> <p>Principal Factor</p> </li> <li>Initial estimate: \u03a8 = diag(1 - h\u00b2)</li> <li>h\u00b2 are communalities</li> <li>Iterate until convergence</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#factor-rotation","title":"Factor Rotation","text":"<ol> <li>Orthogonal Rotation</li> <li>Varimax: maximize variance of squared loadings</li> <li>Quartimax: simplify variables</li> <li> <p>Equamax: compromise between varimax and quartimax</p> </li> <li> <p>Oblique Rotation</p> </li> <li>Promax: start with varimax, allow correlation</li> <li>Direct oblimin: minimize cross-products of loadings</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#t-sne","title":"t-SNE","text":""},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#algorithm","title":"Algorithm","text":"<ol> <li>Similarity Computation</li> <li> <p>High-dimensional similarities: <pre><code>p\u2c7c|\u1d62 = exp(-||x\u1d62-x\u2c7c||\u00b2/2\u03c3\u1d62\u00b2)/\u03a3\u2096exp(-||x\u1d62-x\u2096||\u00b2/2\u03c3\u1d62\u00b2)\np\u1d62\u2c7c = (p\u2c7c|\u1d62 + p\u1d62|\u2c7c)/2n\n</code></pre></p> </li> <li> <p>Low-dimensional Similarities <pre><code>q\u1d62\u2c7c = (1 + ||y\u1d62-y\u2c7c||\u00b2)\u207b\u00b9/\u03a3\u2096,\u2097(1 + ||y\u2096-y\u2097||\u00b2)\u207b\u00b9\n</code></pre></p> </li> <li> <p>Objective Function <pre><code>KL(P||Q) = \u03a3\u1d62\u03a3\u2c7cp\u1d62\u2c7clog(p\u1d62\u2c7c/q\u1d62\u2c7c)\n</code></pre></p> </li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#implementation-details","title":"Implementation Details","text":"<ol> <li>Perplexity</li> <li>Controls effective number of neighbors</li> <li>Typically between 5 and 50</li> <li> <p>Adaptive \u03c3\u1d62 selection</p> </li> <li> <p>Optimization</p> </li> <li>Gradient descent with momentum</li> <li>Early exaggeration</li> <li>Learning rate annealing</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#advantageslimitations","title":"Advantages/Limitations","text":"<ol> <li>Advantages</li> <li>Preserves local structure</li> <li>Handles non-linear relationships</li> <li> <p>Reveals clusters</p> </li> <li> <p>Limitations</p> </li> <li>Non-parametric (no out-of-sample extension)</li> <li>Computationally intensive</li> <li>Non-convex optimization</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#comparison-and-selection","title":"Comparison and Selection","text":""},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#method-selection-criteria","title":"Method Selection Criteria","text":"<ol> <li>Data Characteristics</li> <li>Sample size</li> <li>Dimensionality</li> <li>Linear vs non-linear relationships</li> <li> <p>Sparsity</p> </li> <li> <p>Objectives</p> </li> <li>Visualization</li> <li>Feature extraction</li> <li>Data compression</li> <li>Structure discovery</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#performance-metrics","title":"Performance Metrics","text":"<ol> <li> <p>Reconstruction Error <pre><code>||X - X\u0302||\u00b2\n</code></pre></p> </li> <li> <p>Explained Variance <pre><code>R\u00b2 = 1 - SS_res/SS_tot\n</code></pre></p> </li> <li> <p>Structure Preservation</p> </li> <li>Trustworthiness</li> <li>Continuity</li> <li>Local structure preservation</li> </ol>"},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#best-practices","title":"Best Practices","text":""},{"location":"grassroots/statistics/8_advanced_topics/dimensionality_reduction/#implementation-guidelines","title":"Implementation Guidelines","text":"<ol> <li>Data Preprocessing</li> <li>Scaling/standardization</li> <li>Missing value handling</li> <li> <p>Outlier detection</p> </li> <li> <p>Dimensionality Selection</p> </li> <li>Scree plot (PCA)</li> <li>Parallel analysis (FA)</li> <li> <p>Perplexity tuning (t-SNE)</p> </li> <li> <p>Validation</p> </li> <li>Cross-validation</li> <li>Stability analysis</li> <li>Visual inspection</li> </ol> <p>Remember: 1. Choose method based on objectives 2. Consider computational resources 3. Validate results 4. Understand assumptions 5. Document decisions</p>"},{"location":"research/","title":"Research","text":"<ol> <li>Definition</li> <li>Hallucination classification</li> <li>Detection</li> <li>Benchmarks</li> </ol>"},{"location":"research/0_definition/","title":"Hallucination Definition","text":""},{"location":"research/0_definition/#definition-origin","title":"Definition origin","text":"<p>Some important papers <sup>1</sup> <sup>2</sup>  and meta studies <sup>3</sup> <sup>4</sup>   seems to point to the definition from a summarisation paper from Google <sup>5</sup> (~900 citations)</p>"},{"location":"research/0_definition/#definition","title":"Definition","text":"<p>Hallucinations can be defined in the following way: \"hallucination is typically referred to as a phenomenon in which the generated content appears nonsensical (=! untruthful) or unfaithful to the provided source content\" <sup>5</sup> </p>"},{"location":"research/0_definition/#faithfulness-and-factuality","title":"Faithfulness and Factuality","text":"<p>faithfulness: \"a faithful model will generate a summary that only has information that is supported by its document <sup>5</sup> .\"</p> <p>Factuality: \"A summary S of a document D contains a factual hallucination if it contains information not found in D that is factually correct. Factual hallucinations may be composed of intrinsic hallucinations or extrinsic hallucinations\" <sup>5</sup> </p> <p>Faithfulness and Factuality are independent from one another.</p> <p>==What about Truthfulness==</p>"},{"location":"research/0_definition/#examples","title":"Examples :","text":"<p>Doc: \"In the latest Microsoft keynote, windows 98 has just been released\"  The summary \"windows 11 has just been released\" is unfaithful but factual</p> <p>==The summary \"windows 98 has been released by Microsoft\" is unfaithful but factual== adding external implicit knowledge leads to unfaithful content, here we dont know that microsoft is the company realising windows as it is not clear by the context. The statement is nonetheless factual</p> <p>The summary \"windows 98 has just been released\" is faithful but untruthful (outdated)</p>"},{"location":"research/0_definition/#intrinsic-vs-extrinsic","title":"Intrinsic vs Extrinsic","text":"<p>in the context of summarisation, intrinsic hallucination are miss-representation of the source document while extrinsic hallucinations, are the generation of facts that are not present in the input document</p> <p>These two concept are mutually exclusive.</p> <p>Doc: \"French prime minister just used the 49.3 to pass the budget\"  The summary \"Dutch prime minister just used the 49.3 to pass the budget\" is an intrinsic hallucination The summary \"The prime minister is very unpopular due to its use of article 49.3\"  is an extrinsic hallucination</p>"},{"location":"research/0_definition/#the-point-for-hallucinations","title":"The point for hallucinations:","text":"<p>Hallucination != bad : </p> <p>\u201challucinations in summarisation are acceptable if they lead to better summaries that are factual with respect to the document and the associated background knowledge.\"</p>"},{"location":"research/0_definition/#problem-specific-definition","title":"Problem specific definition","text":"<p>Add task specific definition</p>"},{"location":"research/0_definition/#interrogations","title":"Interrogations","text":"<p>At which point do we consider that two things are the same ? </p> <p>\"Earth is a Round\" =&gt; \"Earth is a Sphere\" ? this implication is considered an extrinsic hallucination by my definition</p> <p>How do we define factuality ?</p> <p>an alternative definition of the problem from Dong et al. <sup>4</sup> (where facts are interpreted as the input context, making faithfulness == factuality)</p> <ol> <li> <p>Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625\u2013630, June 2024. doi:10.1038/s41586-024-07421-0.\u00a0\u21a9</p> </li> <li> <p>Katja Filippova. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. In Findings of the Association for Computational Linguistics: EMNLP 2020, 864\u2013870. Online, 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.findings-emnlp.76.\u00a0\u21a9</p> </li> <li> <p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv., 55(12):248:1\u2013248:38, March 2023. doi:10.1145/3571730.\u00a0\u21a9</p> </li> <li> <p>Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, and Jingjing Liu. Multi-Fact Correction in Abstractive Text Summarization. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 9320\u20139331. Online, November 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.emnlp-main.749.\u00a0\u21a9\u21a9</p> </li> <li> <p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On Faithfulness and Factuality in Abstractive Summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 1906\u20131919. Online, July 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.173.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"research/1_classification/","title":"Index","text":""},{"location":"research/1_classification/#hallucination-classification","title":"Hallucination classification","text":"<p>Reference :  A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</p>"},{"location":"research/1_classification/#definition","title":"Definition","text":"<p>Hallucinations in large language model are in some sense analogous to what they mean in the context of a human being, but there are some nuances. The oxford dictionary define them as \"an experience involving the apparent perception of something not present\", in the context of NLP, hallucination refers to a phenomena where the generated content appears to be non sensical or unfaithful to the provided context. This materialise in a way that is sort of similar to human hallucination but this, as if the model was dreaming.</p> <p>We distinguish two kind of hallucination: intrinsic and extrinsic (unfaithful and untruthful).</p>"},{"location":"research/1_classification/#faithfulness-hallucination","title":"Faithfulness hallucination","text":"<p>Faithfulness hallucinations happen when the model fails to follow or conflict with the provided context when generate new token.</p>"},{"location":"research/1_classification/#truthfulness-hallucination","title":"Truthfulness hallucination","text":"<p>Truthfulness hallucinations happen when the model statements cannot be verified in the context or a knowledge base.</p> <p></p>"},{"location":"research/2_detection/","title":"Index","text":"<p>Based on the following meta study <sup>1</sup>, what are the methods for detecting hallucinations for a model.</p>"},{"location":"research/2_detection/#detection","title":"Detection","text":""},{"location":"research/2_detection/#detection-strategies-taxonomy","title":"Detection strategies taxonomy","text":"<ul> <li>Fact checking<ul> <li>External retrieval</li> <li>Internal retrieval</li> </ul> </li> <li>Incertitude quantification<ul> <li>LLM's internal states</li> <li>LLM's behaviour</li> </ul> </li> </ul>"},{"location":"research/2_detection/#external-retrieval","title":"External retrieval","text":"<p>Using an external knowledge base to compare with the model facts</p> <p>Example with FActScore <sup>2</sup></p> <p></p> <ul> <li>Step 1: Model generates biographies of public figures</li> <li>Step 2: Leverage InstructGPT to turn biographies into atomic facts</li> <li>Step 3: Use instruct Llama to label the atomic fact (\u201cirrelevant\u201d, \u201dsupported\u201d, \u201cnot supported\u201d) based on a retrieved (Generalizable T5-based Retrievers) knowledge base (wikipedia)</li> <li>Step 4: Evaluate the proportion of fact that are supported</li> </ul>"},{"location":"research/2_detection/#internal-retrieval","title":"Internal retrieval","text":"<p>Use the model to check its own replies</p> <p>Example: Chain-of-Verification (CoVe) <sup>3</sup></p> <p>Prompt the model to: 1. Generate a draft 2. Plan verification questions (not templated) 3. Answer verification questions 4. Generate a final response</p> <p></p>"},{"location":"research/2_detection/#internal-states","title":"Internal states","text":"<p>Leveraging LLM's internal states such as the entropy of a given token or the perplexity of a sentence to infer on the hallucination state.</p> <p>Example of an end to end implementation :  A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation <sup>4</sup></p> <ol> <li>Generating sentense tokens</li> <li>Key concept identification (using AI)</li> <li>Uncertainty quantification (Using the lowest probability of any token in a given concept)</li> <li>Generating validation questions (using AI)</li> <li>Knowledge retrieval (external)</li> <li>Question answering</li> <li>Sentence fix</li> </ol> <p>Authors achieved a recall of 88% and a mitigation rate of 56% The false positive didn't have any impact on performances.</p> <p></p>"},{"location":"research/2_detection/#behavioural-approach","title":"Behavioural approach","text":"<p>Use behavioural tools to implement fact verification approaches. This set of tools is usually used when no other technique is applicable (for instance, when the API doesn't give access to token probability).</p> <p>SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models <sup>5</sup></p> <p>Uses multi sampling techniques.</p> <p>Relies on the idea that if an LLM has been trained on a concept, the sampled ideas should be similar and contain consistent facts.</p> <ol> <li>multi sampling</li> <li>Use LLMs to check the consistency between prompts</li> <li>Use the results to assert the probability of the sequence to be hallucinated</li> </ol> <p></p> <ol> <li> <p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. ACM Transactions on Information Systems, pages 3703155, November 2024. arXiv:2311.05232, doi:10.1145/3703155.\u00a0\u21a9</p> </li> <li> <p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. October 2023. arXiv:2305.14251, doi:10.48550/arXiv.2305.14251.\u00a0\u21a9</p> </li> <li> <p>Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-Verification Reduces Hallucination in Large Language Models. September 2023. arXiv:2309.11495, doi:10.48550/arXiv.2309.11495.\u00a0\u21a9</p> </li> <li> <p>Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. August 2023. arXiv:2307.03987, doi:10.48550/arXiv.2307.03987.\u00a0\u21a9</p> </li> <li> <p>Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. October 2023. arXiv:2303.08896, doi:10.48550/arXiv.2303.08896.\u00a0\u21a9</p> </li> </ol>"},{"location":"research/3_benchmarks/","title":"Index","text":""},{"location":"research/3_benchmarks/#benchmarks","title":"Benchmarks","text":"<p>Most of the existing benchmark relies on extensive dataset and simple metrics.</p> <p>One recent approach  from Hugging Face <sup>1</sup> has been to aggregate those benchmark into a more unified leaderboard.</p> <p>We will discuss individual benchmarks and their functioning in this page. </p>"},{"location":"research/3_benchmarks/#truthfulness-benchmarks","title":"Truthfulness Benchmarks","text":""},{"location":"research/3_benchmarks/#closed-book-qa","title":"Closed book QA:","text":"<ul> <li> <p>NQ open, Google, 2019: derived from Natural Questions <sup>2</sup> the first large publicly available data set to pair real (google) user queries with high-quality annotations of answers in (wikipedia) documents.   The dataset is composed of quadruples question-evidence-longAnswer-shortAnswer. Scores on the benchmark at the time present a high upper bound, with good human performance.   End up being a leaderboard where RAG model perform best, but most haven't been tested.</p> </li> <li> <p>TriviaQA <sup>3</sup>, University of Washington (+ a non profit founded by a Microsoft co-founder), 2017: Triples question-excerpt-answer with rich syntax  and complexe phrasing of non important/relatively funny facts (taken in part from online Trivia Quizz).   The issue with this dataset compared to NQ is that those facts are generally niche/useless and the metrics on this benchmark are less relevant to everyday tasks.</p> </li> <li> <p>PopQA <sup>4</sup>, University of Washington (+ a non profit founded by a Microsoft co-founder), ACL 2023.   The goal of the study was to highlight the need for RAG when using long tail knowledge (rarely occurring information within the training data) in comparison with popular knowledge (Pop in PopQA).   PopQA is supposed to cover such long tail knowledge. Wikipedia views are used to determine how popular is a question.   The study highlights 3 main findings: </p> <ul> <li>With those dataset, scaling up models does not significantly improve the performance.</li> <li>Retrieval-augmented LMs are particularly competitive when subject entities are not popular. Surprisingly, retrieval augmentation can hurt the performance of  LLMs on questions about popular entities as the retrieved context can be misleading.\"</li> <li>We can chose wether or not to use RAG based on their provided popularity metric, increasing performances on PopQA by up to 10%.</li> </ul> </li> <li> <p>TruthfulQA <sup>5</sup>, University of Oxford/Open AI 2022 : benchmark composed of question that some human would answer falsely due to false beliefs or misconceptions. Judges the ability of a model to avoid replicating human misconceptions.    The performance difference between audited human and best model is 94% vs 58% respectively.   Interesting findings: the largest model were generally the least truthful and fine tuning is a better approach to enhance performances rather than learning more text coming from internet.</p> </li> </ul> <p>Missing: SimpleQA (openAI)</p>"},{"location":"research/3_benchmarks/#fact-checking","title":"Fact-Checking","text":"<ul> <li>FEVER <sup>6</sup>, University of Sheffield, Amazon, 2018: Fact Extraction and VERification. Taking claims from wikipedia passages and verifying wether or not the claims are supported by the document (using human annotation).  Challenging test as the model successfully achieved only 32%.   The reliance on human annotation make it so it is hard to really evaluate the inherent quality of the dataset (most humans agree on 70% of the labels).</li> </ul>"},{"location":"research/3_benchmarks/#hallucination-detection","title":"Hallucination detection","text":"<ul> <li>True-False <sup>7</sup> ACL 2023: based on a set of sentences and the LLM internal states as it predicts the sentence, train a classifier that output the probability of a statements truthfulness. Classifier achieve 71~83% accuracy based on the model.</li> </ul>"},{"location":"research/3_benchmarks/#faithfulness-benchmarks","title":"Faithfulness Benchmarks","text":""},{"location":"research/3_benchmarks/#summarisation","title":"Summarisation","text":"<ul> <li> <p>XSum <sup>8</sup> University of Edinburgh, 2018: Requires the model to resonate on the document instead of simply extracting facts.   Since it was written in 2018, it propose a CNN to work on the task associated with the Dataset.   The dataset is composed of article from the BBC and single sentence summaries. The summary sentence is typically written by the author of the article.   This benchmark can be used to evaluate how well a model perform on long context comprehension <sup>9</sup>. It is also used in the paper defining hallucination in a modern way <sup>10</sup>.</p> </li> <li> <p>CNN/Daily Mail <sup>11</sup> <sup>12</sup> , Google, NeurIPS 2015, Stanford, 2016 :   Proposing a supervised learning dataset for reading comprehension. The dataset is structured in context-query-answer triples.   1M corpus &amp; queries.   The author tackle the issue with an  attention/LSTM model, unsuccessfully so.   The next paper ups the accuracy from. ~7% to ~70%. It is a nice complement to the first paper as they dive much deeper into the dataset.   (metric: ROUGE-L, which assesses n-gram overlap with reference summaries)   ==SHOULDN'T THIS BE IN READING COMPREHENSION== ?</p> </li> <li> <p>NQ-Swap (derived from Natural Questions, exact match)</p> </li> </ul>"},{"location":"research/3_benchmarks/#reading-comprehension","title":"Reading comprehension","text":"<ul> <li>RACE (accuracy)</li> <li>SQuAD 2.0 (Exact Match)</li> </ul>"},{"location":"research/3_benchmarks/#instruction-following","title":"Instruction following","text":"<ul> <li>MemoTrap</li> <li>IFEval</li> </ul>"},{"location":"research/3_benchmarks/#hallucination-detection_1","title":"Hallucination Detection","text":"<ul> <li>FaithDial</li> <li>HaluEval</li> <li>HotpotQA</li> </ul> <p>Models are evaluated by accuracy</p> <p>Insights: - LLM size has beneficial impact on reducing faithfulness and truthfulness hallucination. It has to be underlined that truthfulness benefits from a better uplift.</p> <p>Fallback: - these benchmark don't optimise the prompt for each individual LLM, meaning that the comparison might be unfair and benefits some LLMs better than others - Benchmarks come and go in terms of popularity and it is hard to truly assess the progress of LLMs over time as they are rarely compared to the same metrics (MMLU aside) - some benchmarks are classified as closed book when they are developed as open book. others are classified as summarisation benchmark when they are reading comprehension, indicating a blurry line between these subjects - Some problematic about benchmark leakage should be taken in consideration <sup>13</sup></p> <ol> <li> <p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. ACM Transactions on Information Systems, pages 3703155, November 2024. arXiv:2311.05232, doi:10.1145/3703155.\u00a0\u21a9</p> </li> <li> <p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, August 2019. doi:10.1162/tacl_a_00276.\u00a0\u21a9</p> </li> <li> <p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. May 2017. arXiv:1705.03551, doi:10.48550/arXiv.1705.03551.\u00a0\u21a9</p> </li> <li> <p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. July 2023. arXiv:2212.10511, doi:10.48550/arXiv.2212.10511.\u00a0\u21a9</p> </li> <li> <p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring How Models Mimic Human Falsehoods. May 2022. arXiv:2109.07958, doi:10.48550/arXiv.2109.07958.\u00a0\u21a9</p> </li> <li> <p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for Fact Extraction and VERification. December 2018. arXiv:1803.05355, doi:10.48550/arXiv.1803.05355.\u00a0\u21a9</p> </li> <li> <p>Amos Azaria and Tom Mitchell. The Internal State of an LLM Knows When It's Lying. October 2023. arXiv:2304.13734, doi:10.48550/arXiv.2304.13734.\u00a0\u21a9</p> </li> <li> <p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. August 2018. arXiv:1808.08745, doi:10.48550/arXiv.1808.08745.\u00a0\u21a9</p> </li> <li> <p>Yifei Gao, Lei Wang, Jun Fang, Longhua Hu, and Jun Cheng. Empower Your Model with Longer and Better Context Comprehension. July 2023. arXiv:2307.13365, doi:10.48550/arXiv.2307.13365.\u00a0\u21a9</p> </li> <li> <p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On Faithfulness and Factuality in Abstractive Summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 1906\u20131919. Online, July 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.173.\u00a0\u21a9</p> </li> <li> <p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching Machines to Read and Comprehend. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.\u00a0\u21a9</p> </li> <li> <p>Danqi Chen, Jason Bolton, and Christopher D. Manning. A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task. August 2016. arXiv:1606.02858, doi:10.48550/arXiv.1606.02858.\u00a0\u21a9</p> </li> <li> <p>Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking Benchmark Leakage in Large Language Models. April 2024. arXiv:2404.18824, doi:10.48550/arXiv.2404.18824.\u00a0\u21a9</p> </li> </ol>"},{"location":"research/misc/DeepSeek/","title":"What makes DeepSeek V3 / R1 so \u2728 special \u2728","text":"<p>The breakthroughs are taking place over the year 2024, through the release of the V2, V3 and R1 papers.</p>"},{"location":"research/misc/DeepSeek/#v2","title":"V2","text":"<p>The V2 paper introduce the architectural changes, in particular they leverage 2 powerful changes : a house baked Mixture of Expert implementation (deepSeekMoE) and Multi Head Latent Attention (MHA) (DeepSeek developed as well).</p> <p>Those two changes intervene on two distinct layers of the transformer block: - MoE takes place in the FFN - MHA takes place in the attention block</p> <p>Architecture : </p> <p></p> <ol> <li>Multi head latent attention     Low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference.     Queries are also cached on the v3 paper</li> <li>MoE     Cross node MoE training - optimisation to significantly enhance training efficiency.     Basically there are a set of shared experts that are active at all stages, as well as routed experts that can be routed on any given tokens.</li> </ol>"},{"location":"research/misc/DeepSeek/#multi-head-latent-attention","title":"Multi head latent attention","text":"<p>While Llama3 uses Grouped-querry, attention</p> <p></p> <p>This leads to a compressed space that caches into 5% of the original size (20x saving in memory)</p> <p>In the attention layer, MLHA is implemented, where they project key, values but ALSO queries (as NOT shown in the following graph).</p> <p>This technique seems to be very close to LoRA.</p>"},{"location":"research/misc/DeepSeek/#deepseekmoe","title":"DeepSeekMoE","text":"<p>Then you have the DeepSeekMoE architecture, with use fine(er)-grained experts and isolate/group them.</p> <p></p> <p>The innovation here seems to be the fact that we have some shared expert (which are all activated at every token) and routed experts, which are selected at every token by the router.</p> <p>These changes, when implemented together, provide a significan bump in performance, as is shown by deepseek v2 paper:</p> <p></p> <p>Some insights about these graphs, Flash Attention was (is) a very big deal when it came out and had throughput benefits of 5 to 9 folds, this architecture gives us a speed boost of 5 fold with the added benefits of saving 93.3% of the KV cache on top of saving for the training cost.</p> <p>Here is a graph showing the speedup of FlashAttention</p> <p></p> <p>Some theory (TO BE CHECKED): FlashAttention is basically bypassing the old way to compute attention, a low level optimisation if you will. There have been rumours about similar practice from DeepSeek where they used PTX to rewrite some functions for optimisation.</p> <p>Source, Source 2</p>"},{"location":"research/misc/DeepSeek/#v3","title":"V3","text":"<p>V3 takes the ideas from v2, add some new pipeline tweaks and scales the model to get new performance. The V3 paper detail the following changes: </p> <ol> <li>Parameter and Data Scaling:     236B to 671B parameters (21 to 37B active at every token)     14.8T token for pre-training, very stable     1.5M samples for fine tuning</li> <li>Auxiliary-loss-free strategy for load balancing      Goal: minimising performance degradation due to load balancing </li> <li>Multi-token prediction training objective     Beneficial to model performance + can be used for speculative decoding and inference acceleration</li> <li>FP8 mixed precision</li> <li>\"DualPipe Algorithm\" </li> <li>New training process:      (2 stages) Context extension : original -&gt; 32K -&gt; 128K      Supervised fine tuning over 1.5M examples</li> </ol> <p>Impressively cheap cost:</p> <p></p> <p>Compared to Llama 3 training, this is very impressive:</p> <p></p> <p>Source </p> <p>Though this doesn't account for ablation studies, data generation cost etc. Also note that this table is for V3, not R1, CoT reasoning training comes at further cost.</p> <p>The R1 paper develop 2 training strategies, yielding both R1-Zero and R1</p>"},{"location":"research/misc/DeepSeek/#r1-zero","title":"R1-zero","text":"<p>R1-zero is their RL only approach to CoT reasoning emergence.</p> <p>Basically they use a lot of complexe reasoning task that are close problems (which have one correct output such as the code compiling etc.) and they just reward the model when it produce an output between the \\ \\ markups, as well as the result being correct.</p> <p>This improved the capabilities quite impressively, reaching performance levels close to o1  </p>"},{"location":"research/misc/DeepSeek/#r1","title":"R1","text":"<p>R1-zero had some limitation which motivated some refinements. The authors claim the output had poor readability and the CoT reasoning suffered from language mixing.</p> <p>Adding SFT as well as another step of RL in the pipeline to get better behaviour and performance</p> <p>Training is done in 4 steps:</p> <ol> <li>SFT to improve stability instead of cold start</li> <li>R1-Zero RL pipeline</li> <li>Data generation through rejection sampling -&gt; SFT</li> <li>RL for Helpfulness and harmlessness</li> </ol> <p>R1 ends up with very solid performance:</p> <p></p> <p>The R1 base model end up being 27x cheaper than contemporary o1 pricing per million token output</p>"},{"location":"research/misc/DeepSeek/#whats-next","title":"What's next ?","text":"<p>Distillation from DeepSeek R1 (chain of thought model)</p> <p>And the distillations are great too:</p> <p></p> <p>Another impressive step is that the distillation process enable levels of performance that can't be attained on the RL pipeline of R1-zero only. </p>"},{"location":"research/misc/DeepSeek/#running-r1-on-a-potato","title":"Running R1 on a potato ?","text":"<p>This is a common misconception that has been circulating online</p> <p>No. R1 is the size of V3, ~670B parameters. But they offer distilled version on Llama and Qwen.</p> <p></p>"}]}